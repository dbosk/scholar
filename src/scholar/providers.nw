\chapter{Search Providers}
\label{providers-module}

\section{Introduction}

This module implements search providers---adapters that connect Scholar to
various bibliographic databases.
Each provider knows how to query a specific database and convert results to
our common [[Paper]] type.

The provider abstraction allows us to:
\begin{description}
\item[Add new databases] without modifying the core search logic.
\item[Test in isolation] by mocking individual providers.
\item[Configure per-provider] settings like API keys and rate limits.
\item[Cache results] to avoid redundant API calls and respect rate limits.
\end{description}

\section{Caching}

Each provider caches search results using [[cachetools]].
The cache is a plain dictionary keyed by the query and limit parameters.
Caches are persisted to disk using pickle, so results survive across program
runs.
This is particularly useful for systematic reviews where the same search may
be run multiple times.


\section{Module structure}

The providers module exports the provider protocol and registry functions:

<<[[providers.py]]>>=
"""
Search providers for bibliographic databases.

Each provider implements the SearchProvider protocol and registers
itself with the provider registry on import.
"""

<<imports>>
<<constants>>
<<provider protocol>>
<<provider registry>>
<<semantic scholar provider>>
<<openalex provider>>
<<dblp provider>>
<<wos provider>>
<<ieee provider>>
@


\section{Testing}
\label{sec:providers-testing}

Tests are distributed throughout this document, appearing immediately after
each provider's implementation.
The test file collects all distributed test chunks:

<<test [[providers.py]]>>=
"""Tests for the providers module."""
import pytest
import requests
from unittest.mock import Mock, patch

from scholar import *
from scholar.providers import *


<<test functions>>
@


\section{Provider Protocol}
\label{sec:provider-protocol}

We define a protocol (structural typing) rather than an abstract base class.
This design choice reflects Python's \enquote{duck typing} philosophy: if an
object has a [[name]] attribute and a [[search]] method with the right
signature, it \emph{is} a provider---no inheritance required.

\subsection{Why Protocol Over ABC?}

Using [[Protocol]] from [[typing]] has several advantages over
[[abc.ABC]]:
\begin{description}
\item[No inheritance required] Providers don't need to inherit from a base
  class. This simplifies testing (mock objects work without setup) and allows
  third-party classes to be providers without modification.
\item[Explicit contracts] The protocol documents exactly what's required.
  Unlike duck typing alone, static type checkers can verify implementations.
\item[Flexibility] A class from another library could satisfy this protocol
  without knowing about Scholar, enabling future extensibility.
\end{description}

\subsection{Required Interface}

A provider must have:
\begin{description}
\item[[[name]]] A unique identifier string.
\item[[[search]]] A method that takes a query and returns papers.
\item[[[is_available]]] A method that returns [[True]] if the provider can be
  used (either requires no API key, or has its API key configured).
\end{description}

<<provider protocol>>=
class SearchProvider(Protocol):
    """Protocol for search providers."""

    name: str

    def search(
        self,
        query: str,
        limit: int = 100,
        filters: "SearchFilters | None" = None,
    ) -> list[Paper]:
        """
        Search for papers matching the query.

        Args:
            query: The search query string.
            limit: Maximum number of results to return.
            filters: Optional SearchFilters to apply.

        Returns:
            List of Paper objects matching the query.
        """
        ...

    def is_available(self) -> bool:
        """
        Check if this provider is available for use.

        A provider is available if it either doesn't require an API key,
        or has its required API key configured in the environment.

        Returns:
            True if the provider can be used, False otherwise.
        """
        ...
@


\section{Provider Registry}
\label{sec:provider-registry}

Providers register themselves when imported. This \emph{self-registration}
pattern allows the [[Search]] class to discover available providers without
hardcoding them.

\subsection{Why Self-Registration?}

The registry pattern provides plugin-like architecture:
\begin{description}
\item[Decoupling] The core search logic doesn't need to know about specific
  providers. Adding a new database is as simple as creating a new provider
  class and calling [[register_provider]].
\item[Lazy loading] Providers register at module import time. If a provider's
  dependencies aren't installed (e.g., missing API key), it simply won't
  register, and searches proceed with available providers.
\item[Discoverability] CLI commands can query the registry to show users
  which providers are available and configured.
\end{description}

\subsection{Implementation}

<<provider registry>>=
PROVIDERS: dict[str, SearchProvider] = {}


def register_provider(provider: SearchProvider) -> None:
    """Register a provider for use in searches."""
    PROVIDERS[provider.name] = provider


def get_provider(name: str) -> SearchProvider | None:
    """Get a provider by name, or None if not found."""
    return PROVIDERS.get(name)


def get_all_providers() -> list[SearchProvider]:
    """Get all registered providers."""
    return list(PROVIDERS.values())
@

\subsection{Testing the registry}

The registry should store and retrieve providers:

<<test functions>>=
class TestProviderRegistry:
    """Tests for the provider registry."""

    def test_s2_registered(self):
        """Semantic Scholar provider is auto-registered."""
        provider = get_provider("s2")
        assert provider is not None
        assert provider.name == "s2"

    def test_openalex_registered(self):
        """OpenAlex provider is auto-registered."""
        provider = get_provider("openalex")
        assert provider is not None
        assert provider.name == "openalex"

    def test_dblp_registered(self):
        """DBLP provider is auto-registered."""
        provider = get_provider("dblp")
        assert provider is not None
        assert provider.name == "dblp"

    def test_wos_registered(self):
        """Web of Science provider is auto-registered."""
        provider = get_provider("wos")
        assert provider is not None
        assert provider.name == "wos"

    def test_ieee_registered(self):
        """IEEE Xplore provider is auto-registered."""
        provider = get_provider("ieee")
        assert provider is not None
        assert provider.name == "ieee"

    def test_get_all_providers(self):
        """get_all_providers returns registered providers."""
        providers = get_all_providers()
        assert len(providers) >= 5
        names = [p.name for p in providers]
        assert "s2" in names
        assert "openalex" in names
        assert "dblp" in names
        assert "wos" in names
        assert "ieee" in names

    def test_get_unknown_provider(self):
        """get_provider returns None for unknown providers."""
        assert get_provider("unknown_provider") is None

    def test_providers_have_max_limit(self):
        """All providers have MAX_LIMIT attribute."""
        for name, provider in PROVIDERS.items():
            assert hasattr(provider, "MAX_LIMIT"), f"{name} missing MAX_LIMIT"

    def test_get_provider_limits(self):
        """get_provider_limits returns correct limits."""
        limits = get_provider_limits()
        assert limits["s2"] is None
        assert limits["openalex"] is None
        assert limits["dblp"] == 1000
        # WoS limit depends on API tier: 50 for Starter, 100 for Expanded
        assert limits["wos"] in (50, 100)
        assert limits["ieee"] == 200
@


\subsection{Default providers}
\label{sec:default-providers}

Default providers are determined \emph{dynamically} based on availability.
A provider is available (and thus included as a default) if it either:
\begin{itemize}
\item Does not require an API key ([[s2]], [[openalex]],
  [[dblp]]), or
\item Requires an API key and has one configured in the environment
  ([[wos]] with [[WOS_API_KEY]], [[ieee]] with [[IEEE_API_KEY]]).
\end{itemize}

This approach ensures that users automatically get access to all providers they
can use, without manual configuration.
When a user obtains an API key for a previously unavailable provider (like
Web of Science), that provider automatically becomes a default.

<<provider registry>>=
def get_default_providers() -> list[SearchProvider]:
    """
    Get providers that are currently available for use.

    Returns all providers where [[is_available()]] returns [[True]].
    This includes providers that don't require API keys (s2,
    openalex, dblp) and providers with required API keys that are configured
    in the environment (wos, ieee).
    """
    return [p for p in PROVIDERS.values() if p.is_available()]


def get_provider_limits() -> dict[str, int | None]:
    """
    Get the maximum result limits for all registered providers.

    Returns a dict mapping provider names to their MAX_LIMIT values.
    A value of None indicates no documented maximum limit.
    """
    return {name: getattr(p, "MAX_LIMIT", None) for name, p in PROVIDERS.items()}
@

\subsubsection{Testing get\_default\_providers}

The [[get_default_providers]] function returns only providers that are
currently available.
We test this behavior:

<<test functions>>=
class TestDefaultProviders:
    """Tests for get_default_providers functionality."""

    def test_get_default_providers_returns_list(self):
        """get_default_providers returns a list of providers."""
        defaults = get_default_providers()
        assert isinstance(defaults, list)

    def test_default_providers_are_available(self):
        """All default providers report is_available() as True."""
        for provider in get_default_providers():
            assert provider.is_available() is True

    def test_always_available_providers_in_defaults(self):
        """Providers that don't require API keys are always in defaults."""
        defaults = get_default_providers()
        default_names = [p.name for p in defaults]
        # These providers don't require API keys
        assert "s2" in default_names
        assert "openalex" in default_names
        assert "dblp" in default_names

    def test_wos_in_defaults_only_with_key(self, monkeypatch):
        """WoS is in defaults only when API keys are set."""
        # Without any keys
        monkeypatch.delenv("WOS_API_KEY", raising=False)
        monkeypatch.delenv("WOS_STARTER_API_KEY", raising=False)
        monkeypatch.delenv("WOS_EXPANDED_API_KEY", raising=False)
        # Re-register provider without keys
        wos_provider = WebOfScienceProvider(
            starter_api_key=None, expanded_api_key=None
        )
        register_provider(wos_provider)
        defaults = get_default_providers()
        assert "wos" not in [p.name for p in defaults]

    def test_ieee_in_defaults_only_with_key(self, monkeypatch):
        """IEEE is in defaults only when IEEE_API_KEY is set."""
        # Without key
        monkeypatch.delenv("IEEE_API_KEY", raising=False)
        # Re-register provider without key
        ieee_provider = IEEEXploreProvider(api_key=None)
        ieee_provider.api_key = None
        register_provider(ieee_provider)
        defaults = get_default_providers()
        assert "ieee" not in [p.name for p in defaults]
@


\section{Semantic Scholar Provider}
\label{sec:semantic-scholar}

Semantic Scholar is an AI-powered research tool that provides free access to
over 200 million academic papers.
Their API allows searching papers by keyword, author, venue, and more.

\subsection{API Access}

We call the Semantic Scholar REST API directly using [[requests]], rather than
using the [[semanticscholar]] package.
This gives us full control over rate limiting and error handling.

The API endpoint is [[https://api.semanticscholar.org/graph/v1/paper/search]].
An optional API key ([[S2_API_KEY]]) increases rate limits:
\begin{description}
\item[Without key] 100 requests per 5 minutes (we space requests 3 seconds apart)
\item[With key] 1 request per second sustained rate
\end{description}

We implement:
\begin{itemize}
\item Adaptive rate limiting based on API key presence
\item Exponential back-off retry on server errors (429, 5xx)
\item Clear error messages explaining rate limits
\end{itemize}

\subsection{Error Handling Philosophy}

All providers return an empty list on errors rather than raising exceptions,
but they log warnings to help diagnose issues.
This design choice ensures:
\begin{description}
\item[Partial results] If one provider fails (network issue, rate limit),
  other providers can still contribute results.
\item[Robustness] Transient errors don't crash the entire search.
\item[Visibility] Errors are logged via [[logging.warning()]], so users can
  see what went wrong (e.g., rate limits, timeouts, API errors).
\item[Simplicity] Callers don't need complex exception handling.
\end{description}

To see error messages, ensure logging is configured at WARNING level or above.
The CLI's [[providers]] command helps diagnose configuration issues like
missing API keys.

\subsection{Provider implementation}

<<semantic scholar provider>>=
class SemanticScholarProvider:
    """Search provider for Semantic Scholar."""

    name = "s2"
    MAX_LIMIT: int | None = None   # No documented maximum
    API_URL = "https://api.semanticscholar.org/graph/v1/paper/search"
    RATE_LIMIT_WITH_KEY = 1.0      # 1 second between requests
    RATE_LIMIT_WITHOUT_KEY = 3.0   # 3 seconds (100 req/5min = 0.33/sec)

    def __init__(self, api_key: str | None = None):
        """
        Initialize the Semantic Scholar provider.

        Args:
            api_key: Optional API key for higher rate limits.
                     If not provided, uses S2_API_KEY environment variable.
        """
        self.api_key = api_key or os.environ.get("S2_API_KEY")
        self._cache: dict = load_cache(self.name)
        register_cache(self.name, self._cache)
        self._last_request_time: float = 0.0
        self._session = self._create_session()

    def is_available(self) -> bool:
        """Semantic Scholar is always available (API key is optional)."""
        return True

    <<semantic scholar rate limiting>>
    <<semantic scholar search method>>
    <<semantic scholar get paper by doi>>
    <<convert semantic scholar paper>>
@

\subsubsection{Rate limiting and retry}

We implement two layers of rate limiting:
\begin{enumerate}
\item \textbf{Proactive rate limiting}: We track the time of the last request
  and wait if necessary before making a new one.
  The interval depends on whether we have an API key.
\item \textbf{Exponential back-off}: If we still hit rate limits (HTTP 429) or
  server errors (5xx), we retry with increasing delays.
\end{enumerate}

<<semantic scholar rate limiting>>=
@property
def _min_request_interval(self) -> float:
    """Return minimum interval based on whether API key is set."""
    if self.api_key:
        return self.RATE_LIMIT_WITH_KEY
    return self.RATE_LIMIT_WITHOUT_KEY

def _create_session(self) -> requests.Session:
    """Create a session with exponential back-off retry logic."""
    session = requests.Session()
    retry = Retry(
        total=3,
        backoff_factor=1,  # 1s, 2s, 4s delays
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["GET"],
    )
    adapter = HTTPAdapter(max_retries=retry)
    session.mount("https://", adapter)
    return session

def _wait_for_rate_limit(self) -> None:
    """Ensure minimum interval between requests."""
    elapsed = time.time() - self._last_request_time
    if elapsed < self._min_request_interval:
        time.sleep(self._min_request_interval - elapsed)
    self._last_request_time = time.time()
@

\subsubsection{Search method}

The search method calls the API with rate limiting and handles errors.
When the user requests more results than a single API call can return (the
Semantic Scholar API caps at 100 results per request), we automatically fetch
multiple pages using the [[offset]] parameter.

This pagination is transparent to the caller: they simply request the number of
results they want, and the provider handles the underlying paging.
The loop terminates when we have fetched enough results, when the API returns
no more data, or when we reach the total number of available results.

<<semantic scholar search method>>=
@cachedmethod(lambda self: self._cache, key=lambda self, query, limit=100, filters=None: (query, limit, filters.cache_key() if filters else ""))
def search(
    self,
    query: str,
    limit: int = 100,
    filters: SearchFilters | None = None,
) -> list[Paper]:
    """
    Search Semantic Scholar for papers matching the query.

    Automatically fetches multiple pages if the requested limit exceeds
    the API's per-request maximum (100 results per page).
    """
    all_papers: list[Paper] = []
    offset = 0
    page_size = min(limit, 100)  # S2 API returns max 100 per request

    try:
        while len(all_papers) < limit:
            self._wait_for_rate_limit()

            headers = {}
            if self.api_key:
                headers["x-api-key"] = self.api_key
                logger.debug("semantic_scholar: Using API key")

            # Calculate how many results we still need
            remaining = limit - len(all_papers)
            current_page_size = min(page_size, remaining)

            params = {
                "query": query,
                "limit": current_page_size,
                "offset": offset,
                "fields": "title,authors,year,abstract,venue,externalIds,url,openAccessPdf,citationCount,publicationTypes",
            }

            # Apply filters (Semantic Scholar supports most filters natively)
            <<semantic scholar apply filters>>

            response = self._session.get(
                self.API_URL,
                params=params,
                headers=headers,
                timeout=30,
            )

            if response.status_code == 429:
                logger.warning(
                    "s2: Rate limited. "
                    "Get a free API key at "
                    "https://www.semanticscholar.org/product/api#api-key-form "
                    "and set S2_API_KEY environment variable."
                )
                break

            response.raise_for_status()
            data = response.json()

            papers = data.get("data", [])
            if not papers:
                # No more results available
                break

            all_papers.extend(self._convert_paper(p) for p in papers)
            offset += len(papers)

            # Check if we've fetched all available results
            total = data.get("total", 0)
            if offset >= total:
                break

        logger.debug(f"semantic_scholar: Retrieved {len(all_papers)} papers")
        return all_papers
    except requests.exceptions.RetryError:
        logger.warning(
            "s2: Rate limited after retries. "
            "Get a free API key at "
            "https://www.semanticscholar.org/product/api#api-key-form "
            "and set S2_API_KEY environment variable."
        )
        return all_papers
    except requests.exceptions.HTTPError as e:
        logger.warning("s2: HTTP error: %s", e)
        return all_papers
    except Exception as e:
        logger.warning("s2: %s", e)
        return all_papers
@

\subsubsection{Applying filters}

Semantic Scholar supports most of our filters natively via API parameters:
\begin{description}
\item[year] Supports [[year]] parameter with format [[YYYY]] or [[YYYY-YYYY]].
\item[open\_access] Use [[openAccessPdf]] field filter.
\item[venue] Supports [[venue]] parameter for exact venue match.
\item[min\_citations] Supports [[minCitationCount]] parameter.
\item[pub\_types] Supports [[publicationTypes]] parameter with values like
  [[JournalArticle]], [[Conference]], [[Review]], [[Book]], [[Dataset]].
\end{description}

<<semantic scholar apply filters>>=
if filters:
    # Year filter: Semantic Scholar accepts "YYYY" or "YYYY-YYYY"
    if filters.year:
        start, end = filters.year_range()
        if start and end:
            if start == end:
                params["year"] = str(start)
            else:
                params["year"] = f"{start}-{end}"
        elif start:
            # Open end: from start year onwards
            params["year"] = f"{start}-"
        elif end:
            # Open start: up to end year
            params["year"] = f"-{end}"

    # Open access: filter via openAccessPdf field
    if filters.open_access:
        params["openAccessPdf"] = ""

    # Venue filter
    if filters.venue:
        params["venue"] = filters.venue

    # Minimum citations
    if filters.min_citations is not None:
        params["minCitationCount"] = filters.min_citations

    # Publication types: map our normalized types to S2 values
    if filters.pub_types:
        s2_types = []
        type_mapping = {
            "article": "JournalArticle",
            "conference": "Conference",
            "review": "Review",
            "book": "Book",
            "preprint": "Preprint",
            "dataset": "Dataset",
        }
        for pt in filters.pub_types:
            if pt.lower() in type_mapping:
                s2_types.append(type_mapping[pt.lower()])
        if s2_types:
            params["publicationTypes"] = ",".join(s2_types)
@

\subsubsection{DOI lookup}

In addition to keyword search, we can fetch a single paper by its DOI.
This is useful for enriching papers that lack abstracts---if we have the DOI,
we can look up the paper directly in Semantic Scholar to get its abstract.

The API endpoint is [[https://api.semanticscholar.org/graph/v1/paper/DOI:{doi}]].
We request the same fields as the search API for consistency.

<<semantic scholar get paper by doi>>=
def get_paper_by_doi(self, doi: str) -> Paper | None:
    """
    Fetch a single paper by its DOI.

    Args:
        doi: The DOI to look up (e.g., "10.1145/1234567.1234568").

    Returns:
        Paper if found, None otherwise.
    """
    if not doi:
        return None

    # Check cache first
    cache_key = ("doi", doi)
    if cache_key in self._cache:
        logger.info(f"semantic_scholar: Cache hit for DOI {doi}")
        return self._cache[cache_key]
    
    logger.info(f"semantic_scholar: Cache miss for DOI {doi}, fetching from API")

    try:
        self._wait_for_rate_limit()

        headers = {}
        if self.api_key:
            headers["x-api-key"] = self.api_key

        # Use the paper endpoint with DOI: prefix
        url = f"https://api.semanticscholar.org/graph/v1/paper/DOI:{doi}"
        response = self._session.get(
            url,
            params={
                "fields": "title,authors,year,abstract,venue,externalIds,url,openAccessPdf",
            },
            headers=headers,
            timeout=30,
        )

        if response.status_code == 404:
            # Paper not found - cache the miss to avoid repeated lookups
            self._cache[cache_key] = None
            return None

        if response.status_code == 429:
            logger.warning("s2: Rate limited during DOI lookup")
            return None

        response.raise_for_status()
        paper_data = response.json()

        paper = self._convert_paper(paper_data)
        self._cache[cache_key] = paper
        return paper

    except requests.exceptions.HTTPError as e:
        logger.warning("s2: DOI lookup error: %s", e)
        return None
    except Exception as e:
        logger.warning("s2: DOI lookup failed: %s", e)
        return None
@

\subsubsection{Converting paper format}

Semantic Scholar returns papers as JSON dictionaries.
We convert them to our common [[Paper]] type:

<<convert semantic scholar paper>>=
def _convert_paper(self, paper_data: dict) -> Paper:
    """Convert a Semantic Scholar API response to our Paper type."""
    # Extract DOI from externalIds if available
    doi = None
    external_ids = paper_data.get("externalIds") or {}
    if external_ids:
        doi = external_ids.get("DOI")

    # Extract author names from list of author dicts
    authors = []
    for author in paper_data.get("authors") or []:
        if author.get("name"):
            authors.append(author["name"])

    # Extract PDF URL from openAccessPdf if available
    pdf_url = None
    open_access_pdf = paper_data.get("openAccessPdf") or {}
    if open_access_pdf:
        pdf_url = open_access_pdf.get("url")

    return Paper(
        title=paper_data.get("title") or "",
        authors=authors,
        year=paper_data.get("year"),
        doi=doi,
        abstract=paper_data.get("abstract"),
        venue=paper_data.get("venue"),
        url=paper_data.get("url"),
        pdf_url=pdf_url,
        source=self.name,
    )
@

\subsection{Auto-registration}

We register the provider when this module is imported:

<<semantic scholar provider>>=
# Register the provider on module import
register_provider(SemanticScholarProvider())
@

\subsection{Dependencies}

<<semantic scholar imports>>=
import time

import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
@

\subsection{Testing Semantic Scholar provider}

We mock the HTTP requests to avoid real network calls in tests.
First, we verify that the provider is always available since Semantic Scholar
does not require an API key:

<<test functions>>=
class TestSemanticScholarProvider:
    """Tests for the SemanticScholar provider."""

    def test_is_available_always_true(self):
        """Semantic Scholar is available without API key."""
        provider = SemanticScholarProvider(api_key=None)
        provider.api_key = None  # Ensure no key
        assert provider.is_available() is True

    def test_search_converts_papers(self):
        """search() converts API results to Paper objects."""
        provider = SemanticScholarProvider()

        # Mock API response
        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.json.return_value = {
            "total": 1,
            "data": [{
                "paperId": "abc123",
                "title": "Test Paper",
                "authors": [{"name": "Alice"}, {"name": "Bob"}],
                "year": 2024,
                "externalIds": {"DOI": "10.1234/test"},
                "abstract": "Test abstract",
                "venue": "Test Conference",
                "url": "https://example.com/paper",
                "openAccessPdf": {
                    "url": "https://arxiv.org/pdf/1234.5678.pdf",
                    "status": "GREEN",
                },
            }]
        }

        with patch.object(provider._session, "get", return_value=mock_response):
            results = provider.search("test query")

            assert len(results) == 1
            paper = results[0]
            assert isinstance(paper, Paper)
            assert paper.title == "Test Paper"
            assert paper.year == 2024
            assert paper.doi == "10.1234/test"
            assert paper.source == "s2"
            assert "Alice" in paper.authors
            assert "Bob" in paper.authors
            assert paper.pdf_url == "https://arxiv.org/pdf/1234.5678.pdf"

    def test_search_handles_empty_results(self):
        """search() returns empty list when no results."""
        provider = SemanticScholarProvider()

        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.json.return_value = {"total": 0, "data": []}

        with patch.object(provider._session, "get", return_value=mock_response):
            results = provider.search("obscure query")
            assert results == []

    def test_search_handles_api_error(self):
        """search() returns empty list on API error."""
        provider = SemanticScholarProvider()

        with patch.object(provider._session, "get") as mock_get:
            mock_get.side_effect = Exception("API error")

            results = provider.search("test")
            assert results == []

    def test_search_handles_rate_limit(self):
        """search() returns empty list on rate limit (429)."""
        provider = SemanticScholarProvider()

        mock_response = Mock()
        mock_response.status_code = 429

        with patch.object(provider._session, "get", return_value=mock_response):
            results = provider.search("test")
            assert results == []

    def test_rate_limiting_with_api_key(self):
        """Provider uses faster rate limit when API key is set."""
        provider = SemanticScholarProvider(api_key="test-key")
        assert provider._min_request_interval == 1.0

    def test_rate_limiting_without_api_key(self):
        """Provider uses slower rate limit without API key."""
        with patch.dict(os.environ, {}, clear=True):
            # Ensure no S2_API_KEY in environment
            if "S2_API_KEY" in os.environ:
                del os.environ["S2_API_KEY"]
            provider = SemanticScholarProvider(api_key=None)
            # Force api_key to None even if env var exists
            provider.api_key = None
            assert provider._min_request_interval == 3.0


class TestSemanticScholarGetPaperByDoi:
    """Tests for SemanticScholar DOI lookup."""

    def test_get_paper_by_doi_returns_paper(self):
        """get_paper_by_doi() returns a Paper when found."""
        provider = SemanticScholarProvider()

        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.json.return_value = {
            "paperId": "abc123",
            "title": "Test Paper",
            "authors": [{"name": "Alice"}],
            "year": 2024,
            "externalIds": {"DOI": "10.1234/test"},
            "abstract": "This is the abstract",
            "venue": "Test Conference",
            "url": "https://example.com/paper",
        }

        with patch.object(provider._session, "get", return_value=mock_response):
            paper = provider.get_paper_by_doi("10.1234/test")

            assert paper is not None
            assert paper.title == "Test Paper"
            assert paper.abstract == "This is the abstract"
            assert paper.doi == "10.1234/test"
            assert paper.source == "s2"

    def test_get_paper_by_doi_returns_none_on_404(self):
        """get_paper_by_doi() returns None when paper not found."""
        provider = SemanticScholarProvider()

        mock_response = Mock()
        mock_response.status_code = 404

        with patch.object(provider._session, "get", return_value=mock_response):
            paper = provider.get_paper_by_doi("10.1234/nonexistent")
            assert paper is None

    def test_get_paper_by_doi_returns_none_on_empty_doi(self):
        """get_paper_by_doi() returns None for empty DOI."""
        provider = SemanticScholarProvider()
        assert provider.get_paper_by_doi("") is None
        assert provider.get_paper_by_doi(None) is None

    def test_get_paper_by_doi_handles_rate_limit(self):
        """get_paper_by_doi() returns None on rate limit."""
        provider = SemanticScholarProvider()

        mock_response = Mock()
        mock_response.status_code = 429

        with patch.object(provider._session, "get", return_value=mock_response):
            paper = provider.get_paper_by_doi("10.1234/test")
            assert paper is None

    def test_get_paper_by_doi_caches_results(self):
        """get_paper_by_doi() caches successful lookups."""
        provider = SemanticScholarProvider()

        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.json.return_value = {
            "paperId": "abc123",
            "title": "Test Paper",
            "authors": [],
            "externalIds": {"DOI": "10.1234/test"},
        }

        with patch.object(provider._session, "get", return_value=mock_response) as mock_get:
            # First call
            paper1 = provider.get_paper_by_doi("10.1234/test")
            # Second call should use cache
            paper2 = provider.get_paper_by_doi("10.1234/test")

            assert mock_get.call_count == 1  # Only one API call
            assert paper1.title == paper2.title


class TestSemanticScholarPagination:
    """Tests for Semantic Scholar pagination."""

    def test_search_fetches_multiple_pages(self):
        """search() fetches multiple pages when limit exceeds page size."""
        provider = SemanticScholarProvider()

        # Create responses for two pages
        page1_response = Mock()
        page1_response.status_code = 200
        page1_response.json.return_value = {
            "total": 150,
            "data": [{"title": f"Paper {i}", "authors": []} for i in range(100)],
        }

        page2_response = Mock()
        page2_response.status_code = 200
        page2_response.json.return_value = {
            "total": 150,
            "data": [{"title": f"Paper {i}", "authors": []} for i in range(100, 150)],
        }

        with patch.object(
            provider._session, "get", side_effect=[page1_response, page2_response]
        ) as mock_get:
            results = provider.search("test query", limit=150)

            assert len(results) == 150
            assert mock_get.call_count == 2
            # Verify offset was used correctly
            calls = mock_get.call_args_list
            assert calls[0][1]["params"]["offset"] == 0
            assert calls[1][1]["params"]["offset"] == 100

    def test_search_stops_when_no_more_results(self):
        """search() stops pagination when API returns empty data."""
        provider = SemanticScholarProvider()

        page1_response = Mock()
        page1_response.status_code = 200
        page1_response.json.return_value = {
            "total": 50,
            "data": [{"title": f"Paper {i}", "authors": []} for i in range(50)],
        }

        page2_response = Mock()
        page2_response.status_code = 200
        page2_response.json.return_value = {"total": 50, "data": []}

        with patch.object(
            provider._session, "get", side_effect=[page1_response, page2_response]
        ):
            results = provider.search("test query", limit=200)

            # Should stop after getting 50 results even though limit is 200
            assert len(results) == 50

    def test_search_returns_partial_results_on_rate_limit(self):
        """search() returns collected results when rate limited mid-pagination."""
        provider = SemanticScholarProvider()

        page1_response = Mock()
        page1_response.status_code = 200
        page1_response.json.return_value = {
            "total": 200,
            "data": [{"title": f"Paper {i}", "authors": []} for i in range(100)],
        }

        page2_response = Mock()
        page2_response.status_code = 429  # Rate limited

        with patch.object(
            provider._session, "get", side_effect=[page1_response, page2_response]
        ):
            results = provider.search("test query", limit=200)

            # Should return partial results from first page
            assert len(results) == 100


class TestSemanticScholarIntegration:
    """Integration test for the SemanticScholar provider."""

    @pytest.mark.integration
    @pytest.mark.skip(reason="Skipped by default due to API rate limits")
    def test_real_api_call(self):
        """Make one real API call to verify the provider works."""
        try:
            provider = SemanticScholarProvider()
            papers = provider.search("machine learning", limit=1)
            assert len(papers) >= 1, "Provider returned no results"

            paper = papers[0]
            assert isinstance(paper, Paper)
            assert paper.title
            assert paper.source == "s2"
        except Exception as e:
            pytest.skip(f"Semantic Scholar API unavailable: {e}")
@


\section{OpenAlex Provider}
\label{sec:openalex}

OpenAlex is a free and open catalog of the world's scholarly works, authors,
venues, institutions, and concepts.
It covers over 250 million works and is fully open access.

\subsection{Package Choice}

We use the [[pyalex]] Python package which provides a clean interface to the
OpenAlex API. Unlike Semantic Scholar's package, [[pyalex]] returns raw
dictionaries rather than typed objects, requiring manual field extraction.

\subsection{Authentication}

No API key is required, but providing an email address (via [[OPENALEX_EMAIL]]
environment variable) gives access to the \enquote{polite pool}---a separate
pool of API resources with faster response times for identified users.

This is OpenAlex's approach to encouraging good API citizenship without
creating barriers to access.

\subsection{Rate Limiting and Pagination}

OpenAlex allows up to 10 requests per second.
We implement proactive rate limiting by tracking the time of the last request
and waiting if necessary before making a new one.
This ensures we stay well within limits even during batch operations like
paper enrichment.

The [[pyalex]] library provides a convenient [[paginate()]] method that
automatically handles pagination.
When the user requests more results than a single API page can return (200
results per page), we iterate through the paginated results until we reach
the requested limit.
This makes large result sets transparent to the caller.

\subsection{Provider implementation}

<<openalex provider>>=
class OpenAlexProvider:
    """Search provider for OpenAlex."""

    name = "openalex"
    MAX_LIMIT: int | None = None   # No documented maximum
    RATE_LIMIT = 0.1               # 10 requests/second = 0.1 seconds between requests

    def __init__(self, email: str | None = None):
        """
        Initialize the OpenAlex provider.

        Args:
            email: Optional email for polite pool access (faster responses).
                   If not provided, uses OPENALEX_EMAIL environment variable.
        """
        configured_email = email or os.environ.get("OPENALEX_EMAIL")
        if configured_email:
            pyalex.config.email = configured_email
        self._cache: dict = load_cache(self.name)
        register_cache(self.name, self._cache)
        self._last_request_time: float = 0.0

    def is_available(self) -> bool:
        """OpenAlex is always available (email is optional)."""
        return True

    def _wait_for_rate_limit(self) -> None:
        """Ensure minimum interval between requests."""
        elapsed = time.time() - self._last_request_time
        if elapsed < self.RATE_LIMIT:
            time.sleep(self.RATE_LIMIT - elapsed)
        self._last_request_time = time.time()

    @cachedmethod(lambda self: self._cache, key=lambda self, query, limit=100, filters=None: (query, limit, filters.cache_key() if filters else ""))
    def search(
        self,
        query: str,
        limit: int = 100,
        filters: SearchFilters | None = None,
    ) -> list[Paper]:
        """
        Search OpenAlex for papers matching the query.

        Automatically fetches multiple pages if the requested limit exceeds
        the API's per-request maximum (200 results per page).
        """
        try:
            all_papers: list[Paper] = []
            page_size = min(limit, 200)  # OpenAlex returns max 200 per page

            logger.debug(f"openalex: Searching for '{query}' with limit={limit}")
            self._wait_for_rate_limit()
            works = pyalex.Works().search(query)

            # Apply filters (OpenAlex supports most filters natively)
            <<openalex apply filters>>

            # Use pagination to fetch all results up to the limit
            for page in works.paginate(per_page=page_size):
                self._wait_for_rate_limit()
                for work in page:
                    all_papers.append(self._convert_work(work))
                    if len(all_papers) >= limit:
                        logger.debug(f"openalex: Retrieved {len(all_papers)} papers")
                        return all_papers

            logger.debug(f"openalex: Retrieved {len(all_papers)} papers")
            return all_papers
        except Exception as e:
            logger.warning("openalex: %s", e)
            return []

    <<openalex get paper by doi>>
    <<convert openalex work>>
@

\subsubsection{Applying filters}

OpenAlex supports filtering via its query builder. We chain filter calls:
\begin{description}
\item[year] Use [[publication_year]] filter.
\item[open\_access] Use [[is_oa]] filter.
\item[venue] Use [[primary_location.source.display_name]] for venue filtering.
\item[min\_citations] Use [[cited_by_count]] filter with [[>]] operator.
\item[pub\_types] Use [[type]] filter with OpenAlex type values.
\end{description}

<<openalex apply filters>>=
if filters:
    # Year filter
    if filters.year:
        start, end = filters.year_range()
        if start and end:
            if start == end:
                works = works.filter(publication_year=start)
            else:
                works = works.filter(publication_year=f"{start}-{end}")
        elif start:
            works = works.filter(publication_year=f">{start-1}")
        elif end:
            works = works.filter(publication_year=f"<{end+1}")

    # Open access filter
    if filters.open_access:
        works = works.filter(is_oa=True)

    # Venue filter (search in source display name)
    if filters.venue:
        works = works.filter(primary_location={"source": {"display_name": filters.venue}})

    # Minimum citations
    if filters.min_citations is not None:
        works = works.filter(cited_by_count=f">{filters.min_citations-1}")

    # Publication types: map our normalized types to OpenAlex values
    if filters.pub_types:
        oa_types = []
        type_mapping = {
            "article": "article",
            "conference": "proceedings-article",
            "review": "review",
            "book": "book",
            "preprint": "preprint",
            "dataset": "dataset",
        }
        for pt in filters.pub_types:
            if pt.lower() in type_mapping:
                oa_types.append(type_mapping[pt.lower()])
        if oa_types:
            # OpenAlex uses pipe for OR in filters
            works = works.filter(type="|".join(oa_types))
@

\subsubsection{Converting work format}

OpenAlex returns works as dictionaries.
We extract the relevant fields and convert to our [[Paper]] type:

<<convert openalex work>>=
def _convert_work(self, work: dict) -> Paper:
    """Convert an OpenAlex work to our Paper type."""
    # Extract author names from authorships
    authors = []
    for authorship in work.get("authorships", []):
        author_info = authorship.get("author", {})
        name = author_info.get("display_name")
        if name:
            authors.append(name)

    # Extract DOI (remove URL prefix if present)
    doi = work.get("doi")
    if doi and doi.startswith("https://doi.org/"):
        doi = doi[16:]

    # Extract venue from primary location
    venue = None
    primary_location = work.get("primary_location", {})
    if primary_location:
        source = primary_location.get("source", {})
        if source:
            venue = source.get("display_name")

    # Extract PDF URL from open_access or primary_location
    pdf_url = None
    open_access = work.get("open_access", {})
    if open_access:
        pdf_url = open_access.get("oa_url")
    if not pdf_url and primary_location:
        pdf_url = primary_location.get("pdf_url")

    return Paper(
        title=work.get("title", ""),
        authors=authors,
        year=work.get("publication_year"),
        doi=doi,
        abstract=work.get("abstract"),
        venue=venue,
        url=work.get("id"),
        pdf_url=pdf_url,
        source=self.name,
    )
@

\subsubsection{DOI lookup}

Like Semantic Scholar, we can fetch a single paper by its DOI.
OpenAlex uses DOI URLs as identifiers, so we query by the full DOI URL.

<<openalex get paper by doi>>=
def get_paper_by_doi(self, doi: str) -> Paper | None:
    """
    Fetch a single paper by its DOI.

    Args:
        doi: The DOI to look up (e.g., "10.1145/1234567.1234568").

    Returns:
        Paper if found, None otherwise.
    """
    if not doi:
        return None

    # Check cache first
    cache_key = ("doi", doi)
    if cache_key in self._cache:
        logger.info(f"openalex: Cache hit for DOI {doi}")
        return self._cache[cache_key]
    
    logger.info(f"openalex: Cache miss for DOI {doi}, fetching from API")

    try:
        self._wait_for_rate_limit()

        # OpenAlex uses full DOI URL as identifier
        doi_url = f"https://doi.org/{doi}"
        work = pyalex.Works()[doi_url]

        if not work:
            self._cache[cache_key] = None
            return None

        paper = self._convert_work(work)
        self._cache[cache_key] = paper
        return paper

    except Exception as e:
        logger.warning("openalex: DOI lookup failed: %s", e)
        return None
@

\subsection{Auto-registration}

<<openalex provider>>=
# Register the provider on module import
register_provider(OpenAlexProvider())
@

\subsection{Dependencies}

<<openalex imports>>=
import time

import pyalex
@

\subsection{Testing OpenAlex provider}

We test the OpenAlex provider with mocked API responses.
First, we verify that the provider is always available since OpenAlex
does not require an API key (email is optional for polite pool access):

<<test functions>>=
class TestOpenAlexProvider:
    """Tests for the OpenAlex provider."""

    def test_is_available_always_true(self):
        """OpenAlex is available without email."""
        provider = OpenAlexProvider(email=None)
        assert provider.is_available() is True

    def test_search_converts_works(self):
        """search() converts API results to Paper objects."""
        provider = OpenAlexProvider()

        # Create mock work (OpenAlex returns dicts)
        mock_work = {
            "title": "Test Paper",
            "authorships": [
                {"author": {"display_name": "Alice"}},
                {"author": {"display_name": "Bob"}},
            ],
            "publication_year": 2024,
            "doi": "https://doi.org/10.1234/test",
            "abstract": "Test abstract",
            "primary_location": {
                "source": {"display_name": "Test Journal"}
            },
            "id": "https://openalex.org/W123456",
        }

        with patch("pyalex.Works") as mock_works:
            # paginate() returns an iterator of pages, where each page is a list
            mock_works.return_value.search.return_value.paginate.return_value = [
                [mock_work]  # Single page with one result
            ]

            results = provider.search("test query")

            assert len(results) == 1
            paper = results[0]
            assert isinstance(paper, Paper)
            assert paper.title == "Test Paper"
            assert paper.authors == ["Alice", "Bob"]
            assert paper.year == 2024
            assert paper.doi == "10.1234/test"
            assert paper.venue == "Test Journal"
            assert paper.source == "openalex"

    def test_search_handles_empty_results(self):
        """search() returns empty list when no results."""
        provider = OpenAlexProvider()

        with patch("pyalex.Works") as mock_works:
            # paginate() returns an iterator of pages - empty list means no pages
            mock_works.return_value.search.return_value.paginate.return_value = []

            results = provider.search("obscure query")
            assert results == []

    def test_search_handles_api_error(self):
        """search() returns empty list on API error."""
        provider = OpenAlexProvider()

        with patch("pyalex.Works") as mock_works:
            mock_works.return_value.search.side_effect = Exception("API error")

            results = provider.search("test")
            assert results == []

    def test_handles_missing_fields(self):
        """search() handles works with missing optional fields."""
        provider = OpenAlexProvider()

        # Minimal work with only required fields
        mock_work = {
            "title": "Minimal Paper",
            "authorships": [],
        }

        with patch("pyalex.Works") as mock_works:
            # paginate() returns an iterator of pages
            mock_works.return_value.search.return_value.paginate.return_value = [
                [mock_work]  # Single page with one result
            ]

            results = provider.search("test")

            assert len(results) == 1
            paper = results[0]
            assert paper.title == "Minimal Paper"
            assert paper.authors == []
            assert paper.year is None
            assert paper.doi is None


class TestOpenAlexGetPaperByDoi:
    """Tests for OpenAlex DOI lookup."""

    def test_get_paper_by_doi_returns_paper(self):
        """get_paper_by_doi() returns a Paper when found."""
        provider = OpenAlexProvider()

        mock_work = {
            "title": "Test Paper",
            "authorships": [{"author": {"display_name": "Alice"}}],
            "publication_year": 2024,
            "doi": "https://doi.org/10.1234/test",
            "abstract": "This is the abstract",
            "id": "https://openalex.org/W123456",
        }

        with patch("pyalex.Works") as mock_works:
            mock_works.return_value.__getitem__.return_value = mock_work

            paper = provider.get_paper_by_doi("10.1234/test")

            assert paper is not None
            assert paper.title == "Test Paper"
            assert paper.abstract == "This is the abstract"
            assert paper.doi == "10.1234/test"
            assert paper.source == "openalex"

    def test_get_paper_by_doi_returns_none_when_not_found(self):
        """get_paper_by_doi() returns None when paper not found."""
        provider = OpenAlexProvider()

        with patch("pyalex.Works") as mock_works:
            mock_works.return_value.__getitem__.return_value = None

            paper = provider.get_paper_by_doi("10.1234/nonexistent")
            assert paper is None

    def test_get_paper_by_doi_returns_none_on_empty_doi(self):
        """get_paper_by_doi() returns None for empty DOI."""
        provider = OpenAlexProvider()
        assert provider.get_paper_by_doi("") is None
        assert provider.get_paper_by_doi(None) is None

    def test_get_paper_by_doi_handles_exception(self):
        """get_paper_by_doi() returns None on API error."""
        provider = OpenAlexProvider()

        with patch("pyalex.Works") as mock_works:
            mock_works.return_value.__getitem__.side_effect = Exception("API error")

            paper = provider.get_paper_by_doi("10.1234/test")
            assert paper is None

    def test_get_paper_by_doi_caches_results(self):
        """get_paper_by_doi() caches successful lookups."""
        provider = OpenAlexProvider()

        mock_work = {
            "title": "Test Paper",
            "authorships": [],
            "doi": "https://doi.org/10.1234/test",
        }

        with patch("pyalex.Works") as mock_works:
            mock_works.return_value.__getitem__.return_value = mock_work

            # First call
            paper1 = provider.get_paper_by_doi("10.1234/test")
            # Second call should use cache
            paper2 = provider.get_paper_by_doi("10.1234/test")

            assert mock_works.return_value.__getitem__.call_count == 1  # Only one API call
            assert paper1.title == paper2.title


class TestOpenAlexIntegration:
    """Integration test for the OpenAlex provider."""

    @pytest.mark.integration
    @pytest.mark.skip(reason="Skipped by default to avoid API calls")
    def test_real_api_call(self):
        """Make one real API call to verify the provider works."""
        provider = OpenAlexProvider()

        try:
            papers = provider.search("machine learning", limit=1)

            assert len(papers) >= 1, "Provider returned no results"

            paper = papers[0]
            assert isinstance(paper, Paper)
            assert paper.title
            assert paper.source == "openalex"
        except Exception as e:
            pytest.skip(f"OpenAlex API unavailable: {e}")
@


\section{DBLP Provider}

DBLP is a computer science bibliography database hosted at the University of
Trier and Schloss Dagstuhl.
It provides comprehensive coverage of major computer science journals and
conference proceedings.

\subsection{Package evaluation}

We evaluated several Python packages for DBLP access:

\begin{description}
\item[dblpy-lib (0.1.3)] Appeared well-maintained with a clean API
  ([[get_publications(q, max_results)]]).
  However, it requires [[requests>=2.27.1,<2.28.0]], which conflicts with our
  [[requests^2.31]] dependency.
\item[dblp (0.1.0)] Returns pandas DataFrames, which seemed convenient.
  However, it fails to install due to a bug in its setup script---it passes a
  list where a string is expected ([[TypeError: expected string or bytes-like
  object, got 'list']]).
\end{description}

Given these issues, we use the DBLP public API directly via [[requests]], which
we already depend on.
No API key is required.
The API returns JSON with publication data that we parse directly.

\subsection{Pagination}

The DBLP API supports pagination through the [[h]] (hits) and [[f]] (first)
parameters.
The [[h]] parameter specifies the maximum number of results per request (capped
at 1000), while [[f]] specifies the offset (0-indexed).
When the user requests more than 1000 results, we automatically fetch multiple
pages.

\subsection{Provider implementation}

<<dblp provider>>=
class DBLPProvider:
    """Search provider for DBLP."""

    name = "dblp"
    MAX_LIMIT = 1000  # DBLP API caps at 1000 results per request

    def __init__(self):
        """Initialize the DBLP provider."""
        self._cache: dict = load_cache(self.name)
        register_cache(self.name, self._cache)

    def is_available(self) -> bool:
        """DBLP is always available (no API key needed)."""
        return True

    @cachedmethod(lambda self: self._cache, key=lambda self, query, limit=100, filters=None: (query, limit, filters.cache_key() if filters else ""))
    def search(
        self,
        query: str,
        limit: int = 100,
        filters: SearchFilters | None = None,
    ) -> list[Paper]:
        """
        Search DBLP for papers matching the query.

        Automatically fetches multiple pages if the requested limit exceeds
        the API's per-request maximum (1000 results per page).
        """
        all_papers: list[Paper] = []
        offset = 0
        page_size = min(limit, 1000)  # DBLP API caps at 1000 per request

        try:
            # Build query with embedded filters
            effective_query = query
            <<dblp apply filters>>

            logger.debug(f"dblp: Searching for '{query}' with limit={limit}")

            while len(all_papers) < limit:
                # Calculate how many results we still need
                remaining = limit - len(all_papers)
                current_page_size = min(page_size, remaining)

                response = requests.get(
                    DBLP_API_URL,
                    params={
                        "q": effective_query,
                        "format": "json",
                        "h": current_page_size,
                        "f": offset,
                    },
                    timeout=30,
                )
                response.raise_for_status()
                data = response.json()

                hits = data.get("result", {}).get("hits", {}).get("hit", [])
                if not hits:
                    # No more results available
                    break

                all_papers.extend(self._convert_hit(hit) for hit in hits)
                offset += len(hits)

                # Check if we've fetched all available results
                total_str = data.get("result", {}).get("hits", {}).get("@total", "0")
                try:
                    total = int(total_str)
                except (ValueError, TypeError):
                    total = 0
                if offset >= total:
                    break

            logger.debug(f"dblp: Retrieved {len(all_papers)} papers")
            return all_papers
        except Exception as e:
            logger.warning("dblp: %s", e)
            return all_papers

    <<convert dblp hit>>
@

\subsubsection{Applying filters}

DBLP supports limited filtering via query syntax:
\begin{description}
\item[year] Embed [[year:YYYY]] or [[year:YYYY:YYYY]] in query.
\item[venue] Embed [[venue:X]] in query (matches venue abbreviation).
\item[pub\_types] Embed [[type:X]] in query (Conference, Journal, etc.).
\item[open\_access] Not supported---warn and ignore.
\item[min\_citations] Not supported---warn and ignore.
\end{description}

<<dblp apply filters>>=
if filters:
    # Year filter: DBLP uses "year:YYYY" or "year:YYYY:YYYY" syntax
    if filters.year:
        start, end = filters.year_range()
        if start and end:
            if start == end:
                effective_query += f" year:{start}"
            else:
                effective_query += f" year:{start}:{end}"
        elif start:
            # Open end: DBLP doesn't support this well, use current year
            import datetime
            current_year = datetime.datetime.now().year
            effective_query += f" year:{start}:{current_year}"
        elif end:
            # Open start: use 1900 as earliest
            effective_query += f" year:1900:{end}"

    # Venue filter
    if filters.venue:
        effective_query += f" venue:{filters.venue}"

    # Publication types: map to DBLP type syntax
    if filters.pub_types:
        dblp_types = []
        type_mapping = {
            "article": "Journal_Articles",
            "conference": "Conference_and_Workshop_Papers",
            "book": "Books_and_Theses",
        }
        for pt in filters.pub_types:
            if pt.lower() in type_mapping:
                dblp_types.append(type_mapping[pt.lower()])
            else:
                logger.warning(
                    "dblp: Publication type '%s' not supported, ignoring", pt
                )
        if dblp_types:
            effective_query += f" type:{dblp_types[0]}"  # DBLP only supports one type

    # Warn about unsupported filters
    if filters.open_access:
        logger.warning("dblp: Open access filter not supported, ignoring")
    if filters.min_citations is not None:
        logger.warning("dblp: Citation count filter not supported, ignoring")
@

\subsubsection{Converting publication format}

The DBLP API returns each publication as a JSON object with an [[info]] field
containing:
\begin{description}
\item[authors] Object with [[author]] array, each having a [[text]] field.
\item[title] The publication title.
\item[venue] The venue (conference or journal).
\item[year] Publication year as string.
\item[doi] The DOI (if available).
\item[ee] Electronic edition URL.
\item[url] URL to the DBLP record.
\end{description}

We convert each hit to our common [[Paper]] type:

<<convert dblp hit>>=
def _convert_hit(self, hit: dict) -> Paper:
    """Convert a DBLP API hit to our Paper type."""
    info = hit.get("info", {})

    # Extract year as integer if available
    year = None
    year_str = info.get("year")
    if year_str:
        try:
            year = int(year_str)
        except (ValueError, TypeError):
            pass

    # Extract author names from nested structure
    authors = []
    authors_data = info.get("authors", {}).get("author", [])
    # Handle single author (dict) vs multiple authors (list)
    if isinstance(authors_data, dict):
        authors_data = [authors_data]
    for author in authors_data:
        name = author.get("text") if isinstance(author, dict) else author
        if name:
            authors.append(name)

    return Paper(
        title=info.get("title", "") or "",
        authors=authors,
        year=year,
        doi=info.get("doi"),
        abstract=None,  # DBLP doesn't provide abstracts
        venue=info.get("venue"),
        url=info.get("url") or info.get("ee"),
        source=self.name,
    )
@

\subsection{Auto-registration}

<<dblp provider>>=
# Register the provider on module import
register_provider(DBLPProvider())
@

\subsection{Dependencies}

<<dblp imports>>=
import requests
@

<<dblp constants>>=
DBLP_API_URL = "https://dblp.org/search/publ/api"
@

\subsection{Testing DBLP provider}

We test the DBLP provider with mocked API responses.
First, we verify that the provider is always available since DBLP
does not require any credentials:

<<test functions>>=
class TestDBLPProvider:
    """Tests for the DBLP provider."""

    def test_is_available_always_true(self):
        """DBLP is available (no credentials needed)."""
        provider = DBLPProvider()
        assert provider.is_available() is True

    def test_search_converts_publications(self):
        """search() converts API results to Paper objects."""
        provider = DBLPProvider()

        mock_response = Mock()
        mock_response.json.return_value = {
            "result": {
                "hits": {
                    "hit": [{
                        "info": {
                            "title": "Test Paper",
                            "authors": {
                                "author": [
                                    {"text": "Alice"},
                                    {"text": "Bob"},
                                ]
                            },
                            "year": "2024",
                            "venue": "ICSE",
                            "doi": "10.1234/test",
                            "url": "https://dblp.org/rec/conf/icse/Test24",
                        }
                    }]
                }
            }
        }
        mock_response.raise_for_status = Mock()

        with patch("requests.get") as mock_get:
            mock_get.return_value = mock_response

            results = provider.search("test query")

            assert len(results) == 1
            paper = results[0]
            assert isinstance(paper, Paper)
            assert paper.title == "Test Paper"
            assert paper.authors == ["Alice", "Bob"]
            assert paper.year == 2024
            assert paper.doi == "10.1234/test"
            assert paper.venue == "ICSE"
            assert paper.source == "dblp"

    def test_search_handles_single_author(self):
        """search() handles single author as dict instead of list."""
        provider = DBLPProvider()

        mock_response = Mock()
        mock_response.json.return_value = {
            "result": {
                "hits": {
                    "hit": [{
                        "info": {
                            "title": "Solo Paper",
                            "authors": {"author": {"text": "Solo Author"}},
                            "year": "2024",
                        }
                    }]
                }
            }
        }
        mock_response.raise_for_status = Mock()

        with patch("requests.get") as mock_get:
            mock_get.return_value = mock_response

            results = provider.search("test")

            assert len(results) == 1
            assert results[0].authors == ["Solo Author"]

    def test_search_handles_empty_results(self):
        """search() returns empty list when no results."""
        provider = DBLPProvider()

        mock_response = Mock()
        mock_response.json.return_value = {
            "result": {"hits": {"hit": []}}
        }
        mock_response.raise_for_status = Mock()

        with patch("requests.get") as mock_get:
            mock_get.return_value = mock_response

            results = provider.search("obscure query")
            assert results == []

    def test_search_handles_api_error(self):
        """search() returns empty list on API error."""
        provider = DBLPProvider()

        with patch("requests.get") as mock_get:
            mock_get.side_effect = Exception("API error")

            results = provider.search("test")
            assert results == []

    def test_handles_missing_fields(self):
        """search() handles publications with missing optional fields."""
        provider = DBLPProvider()

        mock_response = Mock()
        mock_response.json.return_value = {
            "result": {
                "hits": {
                    "hit": [{
                        "info": {"title": "Minimal Paper"}
                    }]
                }
            }
        }
        mock_response.raise_for_status = Mock()

        with patch("requests.get") as mock_get:
            mock_get.return_value = mock_response

            results = provider.search("test")

            assert len(results) == 1
            paper = results[0]
            assert paper.title == "Minimal Paper"
            assert paper.authors == []
            assert paper.year is None
            assert paper.doi is None
            assert paper.abstract is None

    def test_respects_limit_parameter(self):
        """search() passes limit to API."""
        provider = DBLPProvider()

        mock_response = Mock()
        mock_response.json.return_value = {"result": {"hits": {"hit": [], "@total": "0"}}}
        mock_response.raise_for_status = Mock()

        with patch("requests.get") as mock_get:
            mock_get.return_value = mock_response

            provider.search("test", limit=50)

            mock_get.assert_called_once()
            call_args = mock_get.call_args
            assert call_args[1]["params"]["h"] == 50

    def test_fetches_multiple_pages(self):
        """search() fetches multiple pages when limit exceeds page size."""
        provider = DBLPProvider()

        # Create responses for two pages
        page1_response = Mock()
        page1_response.json.return_value = {
            "result": {
                "hits": {
                    "@total": "1500",
                    "hit": [{"info": {"title": f"Paper {i}"}} for i in range(1000)],
                }
            }
        }
        page1_response.raise_for_status = Mock()

        page2_response = Mock()
        page2_response.json.return_value = {
            "result": {
                "hits": {
                    "@total": "1500",
                    "hit": [{"info": {"title": f"Paper {i}"}} for i in range(1000, 1500)],
                }
            }
        }
        page2_response.raise_for_status = Mock()

        with patch("requests.get", side_effect=[page1_response, page2_response]) as mock_get:
            results = provider.search("test", limit=1500)

            assert len(results) == 1500
            assert mock_get.call_count == 2
            # Verify offset was used correctly
            calls = mock_get.call_args_list
            assert calls[0][1]["params"]["f"] == 0
            assert calls[1][1]["params"]["f"] == 1000

    def test_stops_when_total_reached(self):
        """search() stops pagination when all results have been fetched."""
        provider = DBLPProvider()

        page1_response = Mock()
        page1_response.json.return_value = {
            "result": {
                "hits": {
                    "@total": "50",
                    "hit": [{"info": {"title": f"Paper {i}"}} for i in range(50)],
                }
            }
        }
        page1_response.raise_for_status = Mock()

        with patch("requests.get", return_value=page1_response) as mock_get:
            results = provider.search("test", limit=200)

            # Should stop after first page since total (50) < limit (200)
            assert len(results) == 50
            assert mock_get.call_count == 1


class TestDBLPIntegration:
    """Integration test for the DBLP provider."""

    @pytest.mark.integration
    @pytest.mark.skip(reason="Skipped by default to avoid API calls")
    def test_real_api_call(self):
        """Make one real API call to verify the provider works."""
        provider = DBLPProvider()

        try:
            papers = provider.search("machine learning", limit=1)

            assert len(papers) >= 1, "Provider returned no results"

            paper = papers[0]
            assert isinstance(paper, Paper)
            assert paper.title
            assert paper.source == "dblp"
        except Exception as e:
            pytest.skip(f"DBLP API unavailable: {e}")
@


\section{Web of Science Provider}

Web of Science (WoS) is a comprehensive citation index covering scholarly
literature across sciences, social sciences, arts, and humanities.
It is maintained by Clarivate and requires institutional access or an API key.

\subsection{API options}

Clarivate provides two API tiers with different capabilities:

\begin{description}
\item[Web of Science Starter API] Basic search with bibliographic metadata.
  Has a free tier (50 requests/day) and institutional tiers (5,000--20,000
  requests/day).
  Maximum 50 results per page.
  Does \emph{not} include abstracts.
  Environment variable: [[WOS_STARTER_API_KEY]].
\item[Web of Science API Expanded] Full access with advanced features including
  abstracts, affiliations, funding data, and citation operations (related
  records, citing articles, cited references, citation reports).
  Requires institutional subscription.
  Maximum 100 results per page.
  Environment variable: [[WOS_EXPANDED_API_KEY]].
\end{description}

We support both APIs, preferring the Expanded API when available since it
provides richer metadata (including abstracts).
For backward compatibility, we also check the legacy [[WOS_API_KEY]] environment
variable and try it with both APIs.

The official Python client ([[wosstarter_python_client]]) is only available
from GitHub, not PyPI.
For consistency with our other providers, we use the REST APIs directly via
[[requests]].

\subsection{Provider implementation}

The provider detects which API tier is available based on configured environment
variables, preferring the Expanded API when both are available.

<<wos provider>>=
class WebOfScienceProvider:
    """Search provider for Web of Science.

    Supports both Starter and Expanded APIs, preferring Expanded when available.
    """

    name = "wos"

    <<wos init>>
    <<wos is available>>
    <<wos search>>
    <<wos convert starter hit>>
    <<wos safe get nested>>
    <<wos ensure list>>
    <<wos convert expanded hit>>
    <<wos extended methods>>
@

\subsubsection{Initialization}

During initialization, we check for API keys in the following order:
\begin{enumerate}
\item [[WOS_EXPANDED_API_KEY]]---use Expanded API
\item [[WOS_STARTER_API_KEY]]---use Starter API
\item [[WOS_API_KEY]] (legacy)---try both APIs
\end{enumerate}

When only the legacy [[WOS_API_KEY]] is set, we try it with the Expanded API
first.
If that fails during search, we fall back to the Starter API.

We use a sentinel value to distinguish between \enquote{not provided} (use
environment variable) and \enquote{explicitly set to None} (disable this key).
This distinction matters for testing: when we want to test the Starter API in
isolation, we must be able to explicitly disable the Expanded API even if the
[[WOS_EXPANDED_API_KEY]] environment variable is set.
Without a sentinel, passing [[None]] would be indistinguishable from not passing
the argument at all, and the constructor would fall back to the environment
variable.

<<wos init>>=
def __init__(
    self,
    starter_api_key: str | None | object = _WOS_NOT_PROVIDED,
    expanded_api_key: str | None | object = _WOS_NOT_PROVIDED,
):
    """
    Initialize the Web of Science provider.

    Args:
        starter_api_key: API key for WoS Starter API.
                         If not provided, uses WOS_STARTER_API_KEY env var.
                         Pass None explicitly to disable.
        expanded_api_key: API key for WoS Expanded API.
                          If not provided, uses WOS_EXPANDED_API_KEY env var.
                          Pass None explicitly to disable.

    The legacy WOS_API_KEY environment variable is also checked and tried
    with both APIs for backward compatibility.
    """
    # Use env var only if argument was not provided at all
    if starter_api_key is _WOS_NOT_PROVIDED:
        self._starter_key = os.environ.get("WOS_STARTER_API_KEY")
    else:
        self._starter_key = starter_api_key

    if expanded_api_key is _WOS_NOT_PROVIDED:
        self._expanded_key = os.environ.get("WOS_EXPANDED_API_KEY")
    else:
        self._expanded_key = expanded_api_key

    # Legacy support: WOS_API_KEY can be used with either API
    legacy_key = os.environ.get("WOS_API_KEY")
    if legacy_key:
        if not self._expanded_key:
            self._expanded_key = legacy_key
        if not self._starter_key:
            self._starter_key = legacy_key

    # Determine which API tier to use (prefer Expanded)
    if self._expanded_key:
        self._api_tier = "expanded"
        self._api_key = self._expanded_key
        self._api_url = WOS_EXPANDED_API_URL
        self.MAX_LIMIT = 100
    elif self._starter_key:
        self._api_tier = "starter"
        self._api_key = self._starter_key
        self._api_url = WOS_STARTER_API_URL
        self.MAX_LIMIT = 50
    else:
        self._api_tier = None
        self._api_key = None
        self._api_url = None
        self.MAX_LIMIT = 50

    self._cache: dict = load_cache(self.name)
    register_cache(self.name, self._cache)
@

\subsubsection{Availability check}

<<wos is available>>=
def is_available(self) -> bool:
    """Web of Science requires an API key to be configured."""
    return bool(self._api_key)
@

\subsubsection{Search dispatch}

The [[search]] method dispatches to the appropriate API-specific implementation
based on the detected tier.

<<wos search>>=
@cachedmethod(lambda self: self._cache, key=lambda self, query, limit=100, filters=None: (query, limit, filters.cache_key() if filters else ""))
def search(
    self,
    query: str,
    limit: int = 100,
    filters: SearchFilters | None = None,
) -> list[Paper]:
    """Search Web of Science for papers matching the query."""
    if not self._api_key:
        return []

    logger.debug(f"wos: Searching for '{query}' with limit={limit}")

    if self._api_tier == "expanded":
        return self._search_expanded(query, limit, filters)
    else:
        return self._search_starter(query, limit, filters)

<<wos format query>>
<<wos search starter>>
<<wos search expanded>>
@

\subsubsection{Query formatting}

Both WoS APIs require field tags in queries (e.g., [[TS=]], [[TI=]], [[AU=]]).
Plain queries like \enquote{machine learning} fail with a 400 Bad Request error.
We wrap plain queries with [[TS=()]] to search across topic fields (title,
abstract, keywords).
Queries that already contain field tags are preserved as-is.

<<wos format query>>=
def _format_query(self, query: str) -> str:
    """
    Format a query for the WoS APIs.

    Both Starter and Expanded APIs require field tags (e.g., TS=, TI=, AU=).
    If the query doesn't contain a field tag, wrap it with TS=()
    to search across topic fields (title, abstract, keywords).
    """
    if re.search(r'\b[A-Z]{2,3}=', query):
        return query
    return f"TS=({query})"
@

\subsubsection{Starter API search}

The Starter API uses a simpler request format with [[q]], [[limit]], and [[page]]
parameters.
The [[page]] parameter is 1-indexed.
When the user requests more than 50 results (the per-page maximum), we
automatically fetch multiple pages.

<<wos search starter>>=
def _search_starter(
    self,
    query: str,
    limit: int,
    filters: SearchFilters | None,
) -> list[Paper]:
    """
    Search using the WoS Starter API.

    Automatically fetches multiple pages if the requested limit exceeds
    the API's per-request maximum (50 results per page).
    """
    all_papers: list[Paper] = []
    page = 1
    page_size = 50  # Starter API max per page

    try:
        effective_query = self._format_query(query)
        <<wos apply filters starter>>

        while len(all_papers) < limit:
            # Calculate how many results we still need
            remaining = limit - len(all_papers)
            current_page_size = min(page_size, remaining)

            response = requests.get(
                WOS_STARTER_API_URL,
                params={
                    "q": effective_query,
                    "db": "WOS",
                    "limit": current_page_size,
                    "page": page,
                },
                headers={"X-ApiKey": self._api_key},
                timeout=30,
            )
            response.raise_for_status()
            data = response.json()

            hits = data.get("hits", [])
            if not hits:
                # No more results available
                break

            all_papers.extend(self._convert_starter_hit(hit) for hit in hits)
            page += 1

            # Check if we've fetched all available results
            metadata = data.get("metadata", {})
            total = metadata.get("total", 0)
            if len(all_papers) >= total:
                break

        return all_papers
    except Exception as e:
        logger.warning("wos (starter): %s", e)
        return all_papers
@

\subsubsection{Expanded API search}

The Expanded API uses [[usrQuery]], [[count]], and [[firstRecord]] parameters.
The [[firstRecord]] parameter is 1-indexed (the first record is 1, not 0).
It returns a more complex nested structure with full metadata including abstracts.
When the user requests more than 100 results (the per-page maximum), we
automatically fetch multiple pages.

<<wos search expanded>>=
def _search_expanded(
    self,
    query: str,
    limit: int,
    filters: SearchFilters | None,
) -> list[Paper]:
    """
    Search using the WoS Expanded API.

    Automatically fetches multiple pages if the requested limit exceeds
    the API's per-request maximum (100 results per page).
    """
    all_papers: list[Paper] = []
    first_record = 1  # 1-indexed
    page_size = 100  # Expanded API max per page

    try:
        effective_query = self._format_query(query)
        <<wos apply filters expanded>>

        while len(all_papers) < limit:
            # Calculate how many results we still need
            remaining = limit - len(all_papers)
            current_page_size = min(page_size, remaining)

            response = requests.get(
                WOS_EXPANDED_API_URL,
                params={
                    "databaseId": "WOS",
                    "usrQuery": effective_query,
                    "count": current_page_size,
                    "firstRecord": first_record,
                },
                headers={"X-ApiKey": self._api_key},
                timeout=30,
            )
            response.raise_for_status()
            data = response.json()

            # Navigate the nested structure to get records
            records = (
                data.get("Data", {})
                .get("Records", {})
                .get("records", {})
                .get("REC", [])
            )

            if not records:
                # No more results available
                break

            # Store query ID for potential citation report requests
            query_result = data.get("QueryResult", {})
            self._last_query_id = query_result.get("QueryID")

            all_papers.extend(self._convert_expanded_hit(rec) for rec in records)
            first_record += len(records)

            # Check if we've fetched all available results
            total = query_result.get("RecordsFound", 0)
            if first_record > total:
                break

        return all_papers
    except Exception as e:
        logger.warning("wos (expanded): %s", e)
        # Try falling back to Starter API if we have a starter key
        if self._starter_key and self._starter_key != self._expanded_key:
            logger.info("wos: Falling back to Starter API")
            self._api_tier = "starter"
            self._api_key = self._starter_key
            self._api_url = WOS_STARTER_API_URL
            self.MAX_LIMIT = 50
            return self._search_starter(query, limit, filters)
        return all_papers
@

\subsubsection{Applying filters}

Web of Science uses advanced query syntax for filtering.
Both APIs support the same query syntax, but the Expanded API also supports
some filters as separate parameters.

For the Starter API, we embed all filters in the query:

<<wos apply filters starter>>=
if filters:
    <<wos embed year filter>>
    <<wos embed venue filter>>
    <<wos embed pub types filter>>

    # Warn about unsupported filters in Starter API
    if filters.open_access:
        logger.warning("wos: Open access filter not supported in Starter API, ignoring")
    if filters.min_citations is not None:
        logger.warning("wos: Citation count filter not supported in Starter API, ignoring")
@

For the Expanded API, we also embed filters in the query but support additional
filter types:

<<wos apply filters expanded>>=
if filters:
    <<wos embed year filter>>
    <<wos embed venue filter>>
    <<wos embed pub types filter>>

    # Open access is supported in Expanded API via OA field
    if filters.open_access:
        effective_query += " AND OA=(gold OR green OR bronze)"

    # Citation count filtering requires local filtering after results
    if filters.min_citations is not None:
        logger.warning("wos: Citation count filter applied locally after search")
@

The shared filter embedding chunks:

<<wos embed year filter>>=
if filters.year:
    start, end = filters.year_range()
    if start and end:
        if start == end:
            effective_query += f" AND PY={start}"
        else:
            effective_query += f" AND PY={start}-{end}"
    elif start:
        import datetime
        current_year = datetime.datetime.now().year
        effective_query += f" AND PY={start}-{current_year}"
    elif end:
        effective_query += f" AND PY=1900-{end}"
@

<<wos embed venue filter>>=
if filters.venue:
    effective_query += f' AND SO="{filters.venue}"'
@

<<wos embed pub types filter>>=
if filters.pub_types:
    wos_types = []
    type_mapping = {
        "article": "Article",
        "conference": "Proceedings Paper",
        "review": "Review",
        "book": "Book",
    }
    for pt in filters.pub_types:
        if pt.lower() in type_mapping:
            wos_types.append(type_mapping[pt.lower()])
        else:
            logger.warning(
                "wos: Publication type '%s' not supported, ignoring", pt
            )
    if wos_types:
        types_query = " OR ".join(f'DT="{t}"' for t in wos_types)
        effective_query += f" AND ({types_query})"
@

\subsubsection{Converting Starter API response}

The WoS Starter API returns documents with a relatively flat structure:

<<wos convert starter hit>>=
def _convert_starter_hit(self, hit: dict) -> Paper:
    """Convert a WoS Starter API hit to our Paper type."""
    source = hit.get("source", {})
    year = None
    pub_year = source.get("publishYear")
    if pub_year:
        try:
            year = int(pub_year)
        except (ValueError, TypeError):
            pass

    authors = []
    names = hit.get("names", {})
    for author in names.get("authors", []):
        name = author.get("displayName")
        if name:
            authors.append(name)

    identifiers = hit.get("identifiers", {})
    doi = identifiers.get("doi")

    links = hit.get("links", {})
    url = links.get("record")

    return Paper(
        title=hit.get("title", "") or "",
        authors=authors,
        year=year,
        doi=doi,
        abstract=None,  # Not available in Starter API
        venue=source.get("sourceTitle"),
        url=url,
        source=self.name,
    )
@

\subsubsection{Safe nested dictionary access}

The WoS Expanded API can return unexpected types for nested fields.
When data is missing or malformed, the API may return strings instead of
dictionaries, causing [[AttributeError]] when we attempt to call [[.get()]]
on non-dict values.

This helper safely navigates nested dictionaries, returning a default value
if any level in the chain is not a dictionary.

<<wos safe get nested>>=
@staticmethod
def _safe_get_nested(obj: Any, *keys: str, default: Any = None) -> Any:
    """Safely navigate nested dicts, returning default if any level is not a dict."""
    for key in keys:
        if not isinstance(obj, dict):
            return default
        obj = obj.get(key, default)
    return obj
@

\subsubsection{Normalizing API response types}

The WoS API (like many XML-to-JSON APIs) inconsistently returns fields that
should be lists.
When a field contains multiple items, it returns a list of dicts.
When it contains a single item, it may return just that dict (not wrapped in a
list).
Sometimes it returns a string instead of a structured object.

This causes problems when we iterate over these fields: iterating over a string
yields individual characters, and calling [[.get()]] on a character raises
[[AttributeError]].

This helper normalizes values to lists for safe iteration:
\begin{description}
\item[List] returned as-is
\item[Dict] wrapped in a single-element list
\item[String or other] returns empty list (skip gracefully)
\end{description}

<<wos ensure list>>=
@staticmethod
def _ensure_list(value: Any) -> list:
    """Normalize a value to a list for safe iteration."""
    if isinstance(value, list):
        return value
    if isinstance(value, dict):
        return [value]
    return []
@

\subsubsection{Converting Expanded API response}

The WoS Expanded API returns documents with a deeply nested structure that
includes full metadata such as abstracts, affiliations, and citation counts.

<<wos convert expanded hit>>=
def _convert_expanded_hit(self, rec: dict) -> Paper:
    """Convert a WoS Expanded API record to our Paper type."""
    static_data = rec.get("static_data", {})
    summary = static_data.get("summary", {})
    fullrecord = static_data.get("fullrecord_metadata", {})

    # Extract title (look for type="item")
    title = ""
    titles = self._ensure_list(
        self._safe_get_nested(summary, "titles", "title", default=[])
    )
    for t in titles:
        if t.get("type") == "item":
            title = t.get("content", "")
            break
    if not title and titles:
        title = titles[0].get("content", "") if isinstance(titles[0], dict) else ""

    # Extract publication year
    year = None
    pub_year = self._safe_get_nested(summary, "pub_info", "pubyear", default=None)
    if pub_year:
        try:
            year = int(pub_year)
        except (ValueError, TypeError):
            pass

    # Extract authors
    authors = []
    names = self._ensure_list(
        self._safe_get_nested(summary, "names", "name", default=[])
    )
    for author in names:
        if author.get("role") == "author":
            name = author.get("display_name") or author.get("full_name")
            if name:
                authors.append(name)

    # Extract DOI from cluster_related identifiers
    doi = None
    dynamic_data = rec.get("dynamic_data", {})
    cluster = dynamic_data.get("cluster_related", {})
    identifiers = self._ensure_list(
        self._safe_get_nested(cluster, "identifiers", "identifier", default=[])
    )
    for ident in identifiers:
        if ident.get("type") == "doi":
            doi = ident.get("value")
            break

    # Extract abstract
    abstract = self._safe_get_nested(
        fullrecord, "abstracts", "abstract", "abstract_text", "p", default=None
    )

    # Extract venue (source title)
    venue = None
    for t in titles:
        if t.get("type") == "source":
            venue = t.get("content")
            break

    # Construct URL from UID
    uid = rec.get("UID", "")
    url = f"https://www.webofscience.com/wos/woscc/full-record/{uid}" if uid else None

    return Paper(
        title=title,
        authors=authors,
        year=year,
        doi=doi,
        abstract=abstract,
        venue=venue,
        url=url,
        source=self.name,
    )
@

\subsection{Extended functionality (Expanded API only)}

The Expanded API provides additional operations for citation analysis that are
not available in the Starter API.
These methods raise [[NotImplementedError]] if called when only the Starter API
is configured.

<<wos extended methods>>=
def _require_expanded_api(self, operation: str) -> None:
    """Raise an error if the Expanded API is not available."""
    if self._api_tier != "expanded":
        raise NotImplementedError(
            f"WoS {operation} requires the Expanded API. "
            f"Set WOS_EXPANDED_API_KEY environment variable."
        )

def get_related_records(self, uid: str, limit: int = 10) -> list[Paper]:
    """
    Find records that share cited references with the given paper.

    Args:
        uid: Web of Science unique identifier (e.g., "WOS:000270372400005")
        limit: Maximum number of related records to return (1-100)

    Returns:
        List of related papers

    Raises:
        NotImplementedError: If Expanded API is not configured
    """
    self._require_expanded_api("get_related_records")

    try:
        response = requests.get(
            f"{WOS_EXPANDED_API_URL}/related",
            params={
                "databaseId": "WOS",
                "uniqueId": uid,
                "count": min(limit, 100),
                "firstRecord": 1,
            },
            headers={"X-ApiKey": self._api_key},
            timeout=30,
        )
        response.raise_for_status()
        data = response.json()

        records = (
            data.get("Data", {})
            .get("Records", {})
            .get("records", {})
            .get("REC", [])
        )
        return [self._convert_expanded_hit(rec) for rec in records]
    except NotImplementedError:
        raise
    except Exception as e:
        logger.warning("wos: get_related_records failed: %s", e)
        return []

def get_citing_articles(self, uid: str, limit: int = 10) -> list[Paper]:
    """
    Find articles that cite the given paper.

    Args:
        uid: Web of Science unique identifier (e.g., "WOS:000270372400005")
        limit: Maximum number of citing articles to return (1-100)

    Returns:
        List of citing papers

    Raises:
        NotImplementedError: If Expanded API is not configured
    """
    self._require_expanded_api("get_citing_articles")

    try:
        response = requests.get(
            f"{WOS_EXPANDED_API_URL}/citing",
            params={
                "databaseId": "WOS",
                "uniqueId": uid,
                "count": min(limit, 100),
                "firstRecord": 1,
            },
            headers={"X-ApiKey": self._api_key},
            timeout=30,
        )
        response.raise_for_status()
        data = response.json()

        records = (
            data.get("Data", {})
            .get("Records", {})
            .get("records", {})
            .get("REC", [])
        )
        return [self._convert_expanded_hit(rec) for rec in records]
    except NotImplementedError:
        raise
    except Exception as e:
        logger.warning("wos: get_citing_articles failed: %s", e)
        return []

def get_cited_references(self, uid: str, limit: int = 10) -> list[dict]:
    """
    Find references cited by the given paper.

    Args:
        uid: Web of Science unique identifier (e.g., "WOS:000270372400005")
        limit: Maximum number of references to return (1-100)

    Returns:
        List of reference dictionaries with fields:
        - uid: WoS identifier (if available)
        - citedAuthor: Author name
        - citedTitle: Title of cited work
        - citedWork: Source/journal name
        - year: Publication year
        - doi: DOI (if available)
        - timesCited: Citation count

    Raises:
        NotImplementedError: If Expanded API is not configured
    """
    self._require_expanded_api("get_cited_references")

    try:
        response = requests.get(
            f"{WOS_EXPANDED_API_URL}/references",
            params={
                "databaseId": "WOS",
                "uniqueId": uid,
                "count": min(limit, 100),
                "firstRecord": 1,
            },
            headers={"X-ApiKey": self._api_key},
            timeout=30,
        )
        response.raise_for_status()
        data = response.json()

        # References have a different structure than regular records
        references = data.get("Data", [])
        return [
            {
                "uid": ref.get("UID"),
                "citedAuthor": ref.get("citedAuthor"),
                "citedTitle": ref.get("citedTitle"),
                "citedWork": ref.get("citedWork"),
                "year": ref.get("year"),
                "doi": ref.get("doi"),
                "timesCited": ref.get("timesCited"),
            }
            for ref in references
        ]
    except NotImplementedError:
        raise
    except Exception as e:
        logger.warning("wos: get_cited_references failed: %s", e)
        return []

def search_for_citation_report(
    self,
    query: str,
    limit: int = 100,
) -> dict | None:
    """
    Run a search and return citation statistics for the results.

    This method first performs a search to obtain a query ID, then fetches
    the citation report for that query.

    Args:
        query: Search query string
        limit: Maximum number of records to include in report (max 10000)

    Returns:
        Citation report dictionary with fields like:
        - TimesCited: Total citations
        - TimesCitedSansSelf: Citations excluding self-citations
        - AveragePerItem: Average citations per paper
        - AveragePerYear: Average citations per year
        - HValue: h-index for the result set
        - CitingYears: Breakdown by year
        Or None if the request fails

    Raises:
        NotImplementedError: If Expanded API is not configured
    """
    self._require_expanded_api("search_for_citation_report")

    try:
        # First, run a search to get a query ID
        response = requests.get(
            WOS_EXPANDED_API_URL,
            params={
                "databaseId": "WOS",
                "usrQuery": query,
                "count": min(limit, 100),
                "firstRecord": 1,
            },
            headers={"X-ApiKey": self._api_key},
            timeout=30,
        )
        response.raise_for_status()
        data = response.json()

        query_result = data.get("QueryResult", {})
        query_id = query_result.get("QueryID")

        if not query_id:
            logger.warning("wos: No query ID returned for citation report")
            return None

        # Now fetch the citation report
        report_response = requests.get(
            f"{WOS_EXPANDED_API_URL}/citation-report/{query_id}",
            params={"reportLevel": "WOS"},
            headers={"X-ApiKey": self._api_key},
            timeout=30,
        )
        report_response.raise_for_status()
        report_data = report_response.json()

        # Return the first (WOS) report level
        if report_data and len(report_data) > 0:
            return report_data[0]
        return None
    except NotImplementedError:
        raise
    except Exception as e:
        logger.warning("wos: search_for_citation_report failed: %s", e)
        return None
@

\subsection{Auto-registration}

<<wos provider>>=
# Register the provider on module import
register_provider(WebOfScienceProvider())
@

\subsection{Dependencies}

The WoS provider uses [[requests]] for HTTP calls and [[re]] for detecting
field tags in queries (the Starter API requires queries to have explicit field
tags like [[TS=]] or [[TI=]]).

<<wos imports>>=
import re
import requests
from typing import Any
@

We also define the API URL constants for both tiers, plus a sentinel value
for distinguishing \enquote{argument not provided} from \enquote{explicitly
set to None} in the constructor:

<<wos constants>>=
WOS_STARTER_API_URL = "https://api.clarivate.com/apis/wos-starter/v1/documents"
WOS_EXPANDED_API_URL = "https://wos-api.clarivate.com/api/wos"
_WOS_NOT_PROVIDED = object()  # Sentinel for "argument not passed"
@

\subsection{Testing Web of Science provider}

We test the WoS provider with mocked API responses, covering both API tiers.

\subsubsection{Configuration tests}

First, we verify the API tier selection and availability logic.
To isolate tests from environment variables, we explicitly pass [[None]] for
keys we don't want to use.

<<test functions>>=
class TestWebOfScienceProvider:
    """Tests for the Web of Science provider."""

    def test_is_available_false_without_key(self):
        """Web of Science is unavailable without any API key."""
        provider = WebOfScienceProvider(
            starter_api_key=None, expanded_api_key=None
        )
        assert provider.is_available() is False

    def test_is_available_true_with_starter_key(self):
        """Web of Science is available with Starter API key."""
        provider = WebOfScienceProvider(
            starter_api_key="test_key", expanded_api_key=None
        )
        assert provider.is_available() is True
        assert provider._api_tier == "starter"

    def test_is_available_true_with_expanded_key(self):
        """Web of Science is available with Expanded API key."""
        provider = WebOfScienceProvider(
            starter_api_key=None, expanded_api_key="test_key"
        )
        assert provider.is_available() is True
        assert provider._api_tier == "expanded"

    def test_prefers_expanded_when_both_keys_available(self):
        """Provider prefers Expanded API when both keys are set."""
        provider = WebOfScienceProvider(
            starter_api_key="starter_key",
            expanded_api_key="expanded_key",
        )
        assert provider._api_tier == "expanded"
        assert provider._api_key == "expanded_key"
        assert provider.MAX_LIMIT == 100

    def test_uses_starter_when_only_starter_key(self):
        """Provider uses Starter API when only starter key is set."""
        provider = WebOfScienceProvider(
            starter_api_key="starter_key", expanded_api_key=None
        )
        assert provider._api_tier == "starter"
        assert provider._api_key == "starter_key"
        assert provider.MAX_LIMIT == 50

    def test_legacy_wos_api_key_sets_both(self, monkeypatch):
        """Legacy WOS_API_KEY environment variable is tried with both APIs."""
        monkeypatch.setenv("WOS_API_KEY", "legacy_key")
        monkeypatch.delenv("WOS_STARTER_API_KEY", raising=False)
        monkeypatch.delenv("WOS_EXPANDED_API_KEY", raising=False)

        provider = WebOfScienceProvider()

        # Should prefer expanded (tries legacy key as expanded first)
        assert provider._api_tier == "expanded"
        assert provider._expanded_key == "legacy_key"
        assert provider._starter_key == "legacy_key"

    def test_search_returns_empty_without_api_key(self):
        """search() returns empty list when no API key is configured."""
        provider = WebOfScienceProvider(
            starter_api_key=None, expanded_api_key=None
        )
        results = provider.search("test")
        assert results == []
@

\subsubsection{Starter API tests}

Tests for the Starter API search functionality.
To ensure we test the Starter API specifically (not the Expanded API), we pass
[[expanded_api_key=None]] to override any environment variables that may be set.

<<test functions>>=
class TestWebOfScienceStarterAPI:
    """Tests for Web of Science Starter API."""

    def test_search_converts_documents(self):
        """search() converts Starter API results to Paper objects."""
        provider = WebOfScienceProvider(
            starter_api_key="test_key", expanded_api_key=None
        )

        mock_response = Mock()
        mock_response.json.return_value = {
            "hits": [{
                "title": "Test Paper",
                "names": {
                    "authors": [
                        {"displayName": "Alice Smith"},
                        {"displayName": "Bob Jones"},
                    ]
                },
                "source": {
                    "sourceTitle": "Nature",
                    "publishYear": "2024",
                },
                "identifiers": {"doi": "10.1234/test"},
                "links": {"record": "https://wos.com/record/123"},
            }]
        }
        mock_response.raise_for_status = Mock()

        with patch("requests.get") as mock_get:
            mock_get.return_value = mock_response

            results = provider.search("test query")

            assert len(results) == 1
            paper = results[0]
            assert isinstance(paper, Paper)
            assert paper.title == "Test Paper"
            assert paper.authors == ["Alice Smith", "Bob Jones"]
            assert paper.year == 2024
            assert paper.doi == "10.1234/test"
            assert paper.venue == "Nature"
            assert paper.abstract is None  # Starter API has no abstracts
            assert paper.source == "wos"

    def test_search_handles_empty_results(self):
        """search() returns empty list when no results."""
        provider = WebOfScienceProvider(
            starter_api_key="test_key", expanded_api_key=None
        )

        mock_response = Mock()
        mock_response.json.return_value = {"hits": []}
        mock_response.raise_for_status = Mock()

        with patch("requests.get") as mock_get:
            mock_get.return_value = mock_response

            results = provider.search("obscure query")
            assert results == []

    def test_search_handles_api_error(self):
        """search() returns empty list on API error."""
        provider = WebOfScienceProvider(
            starter_api_key="test_key", expanded_api_key=None
        )

        with patch("requests.get") as mock_get:
            mock_get.side_effect = Exception("API error")

            results = provider.search("test")
            assert results == []

    def test_handles_missing_fields(self):
        """search() handles documents with missing optional fields."""
        provider = WebOfScienceProvider(
            starter_api_key="test_key", expanded_api_key=None
        )

        mock_response = Mock()
        mock_response.json.return_value = {
            "hits": [{"title": "Minimal Paper"}]
        }
        mock_response.raise_for_status = Mock()

        with patch("requests.get") as mock_get:
            mock_get.return_value = mock_response

            results = provider.search("test")

            assert len(results) == 1
            paper = results[0]
            assert paper.title == "Minimal Paper"
            assert paper.authors == []
            assert paper.year is None
            assert paper.doi is None
            assert paper.abstract is None

    def test_fetches_multiple_pages(self):
        """Starter API fetches multiple pages when limit exceeds page size."""
        provider = WebOfScienceProvider(
            starter_api_key="test_key", expanded_api_key=None
        )

        # Create responses for two pages
        page1_response = Mock()
        page1_response.json.return_value = {
            "metadata": {"total": 75},
            "hits": [{"title": f"Paper {i}"} for i in range(50)],
        }
        page1_response.raise_for_status = Mock()

        page2_response = Mock()
        page2_response.json.return_value = {
            "metadata": {"total": 75},
            "hits": [{"title": f"Paper {i}"} for i in range(50, 75)],
        }
        page2_response.raise_for_status = Mock()

        with patch("requests.get", side_effect=[page1_response, page2_response]) as mock_get:
            results = provider.search("test", limit=75)

            assert len(results) == 75
            assert mock_get.call_count == 2
            # Verify page parameter increments
            calls = mock_get.call_args_list
            assert calls[0][1]["params"]["page"] == 1
            assert calls[1][1]["params"]["page"] == 2

    def test_sends_api_key_header(self):
        """search() sends API key in header."""
        provider = WebOfScienceProvider(
            starter_api_key="my_secret_key", expanded_api_key=None
        )

        mock_response = Mock()
        mock_response.json.return_value = {"hits": []}
        mock_response.raise_for_status = Mock()

        with patch("requests.get") as mock_get:
            mock_get.return_value = mock_response

            provider.search("test")

            call_args = mock_get.call_args
            assert call_args[1]["headers"]["X-ApiKey"] == "my_secret_key"

    def test_uses_starter_api_url(self):
        """Starter API uses correct URL."""
        provider = WebOfScienceProvider(
            starter_api_key="test_key", expanded_api_key=None
        )

        mock_response = Mock()
        mock_response.json.return_value = {"hits": []}
        mock_response.raise_for_status = Mock()

        with patch("requests.get") as mock_get:
            mock_get.return_value = mock_response

            provider.search("test")

            call_args = mock_get.call_args
            assert "wos-starter" in call_args[0][0]

    def test_formats_plain_query_with_ts_tag(self):
        """Plain queries are wrapped with TS= for topic search."""
        provider = WebOfScienceProvider(
            starter_api_key="test_key", expanded_api_key=None
        )

        mock_response = Mock()
        mock_response.json.return_value = {"hits": []}
        mock_response.raise_for_status = Mock()

        with patch("requests.get") as mock_get:
            mock_get.return_value = mock_response

            provider.search("machine learning")

            call_args = mock_get.call_args
            # Query should be wrapped with TS=()
            assert call_args[1]["params"]["q"] == "TS=(machine learning)"

    def test_preserves_existing_field_tags(self):
        """Queries with field tags are not double-wrapped."""
        provider = WebOfScienceProvider(
            starter_api_key="test_key", expanded_api_key=None
        )

        mock_response = Mock()
        mock_response.json.return_value = {"hits": []}
        mock_response.raise_for_status = Mock()

        with patch("requests.get") as mock_get:
            mock_get.return_value = mock_response

            provider.search("TI=(machine learning) AND AU=Smith")

            call_args = mock_get.call_args
            # Query should NOT be wrapped again
            assert call_args[1]["params"]["q"] == "TI=(machine learning) AND AU=Smith"
@

\subsubsection{Expanded API tests}

Tests for the Expanded API search functionality.
Like the Starter API, the Expanded API requires field tags in queries, so we
verify that plain queries are wrapped with [[TS=()]] and that queries with
existing field tags are preserved.

<<test functions>>=
class TestWebOfScienceExpandedAPI:
    """Tests for Web of Science Expanded API."""

    def test_search_converts_documents_with_abstracts(self):
        """search() converts Expanded API results including abstracts."""
        provider = WebOfScienceProvider(
            starter_api_key=None, expanded_api_key="test_key"
        )

        mock_response = Mock()
        mock_response.json.return_value = {
            "Data": {
                "Records": {
                    "records": {
                        "REC": [{
                            "UID": "WOS:000123456789",
                            "static_data": {
                                "summary": {
                                    "titles": {
                                        "title": [
                                            {"type": "item", "content": "Test Paper"},
                                            {"type": "source", "content": "Nature"},
                                        ]
                                    },
                                    "names": {
                                        "name": [
                                            {"role": "author", "display_name": "Alice Smith"},
                                            {"role": "author", "display_name": "Bob Jones"},
                                        ]
                                    },
                                    "pub_info": {"pubyear": 2024},
                                },
                                "fullrecord_metadata": {
                                    "abstracts": {
                                        "abstract": {
                                            "abstract_text": {"p": "This is the abstract."}
                                        }
                                    }
                                },
                            },
                            "dynamic_data": {
                                "cluster_related": {
                                    "identifiers": {
                                        "identifier": [
                                            {"type": "doi", "value": "10.1234/test"}
                                        ]
                                    }
                                }
                            },
                        }]
                    }
                }
            },
            "QueryResult": {"QueryID": 1, "RecordsFound": 1},
        }
        mock_response.raise_for_status = Mock()

        with patch("requests.get") as mock_get:
            mock_get.return_value = mock_response

            results = provider.search("test query")

            assert len(results) == 1
            paper = results[0]
            assert isinstance(paper, Paper)
            assert paper.title == "Test Paper"
            assert paper.authors == ["Alice Smith", "Bob Jones"]
            assert paper.year == 2024
            assert paper.doi == "10.1234/test"
            assert paper.venue == "Nature"
            assert paper.abstract == "This is the abstract."
            assert paper.source == "wos"

    def test_handles_malformed_nested_fields(self):
        """search() handles cases where nested fields are strings instead of dicts."""
        provider = WebOfScienceProvider(
            starter_api_key=None, expanded_api_key="test_key"
        )

        # This mock response simulates the bug: titles, names, identifiers
        # are strings instead of the expected dict structure
        mock_response = Mock()
        mock_response.json.return_value = {
            "Data": {
                "Records": {
                    "records": {
                        "REC": [{
                            "UID": "WOS:000123456789",
                            "static_data": {
                                "summary": {
                                    "titles": "Some malformed string",  # Should be dict
                                    "names": "Another malformed string",  # Should be dict
                                    "pub_info": "Not a dict either",  # Should be dict
                                },
                                "fullrecord_metadata": {
                                    "abstracts": "String instead of dict"
                                },
                            },
                            "dynamic_data": {
                                "cluster_related": {
                                    "identifiers": "String instead of dict"
                                }
                            },
                        }]
                    }
                }
            },
            "QueryResult": {"QueryID": 1, "RecordsFound": 1},
        }
        mock_response.raise_for_status = Mock()

        with patch("requests.get", return_value=mock_response):
            # This should not raise AttributeError: 'str' object has no attribute 'get'
            results = provider.search("test", limit=10)

            # Should still return a Paper with empty/None fields
            assert len(results) == 1
            paper = results[0]
            assert paper.title == ""
            assert paper.authors == []
            assert paper.year is None
            assert paper.doi is None
            assert paper.abstract is None
            assert paper.source == "wos"

    def test_fetches_multiple_pages(self):
        """Expanded API fetches multiple pages when limit exceeds page size."""
        provider = WebOfScienceProvider(
            starter_api_key=None, expanded_api_key="test_key"
        )

        # Create responses for two pages
        page1_response = Mock()
        page1_response.json.return_value = {
            "Data": {
                "Records": {
                    "records": {
                        "REC": [
                            {
                                "UID": f"WOS:{i}",
                                "static_data": {
                                    "summary": {
                                        "titles": {"title": [{"type": "item", "content": f"Paper {i}"}]},
                                        "names": {"name": []},
                                        "pub_info": {},
                                    },
                                    "fullrecord_metadata": {},
                                },
                                "dynamic_data": {},
                            }
                            for i in range(100)
                        ]
                    }
                }
            },
            "QueryResult": {"QueryID": 1, "RecordsFound": 150},
        }
        page1_response.raise_for_status = Mock()

        page2_response = Mock()
        page2_response.json.return_value = {
            "Data": {
                "Records": {
                    "records": {
                        "REC": [
                            {
                                "UID": f"WOS:{i}",
                                "static_data": {
                                    "summary": {
                                        "titles": {"title": [{"type": "item", "content": f"Paper {i}"}]},
                                        "names": {"name": []},
                                        "pub_info": {},
                                    },
                                    "fullrecord_metadata": {},
                                },
                                "dynamic_data": {},
                            }
                            for i in range(100, 150)
                        ]
                    }
                }
            },
            "QueryResult": {"QueryID": 1, "RecordsFound": 150},
        }
        page2_response.raise_for_status = Mock()

        with patch("requests.get", side_effect=[page1_response, page2_response]) as mock_get:
            results = provider.search("test", limit=150)

            assert len(results) == 150
            assert mock_get.call_count == 2
            # Verify firstRecord parameter increments
            calls = mock_get.call_args_list
            assert calls[0][1]["params"]["firstRecord"] == 1
            assert calls[1][1]["params"]["firstRecord"] == 101

    def test_uses_expanded_api_url(self):
        """Expanded API uses correct URL."""
        provider = WebOfScienceProvider(
            starter_api_key=None, expanded_api_key="test_key"
        )

        mock_response = Mock()
        mock_response.json.return_value = {
            "Data": {"Records": {"records": {"REC": []}}},
            "QueryResult": {"QueryID": 1},
        }
        mock_response.raise_for_status = Mock()

        with patch("requests.get") as mock_get:
            mock_get.return_value = mock_response

            provider.search("test")

            call_args = mock_get.call_args
            assert "wos-api.clarivate.com" in call_args[0][0]

    def test_uses_usrquery_parameter(self):
        """Expanded API uses usrQuery parameter with formatted query."""
        provider = WebOfScienceProvider(
            starter_api_key=None, expanded_api_key="test_key"
        )

        mock_response = Mock()
        mock_response.json.return_value = {
            "Data": {"Records": {"records": {"REC": []}}},
            "QueryResult": {"QueryID": 1},
        }
        mock_response.raise_for_status = Mock()

        with patch("requests.get") as mock_get:
            mock_get.return_value = mock_response

            provider.search("machine learning")

            call_args = mock_get.call_args
            # Plain queries are wrapped with TS=()
            assert call_args[1]["params"]["usrQuery"] == "TS=(machine learning)"

    def test_formats_plain_query_with_ts_tag(self):
        """Plain queries are wrapped with TS= for topic search."""
        provider = WebOfScienceProvider(
            starter_api_key=None, expanded_api_key="test_key"
        )

        mock_response = Mock()
        mock_response.json.return_value = {
            "Data": {"Records": {"records": {"REC": []}}},
            "QueryResult": {"QueryID": 1},
        }
        mock_response.raise_for_status = Mock()

        with patch("requests.get") as mock_get:
            mock_get.return_value = mock_response

            provider.search("deep learning")

            call_args = mock_get.call_args
            assert call_args[1]["params"]["usrQuery"] == "TS=(deep learning)"

    def test_preserves_existing_field_tags(self):
        """Queries with field tags are not double-wrapped."""
        provider = WebOfScienceProvider(
            starter_api_key=None, expanded_api_key="test_key"
        )

        mock_response = Mock()
        mock_response.json.return_value = {
            "Data": {"Records": {"records": {"REC": []}}},
            "QueryResult": {"QueryID": 1},
        }
        mock_response.raise_for_status = Mock()

        with patch("requests.get") as mock_get:
            mock_get.return_value = mock_response

            provider.search("TI=(neural networks) AND AU=LeCun")

            call_args = mock_get.call_args
            assert call_args[1]["params"]["usrQuery"] == "TI=(neural networks) AND AU=LeCun"

    def test_fallback_to_starter_on_expanded_error(self):
        """Falls back to Starter API when Expanded API fails."""
        provider = WebOfScienceProvider(
            starter_api_key="starter_key",
            expanded_api_key="expanded_key",
        )

        # First call (Expanded) fails, second call (Starter) succeeds
        expanded_error = Exception("Expanded API error")
        starter_response = Mock()
        starter_response.json.return_value = {
            "hits": [{"title": "Fallback Paper"}]
        }
        starter_response.raise_for_status = Mock()

        with patch("requests.get") as mock_get:
            mock_get.side_effect = [expanded_error, starter_response]

            results = provider.search("test")

            assert len(results) == 1
            assert results[0].title == "Fallback Paper"
            # Provider should have switched to starter tier
            assert provider._api_tier == "starter"
@

\subsubsection{Extended methods tests}

Tests for the Expanded API extended functionality:

<<test functions>>=
class TestWebOfScienceExtendedMethods:
    """Tests for WoS extended methods (Expanded API only)."""

    def test_extended_methods_require_expanded_api(self):
        """Extended methods raise NotImplementedError on Starter API."""
        provider = WebOfScienceProvider(
            starter_api_key="test_key", expanded_api_key=None
        )

        with pytest.raises(NotImplementedError):
            provider.get_related_records("WOS:000123")

        with pytest.raises(NotImplementedError):
            provider.get_citing_articles("WOS:000123")

        with pytest.raises(NotImplementedError):
            provider.get_cited_references("WOS:000123")

        with pytest.raises(NotImplementedError):
            provider.search_for_citation_report("test query")

    def test_get_related_records_returns_papers(self):
        """get_related_records() returns list of Paper objects."""
        provider = WebOfScienceProvider(
            starter_api_key=None, expanded_api_key="test_key"
        )

        mock_response = Mock()
        mock_response.json.return_value = {
            "Data": {
                "Records": {
                    "records": {
                        "REC": [{
                            "UID": "WOS:000999999",
                            "static_data": {
                                "summary": {
                                    "titles": {"title": [{"type": "item", "content": "Related Paper"}]},
                                    "names": {"name": []},
                                    "pub_info": {"pubyear": 2023},
                                },
                                "fullrecord_metadata": {},
                            },
                            "dynamic_data": {},
                        }]
                    }
                }
            },
        }
        mock_response.raise_for_status = Mock()

        with patch("requests.get") as mock_get:
            mock_get.return_value = mock_response

            results = provider.get_related_records("WOS:000123456")

            assert len(results) == 1
            assert results[0].title == "Related Paper"
            # Verify correct endpoint was called
            call_args = mock_get.call_args
            assert "/related" in call_args[0][0]

    def test_get_citing_articles_returns_papers(self):
        """get_citing_articles() returns list of Paper objects."""
        provider = WebOfScienceProvider(
            starter_api_key=None, expanded_api_key="test_key"
        )

        mock_response = Mock()
        mock_response.json.return_value = {
            "Data": {
                "Records": {
                    "records": {
                        "REC": [{
                            "UID": "WOS:000888888",
                            "static_data": {
                                "summary": {
                                    "titles": {"title": [{"type": "item", "content": "Citing Paper"}]},
                                    "names": {"name": []},
                                    "pub_info": {"pubyear": 2024},
                                },
                                "fullrecord_metadata": {},
                            },
                            "dynamic_data": {},
                        }]
                    }
                }
            },
        }
        mock_response.raise_for_status = Mock()

        with patch("requests.get") as mock_get:
            mock_get.return_value = mock_response

            results = provider.get_citing_articles("WOS:000123456")

            assert len(results) == 1
            assert results[0].title == "Citing Paper"
            call_args = mock_get.call_args
            assert "/citing" in call_args[0][0]

    def test_get_cited_references_returns_dicts(self):
        """get_cited_references() returns list of reference dicts."""
        provider = WebOfScienceProvider(
            starter_api_key=None, expanded_api_key="test_key"
        )

        mock_response = Mock()
        mock_response.json.return_value = {
            "Data": [{
                "UID": "WOS:000111111",
                "citedAuthor": "Smith, J",
                "citedTitle": "A Referenced Paper",
                "citedWork": "Science",
                "year": 2020,
                "doi": "10.1234/ref",
                "timesCited": 100,
            }],
        }
        mock_response.raise_for_status = Mock()

        with patch("requests.get") as mock_get:
            mock_get.return_value = mock_response

            results = provider.get_cited_references("WOS:000123456")

            assert len(results) == 1
            assert results[0]["citedAuthor"] == "Smith, J"
            assert results[0]["citedTitle"] == "A Referenced Paper"
            assert results[0]["doi"] == "10.1234/ref"
            call_args = mock_get.call_args
            assert "/references" in call_args[0][0]

    def test_search_for_citation_report_returns_stats(self):
        """search_for_citation_report() returns citation statistics."""
        provider = WebOfScienceProvider(
            starter_api_key=None, expanded_api_key="test_key"
        )

        # First response: search that returns query ID
        search_response = Mock()
        search_response.json.return_value = {
            "Data": {"Records": {"records": {"REC": []}}},
            "QueryResult": {"QueryID": 42, "RecordsFound": 10},
        }
        search_response.raise_for_status = Mock()

        # Second response: citation report
        report_response = Mock()
        report_response.json.return_value = [{
            "ReportLevel": "WOS",
            "TimesCited": "500",
            "TimesCitedSansSelf": "450",
            "AveragePerItem": "50.0",
            "HValue": "10",
        }]
        report_response.raise_for_status = Mock()

        with patch("requests.get") as mock_get:
            mock_get.side_effect = [search_response, report_response]

            result = provider.search_for_citation_report("machine learning")

            assert result is not None
            assert result["TimesCited"] == "500"
            assert result["HValue"] == "10"

            # Verify citation-report endpoint was called
            calls = mock_get.call_args_list
            assert len(calls) == 2
            assert "/citation-report/42" in calls[1][0][0]
@

\subsubsection{Integration tests}

<<test functions>>=
class TestWebOfScienceIntegration:
    """Integration test for the Web of Science provider."""

    @pytest.mark.integration
    @pytest.mark.skip(reason="Skipped by default; requires WOS API key")
    def test_real_api_call(self):
        """Make one real API call to verify the provider works."""
        import os

        # Try expanded key first, then starter, then legacy
        expanded_key = os.environ.get("WOS_EXPANDED_API_KEY")
        starter_key = os.environ.get("WOS_STARTER_API_KEY")
        legacy_key = os.environ.get("WOS_API_KEY")

        if not (expanded_key or starter_key or legacy_key):
            pytest.skip("No WOS API key environment variable set")

        provider = WebOfScienceProvider(
            expanded_api_key=expanded_key,
            starter_api_key=starter_key,
        )

        try:
            papers = provider.search("machine learning", limit=1)

            assert len(papers) >= 1, "Provider returned no results"

            paper = papers[0]
            assert isinstance(paper, Paper)
            assert paper.title
            assert paper.source == "wos"
        except Exception as e:
            pytest.skip(f"Web of Science API unavailable: {e}")
@


\section{IEEE Xplore Provider}

IEEE Xplore is a research database providing access to scientific and technical
content published by IEEE (Institute of Electrical and Electronics Engineers)
and its publishing partners.
It contains over 6 million documents including journals, conference proceedings,
technical standards, and books.

\subsection{API access}

IEEE provides a Metadata API for searching and retrieving bibliographic data.
The API requires registration and an API key, which can be obtained from the
IEEE Developer Portal.

The official Python SDK ([[xploreapi]]) is provided by IEEE but is not available
on PyPI---it must be downloaded from the Developer Portal.
For consistency with our other providers, we use the REST API directly via
[[requests]].

\subsection{Error handling}
\label{sec:ieee-error-handling}

A common problem with the IEEE API is that searches silently return no results
even when the API key is configured.
This typically happens because IEEE developer accounts become inactive after
periods of non-use, or when the developer needs to accept updated terms of
service.

Unlike a missing API key (which we detect via [[is_available()]]), an inactive
account still \emph{has} a valid key---it just won't work.
The API returns HTTP 403 with detailed error information in response headers:
\begin{description}
\item[[[X-Error-Detail-Header]]] Human-readable error message (e.g.,
  \enquote{Account Inactive}).
\item[[[X-Mashery-Error-Code]]] Machine-readable code (e.g.,
  [[ERR_403_DEVELOPER_INACTIVE]]).
\end{description}

We extract these details to provide actionable guidance rather than a generic
\enquote{403 Forbidden} message.
Users encountering this error should visit the IEEE Developer Portal to
reactivate their account.

<<ieee handle response errors>>=
if response.status_code != 200:
    error_detail = response.headers.get("X-Error-Detail-Header", "Unknown error")
    error_code = response.headers.get("X-Mashery-Error-Code", "")

    if "DEVELOPER_INACTIVE" in error_code or "Inactive" in error_detail:
        logger.warning(
            "ieee: Account inactive. "
            "Visit https://developer.ieee.org/ to reactivate your API key. "
            "Error: %s",
            error_detail,
        )
    elif response.status_code == 403:
        logger.warning(
            "ieee: Access denied (HTTP 403). Error: %s. "
            "Check your API key at https://developer.ieee.org/",
            error_detail,
        )
    elif response.status_code == 429:
        logger.warning(
            "ieee: Rate limited (HTTP 429). "
            "Wait before making more requests. Error: %s",
            error_detail,
        )
    else:
        logger.warning(
            "ieee: API error (HTTP %d): %s",
            response.status_code,
            error_detail,
        )
    response.raise_for_status()
@

\subsection{Pagination}

The IEEE Xplore API supports pagination through the [[max_records]] and
[[start_record]] parameters.
The [[max_records]] parameter specifies the maximum number of results per request
(capped at 200), while [[start_record]] specifies the offset (1-indexed).
When the user requests more than 200 results, we automatically fetch multiple
pages.

\subsection{Provider implementation}

<<ieee provider>>=
class IEEEXploreProvider:
    """Search provider for IEEE Xplore."""

    name = "ieee"
    MAX_LIMIT = 200  # IEEE API returns max 200 results per request

    def __init__(self, api_key: str | None = None):
        """
        Initialize the IEEE Xplore provider.

        Args:
            api_key: API key for IEEE Xplore API.
                     If not provided, uses IEEE_API_KEY environment variable.
        """
        self.api_key = api_key or os.environ.get("IEEE_API_KEY")
        self._cache: dict = load_cache(self.name)
        register_cache(self.name, self._cache)

    def is_available(self) -> bool:
        """IEEE Xplore requires an API key to be configured."""
        return bool(self.api_key)

    @cachedmethod(lambda self: self._cache, key=lambda self, query, limit=100, filters=None: (query, limit, filters.cache_key() if filters else ""))
    def search(
        self,
        query: str,
        limit: int = 100,
        filters: SearchFilters | None = None,
    ) -> list[Paper]:
        """
        Search IEEE Xplore for papers matching the query.

        Automatically fetches multiple pages if the requested limit exceeds
        the API's per-request maximum (200 results per page).
        """
        if not self.api_key:
            return []  # No API key configured

        logger.debug(f"ieee: Searching for '{query}' with limit={limit}")

        all_papers: list[Paper] = []
        start_record = 1  # 1-indexed
        page_size = 200  # IEEE API max per request

        try:
            while len(all_papers) < limit:
                # Calculate how many results we still need
                remaining = limit - len(all_papers)
                current_page_size = min(page_size, remaining)

                params = {
                    "querytext": query,
                    "max_records": current_page_size,
                    "start_record": start_record,
                    "apikey": self.api_key,
                }

                # Apply filters (IEEE supports some filters natively)
                <<ieee apply filters>>

                response = requests.get(
                    IEEE_API_URL,
                    params=params,
                    timeout=30,
                )
                <<ieee handle response errors>>
                data = response.json()

                articles = data.get("articles", [])
                if not articles:
                    # No more results available
                    break

                all_papers.extend(self._convert_article(article) for article in articles)
                start_record += len(articles)

                # Check if we've fetched all available results
                total = data.get("total_records", 0)
                if start_record > total:
                    break

            logger.debug(f"ieee: Retrieved {len(all_papers)} papers")
            return all_papers
        except requests.exceptions.HTTPError:
            # Already logged with details in handle response errors
            return all_papers
        except Exception as e:
            logger.warning("ieee: %s", e)
            return all_papers

    <<convert ieee article>>
@

\subsubsection{Applying filters}

IEEE Xplore supports some filters via API parameters:
\begin{description}
\item[year] Use [[start_year]] and [[end_year]] parameters.
\item[open\_access] Use [[open_access]] parameter (true/false).
\item[pub\_types] Use [[content_type]] parameter (Journals, Conferences, etc.).
\item[venue] Not supported natively---warn and ignore.
\item[min\_citations] Not supported---warn and ignore.
\end{description}

<<ieee apply filters>>=
if filters:
    # Year filter: IEEE uses start_year and end_year parameters
    if filters.year:
        start, end = filters.year_range()
        if start:
            params["start_year"] = start
        if end:
            params["end_year"] = end

    # Open access filter
    if filters.open_access:
        params["open_access"] = "true"

    # Publication types: map to IEEE content_type values
    if filters.pub_types:
        ieee_types = []
        type_mapping = {
            "article": "Journals",
            "conference": "Conferences",
            "book": "Books",
        }
        for pt in filters.pub_types:
            if pt.lower() in type_mapping:
                ieee_types.append(type_mapping[pt.lower()])
            else:
                logger.warning(
                    "ieee: Publication type '%s' not supported, ignoring", pt
                )
        if ieee_types:
            params["content_type"] = ieee_types[0]  # IEEE only supports one type

    # Warn about unsupported filters
    if filters.venue:
        logger.warning("ieee: Venue filter not supported, ignoring")
    if filters.min_citations is not None:
        logger.warning("ieee: Citation count filter not supported, ignoring")
@

\subsubsection{Converting article format}

The IEEE Xplore API returns articles with fields including:
\begin{description}
\item[title] The document title.
\item[authors] Object with [[authors]] array containing author details.
\item[abstract] Brief summary of the document.
\item[publication\_title] Journal or conference name.
\item[publication\_year] Year of publication.
\item[doi] Digital Object Identifier.
\item[html\_url] URL to the article page.
\end{description}

We convert each article to our common [[Paper]] type:

<<convert ieee article>>=
def _convert_article(self, article: dict) -> Paper:
    """Convert an IEEE Xplore article to our Paper type."""
    # Extract publication year
    year = None
    pub_year = article.get("publication_year")
    if pub_year:
        try:
            year = int(pub_year)
        except (ValueError, TypeError):
            pass

    # Extract author names from authors object
    authors = []
    authors_data = article.get("authors", {}).get("authors", [])
    for author in authors_data:
        name = author.get("full_name")
        if name:
            authors.append(name)

    # Prefer html_url, fall back to abstract_url
    url = article.get("html_url") or article.get("abstract_url")

    return Paper(
        title=article.get("title", "") or "",
        authors=authors,
        year=year,
        doi=article.get("doi"),
        abstract=article.get("abstract"),
        venue=article.get("publication_title"),
        url=url,
        source=self.name,
    )
@

\subsection{Auto-registration}

<<ieee provider>>=
# Register the provider on module import
register_provider(IEEEXploreProvider())
@

\subsection{Dependencies}

Since the provider uses the same [[requests]] library as DBLP and WoS, we only
need to define the API URL constant:

<<ieee constants>>=
IEEE_API_URL = "https://ieeexploreapi.ieee.org/api/v1/search/articles"
@

\subsection{Testing IEEE Xplore provider}

We test the IEEE Xplore provider with mocked API responses.
First, we verify the availability depends on having an API key configured:

<<test functions>>=
class TestIEEEXploreProvider:
    """Tests for the IEEE Xplore provider."""

    def test_is_available_false_without_key(self):
        """IEEE Xplore is unavailable without API key."""
        provider = IEEEXploreProvider(api_key=None)
        provider.api_key = None  # Ensure no key
        assert provider.is_available() is False

    def test_is_available_true_with_key(self):
        """IEEE Xplore is available with API key."""
        provider = IEEEXploreProvider(api_key="test_key")
        assert provider.is_available() is True

    def test_search_returns_empty_without_api_key(self):
        """search() returns empty list when no API key is configured."""
        provider = IEEEXploreProvider(api_key=None)
        results = provider.search("test")
        assert results == []

    def test_search_converts_articles(self):
        """search() converts API results to Paper objects."""
        provider = IEEEXploreProvider(api_key="test_key")

        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.json.return_value = {
            "articles": [{
                "title": "Test Paper on Neural Networks",
                "authors": {
                    "authors": [
                        {"full_name": "Alice Smith", "author_order": 1},
                        {"full_name": "Bob Jones", "author_order": 2},
                    ]
                },
                "abstract": "This paper presents...",
                "publication_title": "IEEE Trans. Neural Networks",
                "publication_year": "2024",
                "doi": "10.1109/TNN.2024.1234567",
                "html_url": "https://ieeexplore.ieee.org/document/1234567",
            }]
        }
        mock_response.raise_for_status = Mock()

        with patch("requests.get") as mock_get:
            mock_get.return_value = mock_response

            results = provider.search("neural networks")

            assert len(results) == 1
            paper = results[0]
            assert isinstance(paper, Paper)
            assert paper.title == "Test Paper on Neural Networks"
            assert paper.authors == ["Alice Smith", "Bob Jones"]
            assert paper.year == 2024
            assert paper.doi == "10.1109/TNN.2024.1234567"
            assert paper.abstract == "This paper presents..."
            assert paper.venue == "IEEE Trans. Neural Networks"
            assert paper.source == "ieee"

    def test_search_handles_empty_results(self):
        """search() returns empty list when no results."""
        provider = IEEEXploreProvider(api_key="test_key")

        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.json.return_value = {"articles": []}
        mock_response.raise_for_status = Mock()

        with patch("requests.get") as mock_get:
            mock_get.return_value = mock_response

            results = provider.search("obscure query")
            assert results == []

    def test_search_handles_api_error(self):
        """search() returns empty list on API error."""
        provider = IEEEXploreProvider(api_key="test_key")

        with patch("requests.get") as mock_get:
            mock_get.side_effect = Exception("API error")

            results = provider.search("test")
            assert results == []

    def test_handles_missing_fields(self):
        """search() handles articles with missing optional fields."""
        provider = IEEEXploreProvider(api_key="test_key")

        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.json.return_value = {
            "articles": [{"title": "Minimal Paper"}]
        }
        mock_response.raise_for_status = Mock()

        with patch("requests.get") as mock_get:
            mock_get.return_value = mock_response

            results = provider.search("test")

            assert len(results) == 1
            paper = results[0]
            assert paper.title == "Minimal Paper"
            assert paper.authors == []
            assert paper.year is None
            assert paper.doi is None
            assert paper.abstract is None

    def test_caps_limit_at_200(self):
        """search() caps limit at IEEE's maximum of 200."""
        provider = IEEEXploreProvider(api_key="test_key")

        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.json.return_value = {"articles": []}
        mock_response.raise_for_status = Mock()

        with patch("requests.get") as mock_get:
            mock_get.return_value = mock_response

            provider.search("test", limit=500)

            call_args = mock_get.call_args
            assert call_args[1]["params"]["max_records"] == 200

    def test_sends_api_key_as_parameter(self):
        """search() sends API key as query parameter."""
        provider = IEEEXploreProvider(api_key="my_ieee_key")

        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.json.return_value = {"articles": []}
        mock_response.raise_for_status = Mock()

        with patch("requests.get") as mock_get:
            mock_get.return_value = mock_response

            provider.search("test")

            call_args = mock_get.call_args
            assert call_args[1]["params"]["apikey"] == "my_ieee_key"

    def test_uses_abstract_url_as_fallback(self):
        """search() uses abstract_url when html_url is not available."""
        provider = IEEEXploreProvider(api_key="test_key")

        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.json.return_value = {
            "articles": [{
                "title": "Paper",
                "abstract_url": "https://ieeexplore.ieee.org/abstract/1234",
            }]
        }
        mock_response.raise_for_status = Mock()

        with patch("requests.get") as mock_get:
            mock_get.return_value = mock_response

            results = provider.search("test")

            assert results[0].url == "https://ieeexplore.ieee.org/abstract/1234"

    def test_search_handles_inactive_account(self):
        """search() returns empty list and logs warning for inactive account."""
        provider = IEEEXploreProvider(api_key="test_key")

        mock_response = Mock()
        mock_response.status_code = 403
        mock_response.headers = {
            "X-Error-Detail-Header": "Account Inactive",
            "X-Mashery-Error-Code": "ERR_403_DEVELOPER_INACTIVE",
        }
        mock_response.raise_for_status.side_effect = requests.exceptions.HTTPError(
            "403 Client Error: Forbidden"
        )

        with patch("requests.get") as mock_get:
            mock_get.return_value = mock_response

            results = provider.search("test")

            assert results == []

    def test_search_handles_rate_limit(self):
        """search() returns empty list and logs warning for rate limiting."""
        provider = IEEEXploreProvider(api_key="test_key")

        mock_response = Mock()
        mock_response.status_code = 429
        mock_response.headers = {
            "X-Error-Detail-Header": "Rate limit exceeded",
            "X-Mashery-Error-Code": "ERR_429_RATE_LIMIT_EXCEEDED",
        }
        mock_response.raise_for_status.side_effect = requests.exceptions.HTTPError(
            "429 Client Error: Too Many Requests"
        )

        with patch("requests.get") as mock_get:
            mock_get.return_value = mock_response

            results = provider.search("test")

            assert results == []

    def test_fetches_multiple_pages(self):
        """IEEE fetches multiple pages when limit exceeds page size."""
        provider = IEEEXploreProvider(api_key="test_key")

        # Create responses for two pages (IEEE returns max 200 per page)
        page1_response = Mock()
        page1_response.status_code = 200
        page1_response.json.return_value = {
            "total_records": 350,
            "articles": [
                {
                    "title": f"Paper {i}",
                    "authors": {"authors": []},
                    "publication_year": "2024",
                }
                for i in range(200)
            ],
        }
        page1_response.raise_for_status = Mock()

        page2_response = Mock()
        page2_response.status_code = 200
        page2_response.json.return_value = {
            "total_records": 350,
            "articles": [
                {
                    "title": f"Paper {i}",
                    "authors": {"authors": []},
                    "publication_year": "2024",
                }
                for i in range(200, 350)
            ],
        }
        page2_response.raise_for_status = Mock()

        with patch("requests.get", side_effect=[page1_response, page2_response]) as mock_get:
            results = provider.search("test", limit=350)

            assert len(results) == 350
            assert mock_get.call_count == 2
            # Verify start_record parameter increments (1-indexed)
            calls = mock_get.call_args_list
            assert calls[0][1]["params"]["start_record"] == 1
            assert calls[1][1]["params"]["start_record"] == 201


class TestIEEEXploreIntegration:
    """Integration test for the IEEE Xplore provider."""

    @pytest.mark.integration
    @pytest.mark.skip(reason="Skipped by default; requires IEEE_API_KEY")
    def test_real_api_call(self):
        """Make one real API call to verify the provider works."""
        import os

        api_key = os.environ.get("IEEE_API_KEY")
        if not api_key:
            pytest.skip("IEEE_API_KEY environment variable not set")

        provider = IEEEXploreProvider(api_key=api_key)

        try:
            papers = provider.search("machine learning", limit=1)

            assert len(papers) >= 1, "Provider returned no results"

            paper = papers[0]
            assert isinstance(paper, Paper)
            assert paper.title
            assert paper.source == "ieee"
        except Exception as e:
            pytest.skip(f"IEEE Xplore API unavailable: {e}")
@


\section{Dependencies}

We organize imports so each provider's dependencies are co-located with
its implementation.
If a provider is removed, its dependencies go with it.

<<imports>>=
import logging
import os
from typing import Protocol

from cachetools import cachedmethod

from scholar import Paper, SearchFilters
from scholar.cache import load_cache, register_cache
<<semantic scholar imports>>
<<openalex imports>>
<<dblp imports>>
<<wos imports>>

logger = logging.getLogger(__name__)
@

<<constants>>=
# Provider constants
DEFAULT_LIMIT = 100
<<dblp constants>>
<<wos constants>>
<<ieee constants>>
@
