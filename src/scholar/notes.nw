\chapter{Paper Notes and Decision Persistence}
\label{notes-module}

\section{Introduction}

When conducting systematic literature reviews, researchers need to:
\begin{itemize}
\item Take notes on individual papers that persist across sessions
\item Remember previous keep/discard decisions when re-running the same search
\item Include notes in generated reports
\end{itemize}

This module provides persistent storage for paper notes (global, per-paper) and
search decisions (per-query). Unlike the search cache which stores API results,
this module stores user-generated content that should never be automatically
cleared.

\section{Design overview}

We store data in the user's data directory (not cache directory):
\begin{description}
\item[Linux] \texttt{\textasciitilde/.local/share/scholar/}
\item[macOS] \texttt{\textasciitilde/Library/Application Support/scholar/}
\item[Windows] \texttt{C:\textbackslash Users\textbackslash <user>\textbackslash AppData\textbackslash Local\textbackslash scholar\textbackslash}
\end{description}

The storage format is JSON for human readability and easier debugging.

Paper identity uses DOI when available, otherwise a hash of the lowercase title
combined with the first author's last name. This provides better disambiguation
than title alone.

\section{Module structure}

<<[[notes.py]]>>=
"""
Persistent storage for paper notes and search decisions.

Notes are global (per-paper) and persist forever.
Search decisions are per-query and help restore previous review state.
"""

<<notes imports>>
<<notes constants>>
<<notes data structures>>
<<paper identity functions>>
<<data directory functions>>
<<paper notes functions>>
<<search decisions functions>>
<<editor integration>>
@


\chapter{Data Structures}

We define dataclasses to represent notes and decisions. These are serialized
to JSON for storage.

<<notes imports>>=
from dataclasses import dataclass, field, asdict
from datetime import datetime
from pathlib import Path
from typing import Any
import hashlib
import json
import os

import click
import platformdirs
@

<<notes data structures>>=
@dataclass
class PaperNote:
    """
    A note attached to a paper.
    
    Notes are identified by paper_id (DOI or hash of title+author).
    The title is stored for display purposes in the notes browser.
    """
    paper_id: str
    title: str
    note: str
    created_at: str
    updated_at: str
    
    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary for JSON serialization."""
        return asdict(self)
    
    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> "PaperNote":
        """Create from dictionary."""
        return cls(**data)


@dataclass
class ReviewDecisionRecord:
    """
    A single review decision for a paper within a search.
    
    Stores the status (keep/discard/pending), motivation, and paper details
    needed for reconstruction when merging sessions.
    """
    status: str  # "kept", "discarded", "pending"
    motivation: str = ""
    # Paper details for reconstruction
    title: str = ""
    authors: list[str] = field(default_factory=list)
    year: int | None = None
    doi: str | None = None
    abstract: str | None = None
    venue: str | None = None
    url: str | None = None
    pdf_url: str | None = None
    provider: str = ""
    
    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary for JSON serialization."""
        return asdict(self)
    
    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> "ReviewDecisionRecord":
        """Create from dictionary."""
        # Handle older records that don't have paper details
        return cls(
            status=data.get("status", "pending"),
            motivation=data.get("motivation", ""),
            title=data.get("title", ""),
            authors=data.get("authors", []),
            year=data.get("year"),
            doi=data.get("doi"),
            abstract=data.get("abstract"),
            venue=data.get("venue"),
            url=data.get("url"),
            pdf_url=data.get("pdf_url"),
            provider=data.get("provider", ""),
        )


@dataclass
class SearchDecisions:
    """
    All review decisions for a specific search query.
    
    Decisions are keyed by paper_id, allowing lookup when the same
    paper appears in a repeated search.
    """
    query: str
    query_hash: str
    decisions: dict[str, ReviewDecisionRecord] = field(default_factory=dict)
    timestamp: str = ""
    
    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary for JSON serialization."""
        return {
            "query": self.query,
            "query_hash": self.query_hash,
            "decisions": {
                pid: dec.to_dict() for pid, dec in self.decisions.items()
            },
            "timestamp": self.timestamp,
        }
    
    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> "SearchDecisions":
        """Create from dictionary."""
        decisions = {
            pid: ReviewDecisionRecord.from_dict(dec)
            for pid, dec in data.get("decisions", {}).items()
        }
        return cls(
            query=data["query"],
            query_hash=data["query_hash"],
            decisions=decisions,
            timestamp=data.get("timestamp", ""),
        )
@


\chapter{Paper Identity}

To associate notes with papers across different searches and sessions, we need
a stable identifier. DOI is ideal when available, but many papers lack DOIs.
For papers without DOIs, we create a hash from the title and first author's
last name.

<<notes constants>>=
# File names for persistent storage
NOTES_FILE = "paper_notes.json"
DECISIONS_DIR = "search_decisions"
@

<<paper identity functions>>=
def get_paper_id(paper: Any) -> str:
    """
    Generate a stable identifier for a paper.
    
    Uses DOI if available, otherwise SHA256 hash of normalized
    title + first author's last name.
    
    Args:
        paper: A Paper object with title, authors, and optional doi.
    
    Returns:
        A string identifier unique to this paper.
    """
    if paper.doi:
        return f"doi:{paper.doi.lower()}"
    
    # Normalize title
    title_normalized = paper.title.lower().strip()
    
    # Get first author's last name if available
    author_part = ""
    if paper.authors:
        first_author = paper.authors[0]
        # Extract last name (last word of author name)
        last_name = first_author.split()[-1].lower() if first_author else ""
        author_part = last_name
    
    # Create hash
    content = f"{title_normalized}|{author_part}"
    hash_value = hashlib.sha256(content.encode()).hexdigest()[:16]
    return f"hash:{hash_value}"


def get_query_hash(query: str) -> str:
    """
    Generate a hash for a search query.
    
    Used as filename for storing search decisions.
    """
    normalized = query.lower().strip()
    return hashlib.sha256(normalized.encode()).hexdigest()[:16]
@

\section{Testing paper identity}

<<test paper identity>>=
class TestPaperIdentity:
    """Tests for paper identity functions."""

    def test_doi_takes_precedence(self):
        """Papers with DOI use DOI as identifier."""
        paper = Mock()
        paper.doi = "10.1234/TEST"
        paper.title = "Test Paper"
        paper.authors = ["John Doe"]
        
        pid = get_paper_id(paper)
        assert pid == "doi:10.1234/test"

    def test_hash_without_doi(self):
        """Papers without DOI use hash of title+author."""
        paper = Mock()
        paper.doi = None
        paper.title = "Test Paper"
        paper.authors = ["John Doe"]
        
        pid = get_paper_id(paper)
        assert pid.startswith("hash:")
        assert len(pid) == 5 + 16  # "hash:" + 16 hex chars

    def test_same_paper_same_id(self):
        """Same title+author produces same ID."""
        paper1 = Mock()
        paper1.doi = None
        paper1.title = "Test Paper"
        paper1.authors = ["John Doe"]
        
        paper2 = Mock()
        paper2.doi = None
        paper2.title = "test paper"  # Different case
        paper2.authors = ["John Doe"]
        
        assert get_paper_id(paper1) == get_paper_id(paper2)

    def test_different_author_different_id(self):
        """Different authors produce different IDs."""
        paper1 = Mock()
        paper1.doi = None
        paper1.title = "Test Paper"
        paper1.authors = ["John Doe"]
        
        paper2 = Mock()
        paper2.doi = None
        paper2.title = "Test Paper"
        paper2.authors = ["Jane Smith"]
        
        assert get_paper_id(paper1) != get_paper_id(paper2)

    def test_no_authors(self):
        """Papers with no authors still get valid ID."""
        paper = Mock()
        paper.doi = None
        paper.title = "Test Paper"
        paper.authors = []
        
        pid = get_paper_id(paper)
        assert pid.startswith("hash:")

    def test_query_hash_consistent(self):
        """Same query produces same hash."""
        assert get_query_hash("machine learning") == get_query_hash("machine learning")
        assert get_query_hash("Machine Learning") == get_query_hash("machine learning")

    def test_query_hash_different(self):
        """Different queries produce different hashes."""
        assert get_query_hash("machine learning") != get_query_hash("deep learning")
@


\chapter{Data Directory Management}

We use [[platformdirs]] to find the appropriate data directory for each
operating system. This directory stores user-generated content that should
persist across sessions.

<<data directory functions>>=
def get_data_dir() -> Path:
    """
    Return the platform-appropriate data directory for Scholar.
    
    The directory is created if it doesn't exist.
    Can be overridden with SCHOLAR_DATA_DIR environment variable.
    """
    data_dir = os.environ.get("SCHOLAR_DATA_DIR")
    if data_dir:
        path = Path(data_dir)
    else:
        path = Path(platformdirs.user_data_dir("scholar"))
    path.mkdir(parents=True, exist_ok=True)
    return path


def get_decisions_dir() -> Path:
    """
    Return the directory for storing search decisions.
    
    Creates the directory if it doesn't exist.
    """
    path = get_data_dir() / DECISIONS_DIR
    path.mkdir(parents=True, exist_ok=True)
    return path
@

\section{Testing data directory}

<<test data directory>>=
class TestDataDirectory:
    """Tests for data directory functions."""

    def test_creates_directory(self, tmp_path, monkeypatch):
        """Data directory is created if it doesn't exist."""
        data_dir = tmp_path / "scholar_data"
        monkeypatch.setenv("SCHOLAR_DATA_DIR", str(data_dir))
        result = get_data_dir()
        assert result == data_dir
        assert data_dir.exists()

    def test_respects_environment_variable(self, tmp_path, monkeypatch):
        """SCHOLAR_DATA_DIR overrides default location."""
        custom_dir = tmp_path / "custom"
        monkeypatch.setenv("SCHOLAR_DATA_DIR", str(custom_dir))
        result = get_data_dir()
        assert result == custom_dir

    def test_decisions_dir_created(self, tmp_path, monkeypatch):
        """Decisions subdirectory is created."""
        monkeypatch.setenv("SCHOLAR_DATA_DIR", str(tmp_path))
        result = get_decisions_dir()
        assert result == tmp_path / DECISIONS_DIR
        assert result.exists()
@


\chapter{Paper Notes Functions}

Notes are stored in a single JSON file mapping paper IDs to note objects.
This keeps all notes together for easy backup and export.

<<paper notes functions>>=
def _load_all_notes() -> dict[str, PaperNote]:
    """
    Load all paper notes from disk.
    
    Returns empty dict if file doesn't exist or is corrupted.
    """
    notes_file = get_data_dir() / NOTES_FILE
    if not notes_file.exists():
        return {}
    try:
        with open(notes_file, "r", encoding="utf-8") as f:
            data = json.load(f)
        return {
            pid: PaperNote.from_dict(note_data)
            for pid, note_data in data.items()
        }
    except (json.JSONDecodeError, OSError, KeyError):
        return {}


def _save_all_notes(notes: dict[str, PaperNote]) -> None:
    """
    Save all paper notes to disk.
    """
    notes_file = get_data_dir() / NOTES_FILE
    data = {pid: note.to_dict() for pid, note in notes.items()}
    try:
        with open(notes_file, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
    except OSError:
        pass  # Silently fail if we can't write


def get_note(paper: Any) -> PaperNote | None:
    """
    Get the note for a paper, if one exists.
    
    Args:
        paper: A Paper object.
    
    Returns:
        PaperNote if found, None otherwise.
    """
    paper_id = get_paper_id(paper)
    notes = _load_all_notes()
    return notes.get(paper_id)


def save_note(paper: Any, note_text: str) -> PaperNote:
    """
    Save or update a note for a paper.
    
    Args:
        paper: A Paper object.
        note_text: The markdown note content.
    
    Returns:
        The saved PaperNote object.
    """
    paper_id = get_paper_id(paper)
    notes = _load_all_notes()
    
    now = datetime.now().isoformat()
    
    if paper_id in notes:
        # Update existing note
        existing = notes[paper_id]
        notes[paper_id] = PaperNote(
            paper_id=paper_id,
            title=paper.title,
            note=note_text,
            created_at=existing.created_at,
            updated_at=now,
        )
    else:
        # Create new note
        notes[paper_id] = PaperNote(
            paper_id=paper_id,
            title=paper.title,
            note=note_text,
            created_at=now,
            updated_at=now,
        )
    
    _save_all_notes(notes)
    return notes[paper_id]


def delete_note(paper: Any) -> bool:
    """
    Delete the note for a paper.
    
    Args:
        paper: A Paper object.
    
    Returns:
        True if a note was deleted, False if no note existed.
    """
    paper_id = get_paper_id(paper)
    notes = _load_all_notes()
    
    if paper_id in notes:
        del notes[paper_id]
        _save_all_notes(notes)
        return True
    return False


def list_papers_with_notes() -> list[PaperNote]:
    """
    Get all papers that have notes.
    
    Returns:
        List of PaperNote objects, sorted by updated_at descending.
    """
    notes = _load_all_notes()
    return sorted(
        notes.values(),
        key=lambda n: n.updated_at,
        reverse=True,
    )


def has_note(paper: Any) -> bool:
    """
    Check if a paper has a note.
    
    Args:
        paper: A Paper object.
    
    Returns:
        True if the paper has a note.
    """
    paper_id = get_paper_id(paper)
    notes = _load_all_notes()
    return paper_id in notes
@

\section{Testing paper notes}

<<test paper notes>>=
class TestPaperNotes:
    """Tests for paper notes functions."""

    def test_save_and_get_note(self, tmp_path, monkeypatch):
        """Can save and retrieve a note."""
        monkeypatch.setenv("SCHOLAR_DATA_DIR", str(tmp_path))
        
        paper = Mock()
        paper.doi = "10.1234/test"
        paper.title = "Test Paper"
        paper.authors = ["Author"]
        
        save_note(paper, "This is my note.")
        
        note = get_note(paper)
        assert note is not None
        assert note.note == "This is my note."
        assert note.title == "Test Paper"

    def test_update_note(self, tmp_path, monkeypatch):
        """Updating a note preserves created_at."""
        monkeypatch.setenv("SCHOLAR_DATA_DIR", str(tmp_path))
        
        paper = Mock()
        paper.doi = "10.1234/test"
        paper.title = "Test Paper"
        paper.authors = ["Author"]
        
        note1 = save_note(paper, "First note")
        created_at = note1.created_at
        
        note2 = save_note(paper, "Updated note")
        assert note2.note == "Updated note"
        assert note2.created_at == created_at
        assert note2.updated_at >= note1.updated_at

    def test_delete_note(self, tmp_path, monkeypatch):
        """Can delete a note."""
        monkeypatch.setenv("SCHOLAR_DATA_DIR", str(tmp_path))
        
        paper = Mock()
        paper.doi = "10.1234/test"
        paper.title = "Test Paper"
        paper.authors = ["Author"]
        
        save_note(paper, "Note to delete")
        assert has_note(paper)
        
        result = delete_note(paper)
        assert result is True
        assert not has_note(paper)

    def test_delete_nonexistent_note(self, tmp_path, monkeypatch):
        """Deleting nonexistent note returns False."""
        monkeypatch.setenv("SCHOLAR_DATA_DIR", str(tmp_path))
        
        paper = Mock()
        paper.doi = "10.1234/nonexistent"
        paper.title = "Test"
        paper.authors = []
        
        result = delete_note(paper)
        assert result is False

    def test_list_papers_with_notes(self, tmp_path, monkeypatch):
        """Can list all papers with notes."""
        monkeypatch.setenv("SCHOLAR_DATA_DIR", str(tmp_path))
        
        paper1 = Mock()
        paper1.doi = "10.1234/test1"
        paper1.title = "Paper 1"
        paper1.authors = ["A"]
        
        paper2 = Mock()
        paper2.doi = "10.1234/test2"
        paper2.title = "Paper 2"
        paper2.authors = ["B"]
        
        save_note(paper1, "Note 1")
        save_note(paper2, "Note 2")
        
        notes = list_papers_with_notes()
        assert len(notes) == 2
        # Most recently updated first
        assert notes[0].title == "Paper 2"

    def test_has_note(self, tmp_path, monkeypatch):
        """has_note correctly identifies papers with notes."""
        monkeypatch.setenv("SCHOLAR_DATA_DIR", str(tmp_path))
        
        paper = Mock()
        paper.doi = "10.1234/test"
        paper.title = "Test"
        paper.authors = []
        
        assert not has_note(paper)
        save_note(paper, "A note")
        assert has_note(paper)
@


\chapter{Search Decisions Functions}

Search decisions are stored per-query, allowing us to restore the previous
review state when repeating a search. Each query gets its own JSON file.

<<search decisions functions>>=
def load_search_decisions(query: str) -> SearchDecisions | None:
    """
    Load previous decisions for a search query.
    
    Args:
        query: The search query string.
    
    Returns:
        SearchDecisions if found, None otherwise.
    """
    query_hash = get_query_hash(query)
    decisions_file = get_decisions_dir() / f"{query_hash}.json"
    
    if not decisions_file.exists():
        return None
    
    try:
        with open(decisions_file, "r", encoding="utf-8") as f:
            data = json.load(f)
        return SearchDecisions.from_dict(data)
    except (json.JSONDecodeError, OSError, KeyError):
        return None


def save_search_decisions(query: str, decisions: dict[str, ReviewDecisionRecord]) -> None:
    """
    Save decisions for a search query.
    
    Args:
        query: The search query string.
        decisions: Dictionary mapping paper_id to ReviewDecisionRecord.
    """
    query_hash = get_query_hash(query)
    
    search_decisions = SearchDecisions(
        query=query,
        query_hash=query_hash,
        decisions=decisions,
        timestamp=datetime.now().isoformat(),
    )
    
    decisions_file = get_decisions_dir() / f"{query_hash}.json"
    try:
        with open(decisions_file, "w", encoding="utf-8") as f:
            json.dump(search_decisions.to_dict(), f, indent=2, ensure_ascii=False)
    except OSError:
        pass  # Silently fail


def get_previous_decision(query: str, paper: Any) -> ReviewDecisionRecord | None:
    """
    Get the previous decision for a paper in a specific search.
    
    Args:
        query: The search query string.
        paper: A Paper object.
    
    Returns:
        ReviewDecisionRecord if found, None otherwise.
    """
    search_decisions = load_search_decisions(query)
    if search_decisions is None:
        return None
    
    paper_id = get_paper_id(paper)
    return search_decisions.decisions.get(paper_id)


def clear_all_decisions() -> int:
    """
    Clear all saved search decisions.
    
    Returns:
        Number of decision files deleted.
    """
    decisions_dir = get_decisions_dir()
    count = 0
    for decision_file in decisions_dir.glob("*.json"):
        try:
            decision_file.unlink()
            count += 1
        except OSError:
            pass
    return count
@

\section{Testing search decisions}

<<test search decisions>>=
class TestSearchDecisions:
    """Tests for search decisions functions."""

    def test_save_and_load_decisions(self, tmp_path, monkeypatch):
        """Can save and load search decisions."""
        monkeypatch.setenv("SCHOLAR_DATA_DIR", str(tmp_path))
        
        query = "machine learning"
        decisions = {
            "doi:10.1234/test1": ReviewDecisionRecord(status="kept"),
            "doi:10.1234/test2": ReviewDecisionRecord(
                status="discarded",
                motivation="Not relevant"
            ),
        }
        
        save_search_decisions(query, decisions)
        
        loaded = load_search_decisions(query)
        assert loaded is not None
        assert loaded.query == query
        assert len(loaded.decisions) == 2
        assert loaded.decisions["doi:10.1234/test1"].status == "kept"
        assert loaded.decisions["doi:10.1234/test2"].motivation == "Not relevant"

    def test_load_nonexistent_returns_none(self, tmp_path, monkeypatch):
        """Loading nonexistent decisions returns None."""
        monkeypatch.setenv("SCHOLAR_DATA_DIR", str(tmp_path))
        
        result = load_search_decisions("nonexistent query")
        assert result is None

    def test_get_previous_decision(self, tmp_path, monkeypatch):
        """Can get previous decision for a paper."""
        monkeypatch.setenv("SCHOLAR_DATA_DIR", str(tmp_path))
        
        query = "test query"
        paper = Mock()
        paper.doi = "10.1234/test"
        paper.title = "Test"
        paper.authors = []
        
        paper_id = get_paper_id(paper)
        decisions = {
            paper_id: ReviewDecisionRecord(status="kept"),
        }
        save_search_decisions(query, decisions)
        
        result = get_previous_decision(query, paper)
        assert result is not None
        assert result.status == "kept"

    def test_get_previous_decision_not_found(self, tmp_path, monkeypatch):
        """Returns None for paper not in previous decisions."""
        monkeypatch.setenv("SCHOLAR_DATA_DIR", str(tmp_path))
        
        query = "test query"
        paper = Mock()
        paper.doi = "10.1234/unknown"
        paper.title = "Unknown"
        paper.authors = []
        
        result = get_previous_decision(query, paper)
        assert result is None

    def test_clear_all_decisions(self, tmp_path, monkeypatch):
        """Can clear all decisions."""
        monkeypatch.setenv("SCHOLAR_DATA_DIR", str(tmp_path))
        
        # Create some decisions
        save_search_decisions("query1", {})
        save_search_decisions("query2", {})
        
        decisions_dir = get_decisions_dir()
        assert len(list(decisions_dir.glob("*.json"))) == 2
        
        count = clear_all_decisions()
        assert count == 2
        assert len(list(decisions_dir.glob("*.json"))) == 0
@


\chapter{Editor Integration}

To edit notes, we open the user's preferred editor (from [[VISUAL]] or
[[EDITOR]] environment variables). We use [[click.edit()]] which handles
this automatically and provides a good cross-platform experience.

<<editor integration>>=
def edit_note_in_editor(paper: Any) -> str | None:
    """
    Open the user's editor to edit a note for a paper.
    
    Uses the VISUAL or EDITOR environment variable, with sensible
    fallbacks. The editor is opened with the existing note content
    (if any) and the result is saved.
    
    Args:
        paper: A Paper object.
    
    Returns:
        The edited note text, or None if editing was cancelled.
    """
    existing = get_note(paper)
    initial_content = existing.note if existing else ""
    
    # Add header comment to help user
    header = f"# Notes for: {paper.title}\n"
    header += f"# Authors: {', '.join(paper.authors[:3])}"
    if len(paper.authors) > 3:
        header += " et al."
    header += "\n"
    if paper.doi:
        header += f"# DOI: {paper.doi}\n"
    header += "# Lines starting with # are stripped.\n"
    header += "# Save and close to save notes, or delete all content to cancel.\n"
    header += "#" + "=" * 60 + "\n\n"
    
    editor_content = header + initial_content
    
    # Use click.edit which handles VISUAL/EDITOR env vars
    edited = click.edit(editor_content, extension=".md")
    
    if edited is None:
        # User closed without saving
        return None
    
    # Strip header comments
    lines = edited.split("\n")
    content_lines = [line for line in lines if not line.startswith("#")]
    result = "\n".join(content_lines).strip()
    
    if not result:
        # Empty content - don't save (or delete existing)
        if existing:
            delete_note(paper)
        return None
    
    save_note(paper, result)
    return result
@


\chapter{Export and Import}

Users may want to backup their notes or transfer them between machines.
We provide export and import functions for this purpose.

<<paper notes functions>>=
def export_notes(output_path: Path) -> int:
    """
    Export all notes to a JSON file.
    
    Args:
        output_path: Path to write the export file.
    
    Returns:
        Number of notes exported.
    """
    notes = _load_all_notes()
    data = {
        "version": 1,
        "exported_at": datetime.now().isoformat(),
        "notes": {pid: note.to_dict() for pid, note in notes.items()},
    }
    
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    
    return len(notes)


def import_notes(input_path: Path, merge: bool = True) -> int:
    """
    Import notes from a JSON file.
    
    Args:
        input_path: Path to the export file.
        merge: If True, merge with existing notes (existing take precedence).
               If False, replace all notes.
    
    Returns:
        Number of notes imported.
    """
    with open(input_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    imported_notes = {
        pid: PaperNote.from_dict(note_data)
        for pid, note_data in data.get("notes", {}).items()
    }
    
    if merge:
        existing = _load_all_notes()
        # Imported notes fill in gaps, existing take precedence
        for pid, note in imported_notes.items():
            if pid not in existing:
                existing[pid] = note
        _save_all_notes(existing)
        return len(imported_notes)
    else:
        _save_all_notes(imported_notes)
        return len(imported_notes)
@

\section{Testing export and import}

<<test export import>>=
class TestExportImport:
    """Tests for export and import functions."""

    def test_export_notes(self, tmp_path, monkeypatch):
        """Can export notes to file."""
        monkeypatch.setenv("SCHOLAR_DATA_DIR", str(tmp_path))
        
        paper = Mock()
        paper.doi = "10.1234/test"
        paper.title = "Test Paper"
        paper.authors = ["Author"]
        save_note(paper, "Test note")
        
        export_path = tmp_path / "export.json"
        count = export_notes(export_path)
        
        assert count == 1
        assert export_path.exists()
        
        with open(export_path) as f:
            data = json.load(f)
        assert "notes" in data
        assert len(data["notes"]) == 1

    def test_import_notes_merge(self, tmp_path, monkeypatch):
        """Import with merge preserves existing notes."""
        monkeypatch.setenv("SCHOLAR_DATA_DIR", str(tmp_path))
        
        # Create existing note
        paper1 = Mock()
        paper1.doi = "10.1234/existing"
        paper1.title = "Existing"
        paper1.authors = []
        save_note(paper1, "Existing note")
        
        # Create export file with different note
        export_data = {
            "version": 1,
            "exported_at": "2024-01-01",
            "notes": {
                "doi:10.1234/existing": {
                    "paper_id": "doi:10.1234/existing",
                    "title": "Existing",
                    "note": "Imported note (should be ignored)",
                    "created_at": "2024-01-01",
                    "updated_at": "2024-01-01",
                },
                "doi:10.1234/new": {
                    "paper_id": "doi:10.1234/new",
                    "title": "New Paper",
                    "note": "New note",
                    "created_at": "2024-01-01",
                    "updated_at": "2024-01-01",
                },
            },
        }
        export_path = tmp_path / "import.json"
        with open(export_path, "w") as f:
            json.dump(export_data, f)
        
        import_notes(export_path, merge=True)
        
        # Existing note preserved
        existing_note = get_note(paper1)
        assert existing_note.note == "Existing note"
        
        # New note imported
        notes = list_papers_with_notes()
        assert len(notes) == 2
@


\chapter{Tests}
\label{ch:notes-testing}

Tests are distributed throughout this document, appearing after each
implementation section they verify. The test file collects all distributed
test chunks:

<<test [[notes.py]]>>=
"""Tests for the notes persistence module."""
import json
import pytest
from pathlib import Path
from unittest.mock import Mock

from scholar.notes import (
    get_paper_id,
    get_query_hash,
    get_data_dir,
    get_decisions_dir,
    get_note,
    save_note,
    delete_note,
    has_note,
    list_papers_with_notes,
    export_notes,
    import_notes,
    load_search_decisions,
    save_search_decisions,
    get_previous_decision,
    clear_all_decisions,
    ReviewDecisionRecord,
    DECISIONS_DIR,
)


<<test paper identity>>
<<test data directory>>
<<test paper notes>>
<<test search decisions>>
<<test export import>>
@
