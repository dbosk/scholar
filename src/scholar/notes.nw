\chapter{Paper Notes and Decision Persistence}
\label{notes-module}

\subsection{Introduction}

When conducting systematic literature reviews, researchers need to:
\begin{itemize}
\item Take notes on individual papers that persist across sessions
\item Remember previous keep/discard decisions when re-running the same search
\item Include notes in generated reports
\end{itemize}

This module provides persistent storage for paper notes (global, per-paper) and
search decisions (per-query). Unlike the search cache which stores API results,
this module stores user-generated content that should never be automatically
cleared.

\subsection{Design overview}

We store data in the user's data directory (not cache directory):
\begin{description}
\item[Linux] \texttt{\textasciitilde/.local/share/scholar/}
\item[macOS] \texttt{\textasciitilde/Library/Application Support/scholar/}
\item[Windows] \texttt{C:\textbackslash Users\textbackslash <user>\textbackslash AppData\textbackslash Local\textbackslash scholar\textbackslash}
\end{description}

The storage format is JSON for human readability and easier debugging.

Paper identity uses DOI when available, otherwise a hash of the lowercase title
combined with the first author's last name. This provides better disambiguation
than title alone.

\subsection{Module structure}

<<[[notes.py]]>>=
"""
Persistent storage for paper notes and search decisions.

Notes are global (per-paper) and persist forever.
Search decisions are per-query and help restore previous review state.
"""

<<notes imports>>
<<notes constants>>
<<notes data structures>>
<<paper identity functions>>
<<data directory functions>>
<<paper notes functions>>
<<search decisions functions>>
<<editor integration>>
@


\section{Testing}
\label{ch:notes-testing}

Tests are distributed throughout this document, appearing after each
implementation section they verify. The test file collects all distributed
test chunks:

<<test [[notes.py]]>>=
"""Tests for the notes persistence module."""
import json
import pytest
from pathlib import Path
from unittest.mock import Mock

from scholar.notes import *


<<test functions>>
@


\section{Data Structures}
\label{sec:notes-data-structures}

We define dataclasses to represent notes and decisions. These are serialized
to JSON for storage.

\subsection{Two Types of Persistent Data}

The module manages two related but distinct types of data:
\begin{description}
\item[PaperNote] Global, per-paper notes that persist forever. If you take
  notes on a paper, those notes should be available regardless of which
  search found the paper.
\item[ReviewDecisionRecord] Per-query decisions (keep/discard) with
  motivations. These are tied to specific searches because the same paper
  might be relevant to one query but not another.
\end{description}

The distinction matters: clearing decisions for one search shouldn't affect
notes or decisions from other searches.

<<notes imports>>=
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any
import hashlib
import json
import os

import click
import platformdirs
@

\subsection{Timestamp Representation}

The [[PaperNote]] class stores timestamps as [[datetime]] objects rather than
strings. This design choice enables natural use of datetime methods like
[[strftime]] for display formatting, while maintaining backward compatibility
with existing stored notes through explicit serialization.

The JSON storage format remains unchanged: timestamps are stored as ISO~8601
strings. The [[to_dict]] method serializes datetime objects to strings, and
[[from_dict]] parses them back. This separation of internal representation from
storage format provides flexibility for display code without breaking existing
data.

<<notes data structures>>=
@dataclass
class PaperNote:
    """
    A note attached to a paper.
    
    Notes are identified by paper_id (DOI or hash of title+author).
    The title is stored for display purposes in the notes browser.
    Timestamps are stored as datetime objects internally, but serialized
    to ISO format strings for JSON storage.
    """
    paper_id: str
    title: str
    note: str
    created_at: datetime
    updated_at: datetime
    
    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary for JSON serialization."""
        return {
            "paper_id": self.paper_id,
            "title": self.title,
            "note": self.note,
            "created_at": self.created_at.isoformat(),
            "updated_at": self.updated_at.isoformat(),
        }
    
    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> "PaperNote":
        """Create from dictionary, parsing ISO timestamps to datetime."""
        return cls(
            paper_id=data["paper_id"],
            title=data["title"],
            note=data["note"],
            created_at=datetime.fromisoformat(data["created_at"]),
            updated_at=datetime.fromisoformat(data["updated_at"]),
        )


@dataclass
class ReviewDecisionRecord:
    """
    A single review decision for a paper within a search.

    Stores the status (keep/discard/pending), tags (themes for kept papers,
    motivations for discarded), and paper details needed for reconstruction
    when merging sessions.

    The [[tags]] field replaces the older [[motivation]] field. For backward
    compatibility, [[motivation]] is kept as a property that accesses the
    first tag.

    LLM-related fields track whether the decision was made by human or LLM,
    whether this paper is a training example for future LLM rounds, and the
    LLM's confidence score (if applicable).
    """
    status: str  # "kept", "discarded", "pending"
    tags: list[str] = field(default_factory=list)
    # Paper details for reconstruction
    title: str = ""
    authors: list[str] = field(default_factory=list)
    year: int | None = None
    doi: str | None = None
    abstract: str | None = None
    venue: str | None = None
    url: str | None = None
    pdf_url: str | None = None
    provider: str = ""
    # LLM-related fields
    source: str = "human"  # "human", "llm", "llm_reviewed"
    is_example: bool = False  # True if user corrected an LLM decision
    llm_confidence: float | None = None  # 0.0-1.0 if LLM decided
    
    @property
    def motivation(self) -> str:
        """Get first tag as motivation (backward compatibility)."""
        return self.tags[0] if self.tags else ""
    
    @motivation.setter
    def motivation(self, value: str) -> None:
        """Set motivation as single tag (backward compatibility)."""
        if value:
            if not self.tags:
                self.tags = [value]
            else:
                self.tags[0] = value
        elif self.tags:
            self.tags = self.tags[1:]  # Remove first tag
    
    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary for JSON serialization."""
        return {
            "status": self.status,
            "tags": self.tags,
            "title": self.title,
            "authors": self.authors,
            "year": self.year,
            "doi": self.doi,
            "abstract": self.abstract,
            "venue": self.venue,
            "url": self.url,
            "pdf_url": self.pdf_url,
            "provider": self.provider,
            "source": self.source,
            "is_example": self.is_example,
            "llm_confidence": self.llm_confidence,
        }
    
    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> "ReviewDecisionRecord":
        """Create from dictionary, handling both old and new formats."""
        # Handle old format with 'motivation' field
        tags = data.get("tags", [])
        if not tags and data.get("motivation"):
            tags = [data["motivation"]]

        return cls(
            status=data.get("status", "pending"),
            tags=tags,
            title=data.get("title", ""),
            authors=data.get("authors", []),
            year=data.get("year"),
            doi=data.get("doi"),
            abstract=data.get("abstract"),
            venue=data.get("venue"),
            url=data.get("url"),
            pdf_url=data.get("pdf_url"),
            provider=data.get("provider", ""),
            source=data.get("source", "human"),
            is_example=data.get("is_example", False),
            llm_confidence=data.get("llm_confidence"),
        )


@dataclass
class SearchDecisions:
    """
    All review decisions for a specific search query.
    
    Decisions are keyed by paper_id, allowing lookup when the same
    paper appears in a repeated search.
    """
    query: str
    query_hash: str
    decisions: dict[str, ReviewDecisionRecord] = field(default_factory=dict)
    timestamp: str = ""
    
    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary for JSON serialization."""
        return {
            "query": self.query,
            "query_hash": self.query_hash,
            "decisions": {
                pid: dec.to_dict() for pid, dec in self.decisions.items()
            },
            "timestamp": self.timestamp,
        }
    
    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> "SearchDecisions":
        """Create from dictionary."""
        decisions = {
            pid: ReviewDecisionRecord.from_dict(dec)
            for pid, dec in data.get("decisions", {}).items()
        }
        return cls(
            query=data["query"],
            query_hash=data["query_hash"],
            decisions=decisions,
            timestamp=data.get("timestamp", ""),
        )
@


\section{Paper Identity}
\label{ch:paper-identity}

To associate notes with papers across different searches and sessions, we need
a stable identifier. DOI is ideal when available, but many papers lack DOIs.
For papers without DOIs, we create a hash from the title and first author's
last name.

\subsection{Why Include First Author?}

Using title alone would cause collisions for papers with the same title
(surprisingly common in academia). Adding the first author's last name
provides additional disambiguation. We don't use all authors because:
\begin{itemize}
\item Author order sometimes varies between databases
\item Middle author names may be formatted differently
\item First author is usually consistent and sufficient
\end{itemize}

\subsection{Hash Length Trade-off}

We use 16 hex characters (64 bits) from SHA-256. This provides:
\begin{itemize}
\item Collision probability of $1/2^{32}$ per million papers (negligible)
\item Short enough for readable filenames and JSON keys
\item Deterministic---same paper always produces same ID
\end{itemize}

<<notes constants>>=
# File names for persistent storage
NOTES_FILE = "paper_notes.json"
DECISIONS_DIR = "search_decisions"
@

<<paper identity functions>>=
def get_paper_id(paper: Any) -> str:
    """
    Generate a stable identifier for a paper.
    
    Uses DOI if available, otherwise SHA256 hash of normalized
    title + first author's last name.
    
    Args:
        paper: A Paper object with title, authors, and optional doi.
    
    Returns:
        A string identifier unique to this paper.
    """
    if paper.doi:
        return f"doi:{paper.doi.lower()}"
    
    # Normalize title
    title_normalized = paper.title.lower().strip()
    
    # Get first author's last name if available
    author_part = ""
    if paper.authors:
        first_author = paper.authors[0]
        # Extract last name (last word of author name)
        last_name = first_author.split()[-1].lower() if first_author else ""
        author_part = last_name
    
    # Create hash
    content = f"{title_normalized}|{author_part}"
    hash_value = hashlib.sha256(content.encode()).hexdigest()[:16]
    return f"hash:{hash_value}"


def get_query_hash(query: str) -> str:
    """
    Generate a hash for a search query.
    
    Used as filename for storing search decisions.
    """
    normalized = query.lower().strip()
    return hashlib.sha256(normalized.encode()).hexdigest()[:16]
@

\subsection{Testing paper identity}

<<test functions>>=
class TestPaperIdentity:
    """Tests for paper identity functions."""

    def test_doi_takes_precedence(self):
        """Papers with DOI use DOI as identifier."""
        paper = Mock()
        paper.doi = "10.1234/TEST"
        paper.title = "Test Paper"
        paper.authors = ["John Doe"]
        
        pid = get_paper_id(paper)
        assert pid == "doi:10.1234/test"

    def test_hash_without_doi(self):
        """Papers without DOI use hash of title+author."""
        paper = Mock()
        paper.doi = None
        paper.title = "Test Paper"
        paper.authors = ["John Doe"]
        
        pid = get_paper_id(paper)
        assert pid.startswith("hash:")
        assert len(pid) == 5 + 16  # "hash:" + 16 hex chars

    def test_same_paper_same_id(self):
        """Same title+author produces same ID."""
        paper1 = Mock()
        paper1.doi = None
        paper1.title = "Test Paper"
        paper1.authors = ["John Doe"]
        
        paper2 = Mock()
        paper2.doi = None
        paper2.title = "test paper"  # Different case
        paper2.authors = ["John Doe"]
        
        assert get_paper_id(paper1) == get_paper_id(paper2)

    def test_different_author_different_id(self):
        """Different authors produce different IDs."""
        paper1 = Mock()
        paper1.doi = None
        paper1.title = "Test Paper"
        paper1.authors = ["John Doe"]
        
        paper2 = Mock()
        paper2.doi = None
        paper2.title = "Test Paper"
        paper2.authors = ["Jane Smith"]
        
        assert get_paper_id(paper1) != get_paper_id(paper2)

    def test_no_authors(self):
        """Papers with no authors still get valid ID."""
        paper = Mock()
        paper.doi = None
        paper.title = "Test Paper"
        paper.authors = []
        
        pid = get_paper_id(paper)
        assert pid.startswith("hash:")

    def test_query_hash_consistent(self):
        """Same query produces same hash."""
        assert get_query_hash("machine learning") == get_query_hash("machine learning")
        assert get_query_hash("Machine Learning") == get_query_hash("machine learning")

    def test_query_hash_different(self):
        """Different queries produce different hashes."""
        assert get_query_hash("machine learning") != get_query_hash("deep learning")
@


\section{Data Directory Management}

We use [[platformdirs]] to find the appropriate data directory for each
operating system. This directory stores user-generated content that should
persist across sessions.

<<data directory functions>>=
def get_data_dir() -> Path:
    """
    Return the platform-appropriate data directory for Scholar.
    
    The directory is created if it doesn't exist.
    Can be overridden with SCHOLAR_DATA_DIR environment variable.
    """
    data_dir = os.environ.get("SCHOLAR_DATA_DIR")
    if data_dir:
        path = Path(data_dir)
    else:
        path = Path(platformdirs.user_data_dir("scholar"))
    path.mkdir(parents=True, exist_ok=True)
    return path


def get_decisions_dir() -> Path:
    """
    Return the directory for storing search decisions.
    
    Creates the directory if it doesn't exist.
    """
    path = get_data_dir() / DECISIONS_DIR
    path.mkdir(parents=True, exist_ok=True)
    return path
@

\subsection{Testing data directory}

<<test functions>>=
class TestDataDirectory:
    """Tests for data directory functions."""

    def test_creates_directory(self, tmp_path, monkeypatch):
        """Data directory is created if it doesn't exist."""
        data_dir = tmp_path / "scholar_data"
        monkeypatch.setenv("SCHOLAR_DATA_DIR", str(data_dir))
        result = get_data_dir()
        assert result == data_dir
        assert data_dir.exists()

    def test_respects_environment_variable(self, tmp_path, monkeypatch):
        """SCHOLAR_DATA_DIR overrides default location."""
        custom_dir = tmp_path / "custom"
        monkeypatch.setenv("SCHOLAR_DATA_DIR", str(custom_dir))
        result = get_data_dir()
        assert result == custom_dir

    def test_decisions_dir_created(self, tmp_path, monkeypatch):
        """Decisions subdirectory is created."""
        monkeypatch.setenv("SCHOLAR_DATA_DIR", str(tmp_path))
        result = get_decisions_dir()
        assert result == tmp_path / DECISIONS_DIR
        assert result.exists()
@


\section{Paper Notes Functions}

Notes are stored in a single JSON file mapping paper IDs to note objects.
This keeps all notes together for easy backup and export.

<<paper notes functions>>=
def _load_all_notes() -> dict[str, PaperNote]:
    """
    Load all paper notes from disk.
    
    Returns empty dict if file doesn't exist or is corrupted.
    """
    notes_file = get_data_dir() / NOTES_FILE
    if not notes_file.exists():
        return {}
    try:
        with open(notes_file, "r", encoding="utf-8") as f:
            data = json.load(f)
        return {
            pid: PaperNote.from_dict(note_data)
            for pid, note_data in data.items()
        }
    except (json.JSONDecodeError, OSError, KeyError):
        return {}


def _save_all_notes(notes: dict[str, PaperNote]) -> None:
    """
    Save all paper notes to disk.
    """
    notes_file = get_data_dir() / NOTES_FILE
    data = {pid: note.to_dict() for pid, note in notes.items()}
    try:
        with open(notes_file, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
    except OSError:
        pass  # Silently fail if we can't write


def get_note(paper: Any) -> PaperNote | None:
    """
    Get the note for a paper, if one exists.
    
    Args:
        paper: A Paper object.
    
    Returns:
        PaperNote if found, None otherwise.
    """
    paper_id = get_paper_id(paper)
    notes = _load_all_notes()
    return notes.get(paper_id)


def save_note(paper: Any, note_text: str) -> PaperNote:
    """
    Save or update a note for a paper.
    
    Args:
        paper: A Paper object.
        note_text: The markdown note content.
    
    Returns:
        The saved PaperNote object.
    """
    paper_id = get_paper_id(paper)
    notes = _load_all_notes()
    
    now = datetime.now()
    
    if paper_id in notes:
        # Update existing note
        existing = notes[paper_id]
        notes[paper_id] = PaperNote(
            paper_id=paper_id,
            title=paper.title,
            note=note_text,
            created_at=existing.created_at,
            updated_at=now,
        )
    else:
        # Create new note
        notes[paper_id] = PaperNote(
            paper_id=paper_id,
            title=paper.title,
            note=note_text,
            created_at=now,
            updated_at=now,
        )
    
    _save_all_notes(notes)
    return notes[paper_id]


def delete_note(paper: Any) -> bool:
    """
    Delete the note for a paper.
    
    Args:
        paper: A Paper object.
    
    Returns:
        True if a note was deleted, False if no note existed.
    """
    paper_id = get_paper_id(paper)
    notes = _load_all_notes()
    
    if paper_id in notes:
        del notes[paper_id]
        _save_all_notes(notes)
        return True
    return False


def list_papers_with_notes() -> list[PaperNote]:
    """
    Get all papers that have notes.
    
    Returns:
        List of PaperNote objects, sorted by updated_at descending.
    """
    notes = _load_all_notes()
    return sorted(
        notes.values(),
        key=lambda n: n.updated_at,
        reverse=True,
    )


def has_note(paper: Any) -> bool:
    """
    Check if a paper has a note.
    
    Args:
        paper: A Paper object.
    
    Returns:
        True if the paper has a note.
    """
    paper_id = get_paper_id(paper)
    notes = _load_all_notes()
    return paper_id in notes
@

\subsection{Testing paper notes}

<<test functions>>=
class TestPaperNotes:
    """Tests for paper notes functions."""

    def test_save_and_get_note(self, tmp_path, monkeypatch):
        """Can save and retrieve a note."""
        monkeypatch.setenv("SCHOLAR_DATA_DIR", str(tmp_path))
        
        paper = Mock()
        paper.doi = "10.1234/test"
        paper.title = "Test Paper"
        paper.authors = ["Author"]
        
        save_note(paper, "This is my note.")
        
        note = get_note(paper)
        assert note is not None
        assert note.note == "This is my note."
        assert note.title == "Test Paper"

    def test_update_note(self, tmp_path, monkeypatch):
        """Updating a note preserves created_at."""
        monkeypatch.setenv("SCHOLAR_DATA_DIR", str(tmp_path))
        
        paper = Mock()
        paper.doi = "10.1234/test"
        paper.title = "Test Paper"
        paper.authors = ["Author"]
        
        note1 = save_note(paper, "First note")
        created_at = note1.created_at
        
        note2 = save_note(paper, "Updated note")
        assert note2.note == "Updated note"
        assert note2.created_at == created_at
        assert note2.updated_at >= note1.updated_at

    def test_delete_note(self, tmp_path, monkeypatch):
        """Can delete a note."""
        monkeypatch.setenv("SCHOLAR_DATA_DIR", str(tmp_path))
        
        paper = Mock()
        paper.doi = "10.1234/test"
        paper.title = "Test Paper"
        paper.authors = ["Author"]
        
        save_note(paper, "Note to delete")
        assert has_note(paper)
        
        result = delete_note(paper)
        assert result is True
        assert not has_note(paper)

    def test_delete_nonexistent_note(self, tmp_path, monkeypatch):
        """Deleting nonexistent note returns False."""
        monkeypatch.setenv("SCHOLAR_DATA_DIR", str(tmp_path))
        
        paper = Mock()
        paper.doi = "10.1234/nonexistent"
        paper.title = "Test"
        paper.authors = []
        
        result = delete_note(paper)
        assert result is False

    def test_list_papers_with_notes(self, tmp_path, monkeypatch):
        """Can list all papers with notes."""
        monkeypatch.setenv("SCHOLAR_DATA_DIR", str(tmp_path))
        
        paper1 = Mock()
        paper1.doi = "10.1234/test1"
        paper1.title = "Paper 1"
        paper1.authors = ["A"]
        
        paper2 = Mock()
        paper2.doi = "10.1234/test2"
        paper2.title = "Paper 2"
        paper2.authors = ["B"]
        
        save_note(paper1, "Note 1")
        save_note(paper2, "Note 2")
        
        notes = list_papers_with_notes()
        assert len(notes) == 2
        # Most recently updated first
        assert notes[0].title == "Paper 2"

    def test_has_note(self, tmp_path, monkeypatch):
        """has_note correctly identifies papers with notes."""
        monkeypatch.setenv("SCHOLAR_DATA_DIR", str(tmp_path))
        
        paper = Mock()
        paper.doi = "10.1234/test"
        paper.title = "Test"
        paper.authors = []
        
        assert not has_note(paper)
        save_note(paper, "A note")
        assert has_note(paper)
@


\section{Search Decisions Functions}

Search decisions are stored per-query, allowing us to restore the previous
review state when repeating a search. Each query gets its own JSON file.

<<search decisions functions>>=
def load_search_decisions(query: str) -> SearchDecisions | None:
    """
    Load previous decisions for a search query.
    
    Args:
        query: The search query string.
    
    Returns:
        SearchDecisions if found, None otherwise.
    """
    query_hash = get_query_hash(query)
    decisions_file = get_decisions_dir() / f"{query_hash}.json"
    
    if not decisions_file.exists():
        return None
    
    try:
        with open(decisions_file, "r", encoding="utf-8") as f:
            data = json.load(f)
        return SearchDecisions.from_dict(data)
    except (json.JSONDecodeError, OSError, KeyError):
        return None


def save_search_decisions(query: str, decisions: dict[str, ReviewDecisionRecord]) -> None:
    """
    Save decisions for a search query.
    
    Args:
        query: The search query string.
        decisions: Dictionary mapping paper_id to ReviewDecisionRecord.
    """
    query_hash = get_query_hash(query)
    
    search_decisions = SearchDecisions(
        query=query,
        query_hash=query_hash,
        decisions=decisions,
        timestamp=datetime.now().isoformat(),
    )
    
    decisions_file = get_decisions_dir() / f"{query_hash}.json"
    try:
        with open(decisions_file, "w", encoding="utf-8") as f:
            json.dump(search_decisions.to_dict(), f, indent=2, ensure_ascii=False)
    except OSError:
        pass  # Silently fail


def get_previous_decision(query: str, paper: Any) -> ReviewDecisionRecord | None:
    """
    Get the previous decision for a paper in a specific search.
    
    Args:
        query: The search query string.
        paper: A Paper object.
    
    Returns:
        ReviewDecisionRecord if found, None otherwise.
    """
    search_decisions = load_search_decisions(query)
    if search_decisions is None:
        return None
    
    paper_id = get_paper_id(paper)
    return search_decisions.decisions.get(paper_id)


def clear_all_decisions() -> int:
    """
    Clear all saved search decisions.
    
    Returns:
        Number of decision files deleted.
    """
    decisions_dir = get_decisions_dir()
    count = 0
    for decision_file in decisions_dir.glob("*.json"):
        try:
            decision_file.unlink()
            count += 1
        except OSError:
            pass
    return count
@

\subsection{Testing search decisions}

<<test functions>>=
class TestSearchDecisions:
    """Tests for search decisions functions."""

    def test_save_and_load_decisions(self, tmp_path, monkeypatch):
        """Can save and load search decisions."""
        monkeypatch.setenv("SCHOLAR_DATA_DIR", str(tmp_path))
        
        query = "machine learning"
        decisions = {
            "doi:10.1234/test1": ReviewDecisionRecord(
                status="kept",
                tags=["relevant", "ml"]
            ),
            "doi:10.1234/test2": ReviewDecisionRecord(
                status="discarded",
                tags=["Not relevant", "wrong-domain"]
            ),
        }
        
        save_search_decisions(query, decisions)
        
        loaded = load_search_decisions(query)
        assert loaded is not None
        assert loaded.query == query
        assert len(loaded.decisions) == 2
        assert loaded.decisions["doi:10.1234/test1"].status == "kept"
        assert loaded.decisions["doi:10.1234/test1"].tags == ["relevant", "ml"]
        assert loaded.decisions["doi:10.1234/test2"].tags == ["Not relevant", "wrong-domain"]
        # Backward compat: motivation property still works
        assert loaded.decisions["doi:10.1234/test2"].motivation == "Not relevant"

    def test_load_old_format_decisions(self, tmp_path, monkeypatch):
        """Can load decisions saved in old format (motivation instead of tags)."""
        monkeypatch.setenv("SCHOLAR_DATA_DIR", str(tmp_path))
        
        # Simulate old format file
        old_format = {
            "query": "old query",
            "query_hash": get_query_hash("old query"),
            "decisions": {
                "doi:10.1234/old": {
                    "status": "discarded",
                    "motivation": "Old format motivation",
                    "title": "Old Paper",
                    "authors": [],
                }
            },
            "timestamp": "2024-01-01",
        }
        
        decisions_file = get_decisions_dir() / f"{get_query_hash('old query')}.json"
        with open(decisions_file, "w") as f:
            json.dump(old_format, f)
        
        loaded = load_search_decisions("old query")
        assert loaded is not None
        record = loaded.decisions["doi:10.1234/old"]
        # Old motivation should be converted to tags
        assert record.tags == ["Old format motivation"]
        # Backward compat property still works
        assert record.motivation == "Old format motivation"

    def test_load_nonexistent_returns_none(self, tmp_path, monkeypatch):
        """Loading nonexistent decisions returns None."""
        monkeypatch.setenv("SCHOLAR_DATA_DIR", str(tmp_path))
        
        result = load_search_decisions("nonexistent query")
        assert result is None

    def test_get_previous_decision(self, tmp_path, monkeypatch):
        """Can get previous decision for a paper."""
        monkeypatch.setenv("SCHOLAR_DATA_DIR", str(tmp_path))
        
        query = "test query"
        paper = Mock()
        paper.doi = "10.1234/test"
        paper.title = "Test"
        paper.authors = []
        
        paper_id = get_paper_id(paper)
        decisions = {
            paper_id: ReviewDecisionRecord(status="kept"),
        }
        save_search_decisions(query, decisions)
        
        result = get_previous_decision(query, paper)
        assert result is not None
        assert result.status == "kept"

    def test_get_previous_decision_not_found(self, tmp_path, monkeypatch):
        """Returns None for paper not in previous decisions."""
        monkeypatch.setenv("SCHOLAR_DATA_DIR", str(tmp_path))
        
        query = "test query"
        paper = Mock()
        paper.doi = "10.1234/unknown"
        paper.title = "Unknown"
        paper.authors = []
        
        result = get_previous_decision(query, paper)
        assert result is None

    def test_clear_all_decisions(self, tmp_path, monkeypatch):
        """Can clear all decisions."""
        monkeypatch.setenv("SCHOLAR_DATA_DIR", str(tmp_path))

        # Create some decisions
        save_search_decisions("query1", {})
        save_search_decisions("query2", {})

        decisions_dir = get_decisions_dir()
        assert len(list(decisions_dir.glob("*.json"))) == 2

        count = clear_all_decisions()
        assert count == 2
        assert len(list(decisions_dir.glob("*.json"))) == 0

    def test_save_and_load_llm_fields(self, tmp_path, monkeypatch):
        """LLM-related fields are saved and loaded correctly."""
        monkeypatch.setenv("SCHOLAR_DATA_DIR", str(tmp_path))

        query = "llm test query"
        decisions = {
            "doi:10.1234/llm1": ReviewDecisionRecord(
                status="kept",
                tags=["relevant"],
                source="llm",
                is_example=False,
                llm_confidence=0.85,
            ),
            "doi:10.1234/llm2": ReviewDecisionRecord(
                status="discarded",
                tags=["off-topic"],
                source="llm_reviewed",
                is_example=True,
                llm_confidence=0.45,
            ),
        }

        save_search_decisions(query, decisions)

        loaded = load_search_decisions(query)
        assert loaded is not None

        # Check first record (LLM unreviewed)
        record1 = loaded.decisions["doi:10.1234/llm1"]
        assert record1.source == "llm"
        assert record1.is_example is False
        assert record1.llm_confidence == 0.85

        # Check second record (LLM reviewed, marked as example)
        record2 = loaded.decisions["doi:10.1234/llm2"]
        assert record2.source == "llm_reviewed"
        assert record2.is_example is True
        assert record2.llm_confidence == 0.45

    def test_load_old_format_without_llm_fields(self, tmp_path, monkeypatch):
        """Old format without LLM fields loads with sensible defaults."""
        monkeypatch.setenv("SCHOLAR_DATA_DIR", str(tmp_path))

        # Simulate old format file (no LLM fields)
        old_format = {
            "query": "old llm query",
            "query_hash": get_query_hash("old llm query"),
            "decisions": {
                "doi:10.1234/old_llm": {
                    "status": "kept",
                    "tags": ["good paper"],
                    "title": "Old Paper",
                    "authors": ["Author"],
                }
            },
            "timestamp": "2024-01-01",
        }

        decisions_file = (
            get_decisions_dir() / f"{get_query_hash('old llm query')}.json"
        )
        with open(decisions_file, "w") as f:
            json.dump(old_format, f)

        loaded = load_search_decisions("old llm query")
        assert loaded is not None
        record = loaded.decisions["doi:10.1234/old_llm"]

        # Verify defaults for missing LLM fields
        assert record.source == "human"
        assert record.is_example is False
        assert record.llm_confidence is None
@


\section{Editor Integration}

To edit notes, we open the user's preferred editor (from [[VISUAL]] or
[[EDITOR]] environment variables). We use [[click.edit()]] which handles
this automatically and provides a good cross-platform experience.

<<editor integration>>=
def edit_note_in_editor(paper: Any) -> str | None:
    """
    Open the user's editor to edit a note for a paper.
    
    Uses the VISUAL or EDITOR environment variable, with sensible
    fallbacks. The editor is opened with the existing note content
    (if any) and the result is saved.
    
    Args:
        paper: A Paper object.
    
    Returns:
        The edited note text, or None if editing was cancelled.
    """
    existing = get_note(paper)
    initial_content = existing.note if existing else ""
    
    # Add header comment to help user
    header = f"# Notes for: {paper.title}\n"
    header += f"# Authors: {', '.join(paper.authors[:3])}"
    if len(paper.authors) > 3:
        header += " et al."
    header += "\n"
    if paper.doi:
        header += f"# DOI: {paper.doi}\n"
    header += "# Lines starting with # are stripped.\n"
    header += "# Save and close to save notes, or delete all content to cancel.\n"
    header += "#" + "=" * 60 + "\n\n"
    
    editor_content = header + initial_content
    
    # Use click.edit which handles VISUAL/EDITOR env vars
    edited = click.edit(editor_content, extension=".md")
    
    if edited is None:
        # User closed without saving
        return None
    
    # Strip header comments
    lines = edited.split("\n")
    content_lines = [line for line in lines if not line.startswith("#")]
    result = "\n".join(content_lines).strip()
    
    if not result:
        # Empty content - don't save (or delete existing)
        if existing:
            delete_note(paper)
        return None
    
    save_note(paper, result)
    return result
@


\section{Export and Import}
\label{ch:export-import}

Users may want to backup their notes or transfer them between machines.
We provide export and import functions for this purpose.

\subsection{Use Cases for Export/Import}

These functions support several systematic review workflows:
\begin{description}
\item[Backup] Periodically export notes for safekeeping. Unlike search
  caches, notes represent significant manual effort.
\item[Collaboration] Export notes to share with co-reviewers, then import
  their notes to merge perspectives.
\item[Machine transfer] Move notes between a laptop and desktop, or when
  switching computers.
\item[Archival] Include exported notes with published research for
  reproducibility.
\end{description}

\subsection{Merge Semantics}

When importing with [[merge=True]] (the default), existing notes take
precedence. This prevents accidentally overwriting recent changes with
an older export. The import fills in gaps---papers that don't have notes
locally will get the imported notes.

<<paper notes functions>>=
def export_notes(output_path: Path) -> int:
    """
    Export all notes to a JSON file.
    
    Args:
        output_path: Path to write the export file.
    
    Returns:
        Number of notes exported.
    """
    notes = _load_all_notes()
    data = {
        "version": 1,
        "exported_at": datetime.now().isoformat(),
        "notes": {pid: note.to_dict() for pid, note in notes.items()},
    }
    
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    
    return len(notes)


def import_notes(input_path: Path, merge: bool = True) -> int:
    """
    Import notes from a JSON file.
    
    Args:
        input_path: Path to the export file.
        merge: If True, merge with existing notes (existing take precedence).
               If False, replace all notes.
    
    Returns:
        Number of notes imported.
    """
    with open(input_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    imported_notes = {
        pid: PaperNote.from_dict(note_data)
        for pid, note_data in data.get("notes", {}).items()
    }
    
    if merge:
        existing = _load_all_notes()
        # Imported notes fill in gaps, existing take precedence
        for pid, note in imported_notes.items():
            if pid not in existing:
                existing[pid] = note
        _save_all_notes(existing)
        return len(imported_notes)
    else:
        _save_all_notes(imported_notes)
        return len(imported_notes)
@

\subsection{Testing export and import}

<<test functions>>=
class TestExportImport:
    """Tests for export and import functions."""

    def test_export_notes(self, tmp_path, monkeypatch):
        """Can export notes to file."""
        monkeypatch.setenv("SCHOLAR_DATA_DIR", str(tmp_path))
        
        paper = Mock()
        paper.doi = "10.1234/test"
        paper.title = "Test Paper"
        paper.authors = ["Author"]
        save_note(paper, "Test note")
        
        export_path = tmp_path / "export.json"
        count = export_notes(export_path)
        
        assert count == 1
        assert export_path.exists()
        
        with open(export_path) as f:
            data = json.load(f)
        assert "notes" in data
        assert len(data["notes"]) == 1

    def test_import_notes_merge(self, tmp_path, monkeypatch):
        """Import with merge preserves existing notes."""
        monkeypatch.setenv("SCHOLAR_DATA_DIR", str(tmp_path))
        
        # Create existing note
        paper1 = Mock()
        paper1.doi = "10.1234/existing"
        paper1.title = "Existing"
        paper1.authors = []
        save_note(paper1, "Existing note")
        
        # Create export file with different note
        export_data = {
            "version": 1,
            "exported_at": "2024-01-01",
            "notes": {
                "doi:10.1234/existing": {
                    "paper_id": "doi:10.1234/existing",
                    "title": "Existing",
                    "note": "Imported note (should be ignored)",
                    "created_at": "2024-01-01",
                    "updated_at": "2024-01-01",
                },
                "doi:10.1234/new": {
                    "paper_id": "doi:10.1234/new",
                    "title": "New Paper",
                    "note": "New note",
                    "created_at": "2024-01-01",
                    "updated_at": "2024-01-01",
                },
            },
        }
        export_path = tmp_path / "import.json"
        with open(export_path, "w") as f:
            json.dump(export_data, f)
        
        import_notes(export_path, merge=True)
        
        # Existing note preserved
        existing_note = get_note(paper1)
        assert existing_note.note == "Existing note"
        
        # New note imported
        notes = list_papers_with_notes()
        assert len(notes) == 2
@
