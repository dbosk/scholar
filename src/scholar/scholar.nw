\chapter{The Core Module}
\label{scholar-module}

\section{Introduction}

Systematic literature reviews and structured searches are essential for
rigorous academic research.
However, searching across multiple databases---such as Web of Science,
IEEE~Xplore, Semantic~Scholar, and OpenAlex---is tedious and error-prone.
Each database has its own query syntax, interface, and export format.
Reproducing a search requires careful documentation of queries, filters, and
timestamps.

This program, \emph{Scholar}, provides a unified interface for structured
searches across multiple bibliographic databases.
It produces reproducible reports documenting the search process: the queries
used, filters applied, timestamps, and resulting papers.
This supports the transparency required for systematic reviews while reducing
manual effort.

\section{Overall design}

The system consists of several components:
\begin{description}
\item[Search providers] Adapters for different bibliographic databases
  (Semantic~Scholar, OpenAlex, IEEE~Xplore, Web of Science, DBLP, etc.).
\item[Query abstraction] A common query format that translates to
  provider-specific syntax.
\item[Results management] Data structures for papers, with deduplication across
  sources.
\item[Report generation] Tools to export structured, reproducible search
  documentation.
\end{description}

\section{Module structure}

We organize the code as a Python package.
The main module provides the public API:

<<[[scholar.py]]>>=
"""
Scholar: A tool for structured literature searches.

Provides a unified interface for searching bibliographic databases
and generating reproducible search reports.
"""

<<imports>>
<<constants>>
<<classes>>
<<functions>>
@

The package initialization exports the public interface:

<<[[__init__.py]]>>=
"""
Scholar package for structured literature searches.
"""

from .scholar import Search, SearchResult, Paper, SearchFilters
from .scholar import search, filter_papers

__all__ = [
    "Search",
    "SearchResult",
    "Paper",
    "SearchFilters",
    "search",
    "filter_papers",
]
@


\section{Testing}
\label{sec:testing}

Tests are distributed throughout this document, appearing after each
implementation section they verify.
The test file collects all distributed test chunks:

<<test [[scholar.py]]>>=
"""Tests for the scholar module."""
import pytest
from unittest.mock import Mock

from scholar import *


<<test functions>>
@


\section{Data model}
\label{sec:data-model}

Before implementing search functionality, we establish the data structures that
represent papers and search results.
Getting these abstractions right is crucial: they determine how papers from
different sources are unified and how results can be exported.

\subsection{Why Dataclasses?}

We use Python dataclasses rather than named tuples, dictionaries, or plain
classes because:
\begin{description}
\item[Conciseness] Automatic [[__init__]], [[__repr__]], and [[__eq__]]
  generation reduces boilerplate.
\item[Type hints] Field annotations serve as documentation and enable IDE
  support.
\item[Mutability] Unlike frozen dataclasses or named tuples, we can update
  fields when merging papers from different sources.
\item[Serializability] Works with [[asdict()]] for easy JSON/dict conversion.
\end{description}

\subsection{Representing papers}

A paper has bibliographic metadata that we need for deduplication and export.
Different databases return varying amounts of detail, so we design for the
common core while allowing optional fields:

<<classes>>=
@dataclass
class Paper:
    """Represents a paper from a bibliographic database."""

    title: str
    authors: list[str]
    year: int | None = None
    doi: str | None = None
    abstract: str | None = None
    venue: str | None = None
    url: str | None = None
    pdf_url: str | None = None
    source: str | None = None

    <<paper methods>>
@

We use a dataclass for simplicity.
The [[source]] field tracks which database returned this paper, useful for
provenance in reports.

\subsubsection{Paper identity and deduplication}

When merging results from multiple databases, we need to identify duplicate
papers.
The DOI is the most reliable identifier when available:

<<paper methods>>=
def __eq__(self, other: object) -> bool:
    """Check equality based on DOI if available, else title."""
    if not isinstance(other, Paper):
        return NotImplemented
    if self.doi and other.doi:
        return self.doi.lower() == other.doi.lower()
    return self.title.lower() == other.title.lower()

def __hash__(self) -> int:
    """Hash based on DOI if available, else title."""
    if self.doi:
        return hash(self.doi.lower())
    return hash(self.title.lower())
@

This allows us to use papers in sets for automatic deduplication.

\subsubsection{Consolidating data from multiple sources}

When the same paper appears from multiple providers, each may have different
metadata: one might have an abstract while another has a PDF~URL.
Rather than arbitrarily keeping one version, we consolidate the data to get
the most complete record.

The [[merge_with]] method combines two equal papers (same DOI or title),
keeping non-[[None]] values from either source.
We prefer [[self]]'s values when both have data, since the caller controls
the order of merging.

<<paper methods>>=
def merge_with(self, other: "Paper") -> "Paper":
    """
    Create a consolidated paper from two equal papers.

    Keeps non-None values, preferring self when both have values.
    Combines sources if different.

    Raises:
        ValueError: If papers are not equal (different DOI/title).
    """
    if self != other:
        raise ValueError("Can only merge equal papers")

    # Combine sources for provenance tracking
    sources = []
    if self.source:
        sources.append(self.source)
    if other.source and other.source not in sources:
        sources.append(other.source)

    return Paper(
        title=self.title or other.title,
        authors=self.authors if self.authors else other.authors,
        year=self.year if self.year is not None else other.year,
        doi=self.doi or other.doi,
        abstract=self.abstract or other.abstract,
        venue=self.venue or other.venue,
        url=self.url or other.url,
        pdf_url=self.pdf_url or other.pdf_url,
        source=", ".join(sources) if sources else None,
    )
@

\subsubsection{Testing paper equality}

Let's verify that paper equality and hashing work correctly for deduplication:

<<test functions>>=
class TestPaper:
    """Tests for the Paper class."""

    def test_equality_by_doi(self):
        """Papers with same DOI are equal."""
        p1 = Paper(title="Paper One", authors=["A"], doi="10.1234/test")
        p2 = Paper(title="Paper 1", authors=["B"], doi="10.1234/test")
        assert p1 == p2

    def test_equality_by_title(self):
        """Papers without DOI are equal by title."""
        p1 = Paper(title="Test Paper", authors=["A"])
        p2 = Paper(title="test paper", authors=["B"])
        assert p1 == p2

    def test_inequality(self):
        """Different papers are not equal."""
        p1 = Paper(title="Paper A", authors=["A"])
        p2 = Paper(title="Paper B", authors=["B"])
        assert p1 != p2

    def test_hash_consistency(self):
        """Equal papers have the same hash."""
        p1 = Paper(title="Test", authors=["A"], doi="10.1234/test")
        p2 = Paper(title="Different", authors=["B"], doi="10.1234/test")
        assert hash(p1) == hash(p2)


class TestPaperMergeWith:
    """Tests for Paper.merge_with() consolidation."""

    def test_merge_keeps_abstract_from_other(self):
        """Merging keeps abstract from paper that has it."""
        p1 = Paper(title="Test", authors=["A"], doi="10.1/test", source="dblp")
        p2 = Paper(
            title="Test", authors=["A"], doi="10.1/test",
            abstract="This is the abstract", source="s2"
        )
        merged = p1.merge_with(p2)
        assert merged.abstract == "This is the abstract"

    def test_merge_prefers_self_values(self):
        """When both have a value, prefer self."""
        p1 = Paper(
            title="Test", authors=["A"], doi="10.1/test",
            venue="Conference A", source="provider1"
        )
        p2 = Paper(
            title="Test", authors=["A"], doi="10.1/test",
            venue="Conference B", source="provider2"
        )
        merged = p1.merge_with(p2)
        assert merged.venue == "Conference A"

    def test_merge_combines_sources(self):
        """Merging combines sources from both papers."""
        p1 = Paper(title="Test", authors=["A"], source="dblp")
        p2 = Paper(title="test", authors=["B"], source="s2")
        merged = p1.merge_with(p2)
        assert "dblp" in merged.source
        assert "s2" in merged.source

    def test_merge_raises_for_unequal_papers(self):
        """Cannot merge papers that aren't equal."""
        p1 = Paper(title="Paper A", authors=["A"], doi="10.1/a")
        p2 = Paper(title="Paper B", authors=["B"], doi="10.1/b")
        with pytest.raises(ValueError, match="Can only merge equal papers"):
            p1.merge_with(p2)

    def test_merge_consolidates_all_fields(self):
        """Merging fills in all missing fields from other."""
        p1 = Paper(
            title="Test", authors=["A"], doi="10.1/test",
            year=2024, source="provider1"
        )
        p2 = Paper(
            title="Test", authors=["A"], doi="10.1/test",
            abstract="Abstract", venue="Venue", url="http://example.com",
            pdf_url="http://example.com/pdf", source="provider2"
        )
        merged = p1.merge_with(p2)
        assert merged.year == 2024  # From p1
        assert merged.abstract == "Abstract"  # From p2
        assert merged.venue == "Venue"  # From p2
        assert merged.url == "http://example.com"  # From p2
        assert merged.pdf_url == "http://example.com/pdf"  # From p2
@


\subsection{Search filters}
\label{sec:search-filters}

Structured literature reviews often require filtering by publication year,
open access availability, venue, citation count, or publication type.
Different providers support different filters natively, so we define a common
filter specification that each provider interprets according to its capabilities.

The [[SearchFilters]] dataclass captures these common filtering criteria.
Providers that don't support a particular filter either ignore it (with a
warning) or embed it in the query string where possible.

<<classes>>=
@dataclass
class SearchFilters:
    """
    Common filtering criteria for academic paper searches.

    Fields:
        year: Publication year or range (e.g., "2020", "2020-2024", "2020-", "-2024")
        open_access: If True, only return open access papers
        venue: Filter by venue/journal name (substring match)
        min_citations: Minimum citation count
        pub_types: List of publication types to include
            (article, conference, review, book, preprint, dataset)
    """

    year: str | None = None
    open_access: bool = False
    venue: str | None = None
    min_citations: int | None = None
    pub_types: list[str] | None = None

    <<search filters methods>>
@

\subsubsection{Parsing year ranges}

The [[year]] field accepts several formats for flexibility:
\begin{description}
\item[Single year] [[2020]] matches papers from 2020 only.
\item[Range] [[2020-2024]] matches papers from 2020 through 2024.
\item[Open start] [[-2024]] matches papers up to and including 2024.
\item[Open end] [[2020-]] matches papers from 2020 onwards.
\end{description}

The [[year_range]] method parses these formats and returns a tuple of
[[(start_year, end_year)]], where [[None]] indicates an open boundary:

<<search filters methods>>=
def year_range(self) -> tuple[int | None, int | None]:
    """
    Parse the year field into a (start, end) tuple.

    Returns:
        Tuple of (start_year, end_year) where None indicates open boundary.
        Returns (None, None) if no year filter is set.

    Raises:
        ValueError: If year format is invalid.
    """
    if not self.year:
        return (None, None)

    year_str = self.year.strip()

    # Single year: "2020"
    if year_str.isdigit():
        year = int(year_str)
        return (year, year)

    # Range with dash
    if "-" in year_str:
        parts = year_str.split("-", 1)

        # Open start: "-2024"
        if parts[0] == "":
            return (None, int(parts[1]))

        # Open end: "2020-"
        if parts[1] == "":
            return (int(parts[0]), None)

        # Full range: "2020-2024"
        return (int(parts[0]), int(parts[1]))

    raise ValueError(f"Invalid year format: {year_str}")
@

\subsubsection{Serialization for display and caching}

For display in search results and for cache key generation, we need to
serialize the filters to a dictionary and generate a stable cache key:

<<search filters methods>>=
def as_dict(self) -> dict:
    """
    Convert filters to a dictionary for display or storage.

    Only includes non-default (active) filters.
    """
    result = {}
    if self.year:
        result["year"] = self.year
    if self.open_access:
        result["open_access"] = True
    if self.venue:
        result["venue"] = self.venue
    if self.min_citations is not None:
        result["min_citations"] = self.min_citations
    if self.pub_types:
        result["pub_types"] = self.pub_types
    return result

def cache_key(self) -> str:
    """
    Generate a stable string key for caching filtered searches.

    Returns an empty string if no filters are active, so unfiltered
    searches maintain backward compatibility with existing cache entries.
    """
    parts = []
    if self.year:
        parts.append(f"y:{self.year}")
    if self.open_access:
        parts.append("oa:1")
    if self.venue:
        parts.append(f"v:{self.venue}")
    if self.min_citations is not None:
        parts.append(f"c:{self.min_citations}")
    if self.pub_types:
        parts.append(f"t:{','.join(sorted(self.pub_types))}")
    return "|".join(parts)

def is_empty(self) -> bool:
    """Return True if no filters are active."""
    return not any([
        self.year,
        self.open_access,
        self.venue,
        self.min_citations is not None,
        self.pub_types,
    ])

def matches(self, paper: Paper) -> bool:
    """
    Check if a paper matches all active filters.
    
    Args:
        paper: The Paper object to check.
        
    Returns:
        True if the paper matches all active filters, False otherwise.
        Returns True if no filters are active.
    """
    if self.is_empty():
        return True
    
    # Year filter
    if self.year:
        try:
            start, end = self.year_range()
            if paper.year is None:
                return False
            if start is not None and paper.year < start:
                return False
            if end is not None and paper.year > end:
                return False
        except ValueError:
            pass  # Invalid year format, skip filter
    
    # Venue filter (case-insensitive substring match)
    if self.venue:
        if not paper.venue:
            return False
        if self.venue.lower() not in paper.venue.lower():
            return False
    
    # Citation count filter
    if self.min_citations is not None:
        paper_citations = getattr(paper, 'citation_count', None)
        if paper_citations is None or paper_citations < self.min_citations:
            return False
    
    # Open access filter
    if self.open_access:
        paper_oa = getattr(paper, 'open_access', False)
        if not paper_oa:
            return False
    
    # Publication type filter
    if self.pub_types:
        paper_type = getattr(paper, 'publication_type', None)
        if paper_type is None or paper_type not in self.pub_types:
            return False
    
    return True
@

\subsubsection{Filtering paper collections}

For convenience, we provide a module-level function to filter a list of papers
using a [[SearchFilters]] object. This is useful for post-hoc filtering of
search results without re-querying the providers.

<<functions>>=
def filter_papers(papers: list[Paper], filters: SearchFilters) -> list[Paper]:
    """
    Filter a list of papers using the given filters.
    
    This function applies filters locally to an existing collection of papers,
    without making any API calls. Useful for narrowing down results from
    multiple merged searches.
    
    Args:
        papers: List of Paper objects to filter.
        filters: SearchFilters specifying the filtering criteria.
        
    Returns:
        List of papers matching all active filters.
        
    Example:
        >>> from scholar import Paper, SearchFilters, filter_papers
        >>> papers = [Paper(title="ML Paper", year=2020), 
        ...           Paper(title="Old Paper", year=2015)]
        >>> filters = SearchFilters(year="2018-")
        >>> filter_papers(papers, filters)
        [Paper(title="ML Paper", year=2020)]
    """
    if filters.is_empty():
        return papers
    return [p for p in papers if filters.matches(p)]
@

\subsubsection{Testing SearchFilters}

Let's verify that year parsing and serialization work correctly:

<<test functions>>=
class TestSearchFilters:
    """Tests for the SearchFilters class."""

    def test_year_range_single_year(self):
        """Single year returns same start and end."""
        f = SearchFilters(year="2020")
        assert f.year_range() == (2020, 2020)

    def test_year_range_full_range(self):
        """Full range returns both bounds."""
        f = SearchFilters(year="2020-2024")
        assert f.year_range() == (2020, 2024)

    def test_year_range_open_start(self):
        """Open start range returns None for start."""
        f = SearchFilters(year="-2024")
        assert f.year_range() == (None, 2024)

    def test_year_range_open_end(self):
        """Open end range returns None for end."""
        f = SearchFilters(year="2020-")
        assert f.year_range() == (2020, None)

    def test_year_range_no_year(self):
        """No year filter returns (None, None)."""
        f = SearchFilters()
        assert f.year_range() == (None, None)

    def test_year_range_invalid_format(self):
        """Invalid year format raises ValueError."""
        f = SearchFilters(year="invalid")
        with pytest.raises(ValueError, match="Invalid year format"):
            f.year_range()

    def test_as_dict_empty(self):
        """Empty filters produce empty dict."""
        f = SearchFilters()
        assert f.as_dict() == {}

    def test_as_dict_with_values(self):
        """Active filters appear in dict."""
        f = SearchFilters(
            year="2020-2024",
            open_access=True,
            venue="Nature",
            min_citations=10,
            pub_types=["article", "conference"],
        )
        d = f.as_dict()
        assert d["year"] == "2020-2024"
        assert d["open_access"] is True
        assert d["venue"] == "Nature"
        assert d["min_citations"] == 10
        assert d["pub_types"] == ["article", "conference"]

    def test_cache_key_empty(self):
        """Empty filters produce empty cache key."""
        f = SearchFilters()
        assert f.cache_key() == ""

    def test_cache_key_with_values(self):
        """Active filters produce stable cache key."""
        f = SearchFilters(year="2020", open_access=True, min_citations=5)
        key = f.cache_key()
        assert "y:2020" in key
        assert "oa:1" in key
        assert "c:5" in key

    def test_cache_key_stable_ordering(self):
        """Cache key is stable regardless of pub_types order."""
        f1 = SearchFilters(pub_types=["article", "conference"])
        f2 = SearchFilters(pub_types=["conference", "article"])
        assert f1.cache_key() == f2.cache_key()

    def test_is_empty_true(self):
        """is_empty returns True for default filters."""
        f = SearchFilters()
        assert f.is_empty() is True

    def test_is_empty_false(self):
        """is_empty returns False when any filter is set."""
        assert SearchFilters(year="2020").is_empty() is False
        assert SearchFilters(open_access=True).is_empty() is False
        assert SearchFilters(venue="Nature").is_empty() is False
        assert SearchFilters(min_citations=0).is_empty() is False
        assert SearchFilters(pub_types=["article"]).is_empty() is False

    def test_matches_empty_filters(self):
        """Empty filters match any paper."""
        f = SearchFilters()
        paper = Paper(title="Test", authors=[], year=2020)
        assert f.matches(paper) is True

    def test_matches_year_in_range(self):
        """Paper within year range matches."""
        f = SearchFilters(year="2018-2022")
        assert f.matches(Paper(title="Test", authors=[], year=2020)) is True
        assert f.matches(Paper(title="Test", authors=[], year=2018)) is True
        assert f.matches(Paper(title="Test", authors=[], year=2022)) is True

    def test_matches_year_out_of_range(self):
        """Paper outside year range doesn't match."""
        f = SearchFilters(year="2018-2022")
        assert f.matches(Paper(title="Test", authors=[], year=2017)) is False
        assert f.matches(Paper(title="Test", authors=[], year=2023)) is False

    def test_matches_year_none(self):
        """Paper without year doesn't match year filter."""
        f = SearchFilters(year="2020")
        assert f.matches(Paper(title="Test", authors=[], year=None)) is False

    def test_matches_venue(self):
        """Venue filter uses case-insensitive substring match."""
        f = SearchFilters(venue="Nature")
        assert f.matches(Paper(title="Test", authors=[], venue="Nature Communications")) is True
        assert f.matches(Paper(title="Test", authors=[], venue="nature")) is True
        assert f.matches(Paper(title="Test", authors=[], venue="Science")) is False
        assert f.matches(Paper(title="Test", authors=[], venue=None)) is False

    def test_matches_multiple_filters(self):
        """All active filters must match."""
        f = SearchFilters(year="2020-", venue="Nature")
        assert f.matches(Paper(title="Test", authors=[], year=2021, venue="Nature")) is True
        assert f.matches(Paper(title="Test", authors=[], year=2019, venue="Nature")) is False
        assert f.matches(Paper(title="Test", authors=[], year=2021, venue="Science")) is False


class TestFilterPapers:
    """Tests for the filter_papers function."""

    def test_filter_papers_empty_filters(self):
        """Empty filters return all papers."""
        papers = [Paper(title="A", authors=[], year=2020), Paper(title="B", authors=[], year=2015)]
        result = filter_papers(papers, SearchFilters())
        assert result == papers

    def test_filter_papers_by_year(self):
        """Filters papers by year range."""
        papers = [
            Paper(title="A", authors=[], year=2020),
            Paper(title="B", authors=[], year=2015),
            Paper(title="C", authors=[], year=2022),
        ]
        result = filter_papers(papers, SearchFilters(year="2018-"))
        assert len(result) == 2
        assert all(p.year >= 2018 for p in result)

    def test_filter_papers_preserves_order(self):
        """Filtered list preserves original order."""
        papers = [
            Paper(title="A", authors=[], year=2020),
            Paper(title="B", authors=[], year=2021),
            Paper(title="C", authors=[], year=2022),
        ]
        result = filter_papers(papers, SearchFilters(year="2020-"))
        assert [p.title for p in result] == ["A", "B", "C"]
@


\subsection{Search results}

A search result bundles papers with metadata about the search itself, supporting
reproducibility:

<<classes>>=
@dataclass
class SearchResult:
    """Represents the result of a search query."""

    query: str
    provider: str
    timestamp: str
    papers: list[Paper]
    filters: dict | None = None

    <<search result methods>>
@

The [[filters]] dictionary captures any provider-specific constraints (date
ranges, document types, etc.) that were applied.

\subsubsection{Combining results}

We can merge results from multiple searches, deduplicating and consolidating
papers.
When the same paper appears from multiple providers, we consolidate the data
to get the most complete record (keeping abstracts, DOIs, etc.\ from whichever
source has them).

<<search result methods>>=
def merge(self, other: "SearchResult") -> "SearchResult":
    """Merge two search results, deduplicating and consolidating papers."""
    logger.debug(f"Merging search results: {len(self.papers)} + {len(other.papers)} papers")
    
    # Use dict keyed by paper identity to enable consolidation
    paper_map: dict[Paper, Paper] = {}

    for paper in self.papers:
        paper_map[paper] = paper

    duplicates = 0
    for paper in other.papers:
        if paper in paper_map:
            # Consolidate with existing paper
            paper_map[paper] = paper_map[paper].merge_with(paper)
            duplicates += 1
        else:
            paper_map[paper] = paper

    merged_count = len(paper_map)
    logger.info(f"Merged results: {merged_count} unique papers ({duplicates} duplicates removed)")

    return SearchResult(
        query=f"{self.query} | {other.query}",
        provider=f"{self.provider}, {other.provider}",
        timestamp=self.timestamp,
        papers=list(paper_map.values()),
        filters=None,
    )
@

\subsubsection{Testing search results}

The merge operation should deduplicate and consolidate papers:

<<test functions>>=
class TestSearchResult:
    """Tests for the SearchResult class."""

    def test_merge_deduplicates(self):
        """Merging results deduplicates papers."""
        p1 = Paper(title="Paper A", authors=["A"])
        p2 = Paper(title="Paper B", authors=["B"])
        p3 = Paper(title="Paper A", authors=["A"])  # Duplicate of p1

        r1 = SearchResult(
            query="test",
            provider="p1",
            timestamp="2024-01-01",
            papers=[p1, p2],
        )
        r2 = SearchResult(
            query="test",
            provider="p2",
            timestamp="2024-01-01",
            papers=[p2, p3],
        )

        merged = r1.merge(r2)
        assert len(merged.papers) == 2

    def test_merge_consolidates_data(self):
        """Merging consolidates data from duplicate papers."""
        # DBLP paper without abstract
        p1 = Paper(
            title="Paper A", authors=["Author"], doi="10.1/a",
            source="dblp"
        )
        # Semantic Scholar paper with abstract
        p2 = Paper(
            title="Paper A", authors=["Author"], doi="10.1/a",
            abstract="This is the abstract", source="s2"
        )

        r1 = SearchResult(
            query="test", provider="dblp",
            timestamp="2024-01-01", papers=[p1]
        )
        r2 = SearchResult(
            query="test", provider="s2",
            timestamp="2024-01-01", papers=[p2]
        )

        merged = r1.merge(r2)
        assert len(merged.papers) == 1
        # Consolidated paper should have the abstract
        assert merged.papers[0].abstract == "This is the abstract"
        # And combined sources
        assert "dblp" in merged.papers[0].source
        assert "s2" in merged.papers[0].source
@


\subsection{Search abstraction}

The [[Search]] class provides the main interface for conducting searches.
It follows a builder-like pattern: create a [[Search]] object, optionally
configure it, then call [[execute()]] to run the search.

The class accumulates results in [[self.results]], allowing:
\begin{itemize}
\item Multiple calls to [[execute()]] with different provider subsets
\item Inspection of per-provider results before merging
\item Iterative refinement of searches
\end{itemize}

<<classes>>=
class Search:
    """Manages a structured search across bibliographic databases."""

    def __init__(self, query: str):
        """Initialize a search with the given query."""
        self.query = query
        self.results: list[SearchResult] = []

    <<search methods>>
@


\section{Search functionality}

With the data model in place, we implement the search functionality.
For now, we provide a simple interface; provider implementations will be added
as the project develops.

\subsection{Simple search function}

A convenience function for quick searches:

<<functions>>=
def search(query: str) -> SearchResult:
    """
    Perform a simple search and return results.

    This is a convenience function for quick searches.
    For more control, use the Search class directly.
    """
    s = Search(query)
    s.execute()
    return s.results[0] if s.results else SearchResult(
        query=query,
        provider="none",
        timestamp=datetime.now().isoformat(),
        papers=[],
    )
@

\subsection{Executing searches}

The [[Search]] class orchestrates queries across providers.

\subsubsection{Lazy Import Pattern}

We import the provider registry inside [[execute()]] rather than at module
level. This avoids circular imports: [[providers.py]] imports [[Paper]] from
this module, so if we imported [[providers]] here at module level, Python
would encounter an import cycle.

The lazy import pattern has a small runtime cost (import check on each call)
but is negligible compared to API call latency.

<<search methods>>=
def execute(
    self,
    providers: list[str] | None = None,
    limit: int = 100,
    filters: SearchFilters | None = None,
) -> list[SearchResult]:
    """
    Execute the search across specified providers.

    Args:
        providers: List of provider names to search. If None, uses the
            default providers (openalex, dblp).
        limit: Maximum number of results per provider.
        filters: Optional SearchFilters to apply to the search.

    Returns:
        List of SearchResult objects, one per provider.
    """
    from scholar.providers import get_provider, get_default_providers

    timestamp = datetime.now().isoformat()

    # Determine which providers to use
    if providers is None:
        provider_list = get_default_providers()
    else:
        provider_list = [
            get_provider(name)
            for name in providers
            if get_provider(name) is not None
        ]

    logger.info(f"Executing search for query: '{self.query}'")
    logger.debug(f"Using {len(provider_list)} provider(s): {[p.name for p in provider_list]}")

    # Query each provider
    for provider in provider_list:
        logger.debug(f"Querying {provider.name} with limit={limit}")
        papers = provider.search(self.query, limit=limit, filters=filters)
        logger.info(f"{provider.name}: Retrieved {len(papers)} papers")
        result = SearchResult(
            query=self.query,
            provider=provider.name,
            timestamp=timestamp,
            papers=papers,
            filters=filters.as_dict() if filters else None,
        )
        self.results.append(result)

    logger.info(f"Search complete: {len(self.results)} result set(s)")
    return self.results
@

\subsubsection{Testing the search interface}

The search function should return a valid result:

<<test functions>>=
class TestSearch:
    """Tests for the Search class and search function."""

    def test_search_returns_result(self, monkeypatch):
        """search() returns a SearchResult."""
        from scholar import providers

        mock_provider = Mock()
        mock_provider.name = "mock"
        mock_provider.search.return_value = [
            Paper(title="Test", authors=["Author"])
        ]
        monkeypatch.setattr(providers, "PROVIDERS", {"mock": mock_provider})

        result = search("test query")
        assert isinstance(result, SearchResult)
        assert result.query == "test query"

    def test_search_class_execute(self, monkeypatch):
        """Search.execute() returns results when providers are specified."""
        from scholar import providers

        mock_provider = Mock()
        mock_provider.name = "mock"
        mock_provider.search.return_value = []
        monkeypatch.setattr(providers, "PROVIDERS", {"mock": mock_provider})

        s = Search("test")
        # Explicitly specify the mock provider since "mock" is not a default
        results = s.execute(providers=["mock"])
        assert len(results) > 0
@


\section{Dependencies}

We collect the imports used throughout the module:

<<imports>>=
from dataclasses import dataclass
from datetime import datetime
import logging

logger = logging.getLogger(__name__)
@

<<constants>>=
VERSION = "0.1.0"
@
