\chapter{Review Session Management}
\label{review-module}

\section{Introduction}

This module provides the core data structures and functions for managing
systematic literature review sessions. It separates the domain logic from
the TUI, enabling programmatic review workflows.

\subsection{Design Goals}

The review module is designed with these goals:
\begin{description}
\item[TUI Independence] All review functionality should be usable from Python
  code without any TUI dependencies.
\item[Tag-Based Decisions] Decisions support multiple tags (themes for kept
  papers, motivations for discarded papers) rather than single strings.
\item[Session Persistence] Review sessions can be saved and loaded, supporting
  interrupted workflows.
\item[Programmatic Access] Enable automation of review decisions based on
  paper content, keywords, or other criteria.
\end{description}

\subsection{Tags: Themes and Motivations}

The module uses a unified \enquote{tags} concept for both:
\begin{description}
\item[Themes] Optional tags for kept papers, categorizing them by topic or
  relevance (e.g., \enquote{privacy-focused}, \enquote{federated-learning}).
\item[Motivations] Required tags for discarded papers, explaining why they
  were excluded (e.g., \enquote{wrong-domain}, \enquote{not-peer-reviewed}).
\end{description}

Both are stored in the same [[tags]] field, with the semantic difference
determined by the decision status. Discarded papers require at least one
tag (motivation), while kept papers may have zero or more tags (themes).

\section{Module Structure}

<<[[review.py]]>>=
"""
Core review session management for systematic literature reviews.

Provides data structures and functions for managing paper review decisions,
session persistence, and report generation.
"""
<<review imports>>
<<review constants>>
<<review data structures>>
<<review session functions>>
<<review filtering functions>>
<<review report generation>>
@


\section{Testing}
\label{sec:review-tests}

Tests are distributed throughout this document, appearing after each
implementation section they verify.

<<test [[review.py]]>>=
"""Tests for the review module."""
import pytest
from datetime import datetime
from pathlib import Path
from unittest.mock import Mock

from scholar.review import *
from scholar.scholar import Paper, SearchFilters


<<test functions>>
@


\section{Imports and Constants}

<<review imports>>=
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any
import json
import logging

from scholar.scholar import Paper, SearchFilters
from scholar.notes import (
    get_paper_id, get_query_hash, get_data_dir,
    load_search_decisions, save_search_decisions as notes_save_decisions,
    ReviewDecisionRecord,
)

logger = logging.getLogger(__name__)
@

<<review constants>>=
SESSIONS_DIR = "review_sessions"
@


\section{Data Structures}

\subsection{Decision Status}

The decision status uses an enum to ensure type safety and clear semantics.

<<review data structures>>=
class DecisionStatus(Enum):
    """Status of a paper review decision."""
    PENDING = "pending"
    KEPT = "kept"
    DISCARDED = "discarded"
@

\subsection{Review Source}

When using LLM-assisted review, we track where each decision originated.
This enables:
\begin{description}
\item[Learning from corrections] When a user corrects an LLM decision, that
  paper becomes a training example for future LLM rounds.
\item[Progress tracking] Users can see how many papers were reviewed by humans
  versus the LLM, and how many LLM decisions are pending review.
\item[Confidence prioritization] LLM decisions with low confidence can be
  prioritized for human review.
\end{description}

<<review data structures>>=
class ReviewSource(Enum):
    """Source of a review decision."""
    HUMAN = "human"              # User decided directly
    LLM_UNREVIEWED = "llm"       # LLM decided, not yet reviewed by user
    LLM_REVIEWED = "llm_reviewed"  # LLM decided, user confirmed or changed
@

\subsection{Review Decision}

A [[ReviewDecision]] captures the review state of a single paper, including
its status and associated tags.

<<review data structures>>=
@dataclass
class ReviewDecision:
    """
    A review decision for a single paper.

    Tracks whether the paper was kept or discarded during review,
    along with tags (themes for kept, motivations for discarded).

    Attributes:
        paper: The Paper being reviewed.
        provider: Which search provider returned this paper.
        status: Current decision status (pending/kept/discarded).
        tags: List of tags (themes for kept, motivations for discarded).
        source: Where the decision came from (human or LLM).
        is_example: True if this paper is a training example for LLM.
        llm_confidence: Confidence score from LLM (0.0-1.0), if applicable.
    """
    paper: Paper
    provider: str
    status: DecisionStatus = DecisionStatus.PENDING
    tags: list[str] = field(default_factory=list)
    source: ReviewSource = ReviewSource.HUMAN
    is_example: bool = False
    llm_confidence: float | None = None
    
    @property
    def is_decided(self) -> bool:
        """Check if a decision has been made."""
        return self.status != DecisionStatus.PENDING
    
    @property
    def is_valid(self) -> bool:
        """
        Check if the decision is valid.
        
        Discarded papers require at least one tag (motivation).
        Kept and pending papers are always valid.
        """
        if self.status == DecisionStatus.DISCARDED:
            return len(self.tags) > 0
        return True
    
    def add_tag(self, tag: str) -> None:
        """Add a tag if not already present."""
        if tag and tag not in self.tags:
            self.tags.append(tag)
    
    def remove_tag(self, tag: str) -> None:
        """Remove a tag if present."""
        if tag in self.tags:
            self.tags.remove(tag)
    
    def has_tag(self, tag: str) -> bool:
        """Check if this decision has a specific tag."""
        return tag in self.tags
    
    def clear_tags(self) -> None:
        """Remove all tags."""
        self.tags.clear()
    
    # Backward compatibility for code using 'motivation' as single string
    @property
    def motivation(self) -> str:
        """Get first tag as motivation (backward compatibility)."""
        return self.tags[0] if self.tags else ""
    
    @motivation.setter
    def motivation(self, value: str) -> None:
        """Set motivation as single tag (backward compatibility)."""
        self.tags = [value] if value else []
@

\subsection{Testing ReviewDecision}

<<test functions>>=
class TestReviewDecision:
    """Tests for ReviewDecision dataclass."""

    def test_default_status_is_pending(self):
        """New decisions should be pending."""
        paper = Paper(title="Test", authors=["Author"], year=2024)
        decision = ReviewDecision(paper=paper, provider="test")
        assert decision.status == DecisionStatus.PENDING
        assert not decision.is_decided

    def test_kept_is_decided(self):
        """Kept papers are decided."""
        paper = Paper(title="Test", authors=["Author"], year=2024)
        decision = ReviewDecision(
            paper=paper,
            provider="test",
            status=DecisionStatus.KEPT
        )
        assert decision.is_decided

    def test_discarded_requires_tag(self):
        """Discarded papers without tags are invalid."""
        paper = Paper(title="Test", authors=["Author"], year=2024)
        decision = ReviewDecision(
            paper=paper,
            provider="test",
            status=DecisionStatus.DISCARDED,
            tags=[]
        )
        assert not decision.is_valid
        
        decision.add_tag("Not relevant")
        assert decision.is_valid

    def test_add_remove_tags(self):
        """Can add and remove tags."""
        paper = Paper(title="Test", authors=["Author"], year=2024)
        decision = ReviewDecision(paper=paper, provider="test")
        
        decision.add_tag("theme1")
        decision.add_tag("theme2")
        assert decision.has_tag("theme1")
        assert len(decision.tags) == 2
        
        # Adding duplicate doesn't add again
        decision.add_tag("theme1")
        assert len(decision.tags) == 2
        
        decision.remove_tag("theme1")
        assert not decision.has_tag("theme1")
        assert len(decision.tags) == 1

    def test_backward_compat_motivation(self):
        """motivation property works for backward compatibility."""
        paper = Paper(title="Test", authors=["Author"], year=2024)
        decision = ReviewDecision(paper=paper, provider="test")

        decision.motivation = "Not relevant"
        assert decision.tags == ["Not relevant"]
        assert decision.motivation == "Not relevant"

        decision.motivation = ""
        assert decision.tags == []
        assert decision.motivation == ""

    def test_default_source_is_human(self):
        """New decisions default to human source."""
        paper = Paper(title="Test", authors=["Author"], year=2024)
        decision = ReviewDecision(paper=paper, provider="test")
        assert decision.source == ReviewSource.HUMAN
        assert not decision.is_example
        assert decision.llm_confidence is None

    def test_llm_source_tracking(self):
        """Can track LLM source and confidence."""
        paper = Paper(title="Test", authors=["Author"], year=2024)
        decision = ReviewDecision(
            paper=paper,
            provider="test",
            status=DecisionStatus.KEPT,
            source=ReviewSource.LLM_UNREVIEWED,
            llm_confidence=0.85,
        )
        assert decision.source == ReviewSource.LLM_UNREVIEWED
        assert decision.llm_confidence == 0.85

    def test_example_flag(self):
        """Can mark a decision as a training example."""
        paper = Paper(title="Test", authors=["Author"], year=2024)
        decision = ReviewDecision(
            paper=paper,
            provider="test",
            is_example=True,
        )
        assert decision.is_example
@


\subsection{Review Session}

A [[ReviewSession]] contains all decisions for a review, along with metadata
about the search and analysis methods.

The session tracks which query was used with which provider through the
[[query_provider_pairs]] field. This enables using different query syntaxes for
different providers---for example, natural language queries for OpenAlex and
Boolean syntax for Web of Science. The [[query]] and [[providers]] fields are
retained for backward compatibility with older session files.

<<review data structures>>=
@dataclass
class ReviewSession:
    """
    A complete review session with all papers and decisions.

    Maintains the search parameters and all review decisions,
    supporting sorting, filtering, and analysis operations.

    The [[research_context]] field stores a description of the research goals,
    which is used by the LLM to understand what types of papers to keep or
    discard. This context is reused across LLM classification rounds.
    """
    query: str
    providers: list[str]
    timestamp: datetime
    decisions: list[ReviewDecision] = field(default_factory=list)
    name: str | None = None  # Optional session name for persistence
    research_context: str | None = None  # Research goals for LLM review
    query_provider_pairs: list[tuple[str, str]] = field(default_factory=list)
    
    @property
    def kept_papers(self) -> list[ReviewDecision]:
        """Get all papers marked as kept."""
        return [d for d in self.decisions if d.status == DecisionStatus.KEPT]
    
    @property
    def discarded_papers(self) -> list[ReviewDecision]:
        """Get all papers marked as discarded."""
        return [d for d in self.decisions if d.status == DecisionStatus.DISCARDED]
    
    @property
    def pending_papers(self) -> list[ReviewDecision]:
        """Get all papers still pending review."""
        return [d for d in self.decisions if d.status == DecisionStatus.PENDING]
    
    def all_themes(self) -> set[str]:
        """Get all unique themes from kept papers."""
        themes: set[str] = set()
        for d in self.kept_papers:
            themes.update(d.tags)
        return themes
    
    def all_motivations(self) -> set[str]:
        """Get all unique motivations from discarded papers."""
        motivations: set[str] = set()
        for d in self.discarded_papers:
            motivations.update(d.tags)
        return motivations
    
    def theme_counts(self) -> dict[str, int]:
        """Count papers per theme."""
        counts: dict[str, int] = {}
        for d in self.kept_papers:
            for tag in d.tags:
                counts[tag] = counts.get(tag, 0) + 1
        return counts
    
    def motivation_counts(self) -> dict[str, int]:
        """Count papers per motivation."""
        counts: dict[str, int] = {}
        for d in self.discarded_papers:
            for tag in d.tags:
                counts[tag] = counts.get(tag, 0) + 1
        return counts
    
    def papers_with_tag(self, tag: str) -> list[ReviewDecision]:
        """Get all decisions with a specific tag."""
        return [d for d in self.decisions if d.has_tag(tag)]
    
    def sort_by(self, key: str, reverse: bool = False) -> None:
        """
        Sort decisions by a paper attribute.
        
        Supported keys: title, year, author, provider
        """
        if key == "title":
            self.decisions.sort(key=lambda d: d.paper.title.lower(), reverse=reverse)
        elif key == "year":
            self.decisions.sort(key=lambda d: d.paper.year or 0, reverse=reverse)
        elif key == "author":
            self.decisions.sort(
                key=lambda d: d.paper.authors[0].lower() if d.paper.authors else "",
                reverse=reverse
            )
        elif key == "provider":
            self.decisions.sort(key=lambda d: d.provider.lower(), reverse=reverse)

    def llm_unreviewed_papers(self) -> list[ReviewDecision]:
        """Get papers decided by LLM but not yet reviewed by user."""
        return [
            d for d in self.decisions
            if d.source == ReviewSource.LLM_UNREVIEWED
        ]

    def example_papers(self) -> list[ReviewDecision]:
        """
        Get papers that serve as training examples for LLM.

        This includes:
        - Human decisions that have tags (themes or motivations)
        - User corrections of LLM decisions (is_example=True)
        """
        return [
            d for d in self.decisions
            if d.is_decided and d.tags and (
                d.source == ReviewSource.HUMAN or d.is_example
            )
        ]

    def llm_review_statistics(self) -> dict[str, int]:
        """
        Get statistics about LLM review progress.

        Returns dict with counts:
        - human: Human-only decisions
        - llm_unreviewed: LLM decisions pending review
        - llm_reviewed: LLM decisions confirmed by user
        - examples: Training examples for LLM
        - pending: Papers not yet decided
        """
        human = sum(
            1 for d in self.decisions
            if d.source == ReviewSource.HUMAN and d.is_decided
        )
        llm_unreviewed = sum(
            1 for d in self.decisions
            if d.source == ReviewSource.LLM_UNREVIEWED
        )
        llm_reviewed = sum(
            1 for d in self.decisions
            if d.source == ReviewSource.LLM_REVIEWED
        )
        examples = len(self.example_papers())
        pending = len(self.pending_papers)

        return {
            "human": human,
            "llm_unreviewed": llm_unreviewed,
            "llm_reviewed": llm_reviewed,
            "examples": examples,
            "pending": pending,
        }

    <<query-provider pair methods>>
@

We provide helper methods for managing query-provider pairs. The
[[add_query_provider_pair]] method adds a pair only if it doesn't already exist,
preventing duplicates when appending results to an existing session. The
[[queries_for_provider]] method retrieves all queries used with a specific
provider, useful for displaying in reports and the UI.

<<query-provider pair methods>>=
def add_query_provider_pair(self, query: str, provider: str) -> None:
    """
    Add a query-provider pair if not already present.

    This tracks which query was used with which provider, enabling
    different queries for different providers (e.g., natural language
    for OpenAlex, Boolean syntax for Web of Science).
    """
    pair = (query, provider)
    if pair not in self.query_provider_pairs:
        self.query_provider_pairs.append(pair)

def queries_for_provider(self, provider: str) -> list[str]:
    """
    Get all queries used with a specific provider.

    Returns a list of queries that were used to search the given provider.
    """
    return [q for q, p in self.query_provider_pairs if p == provider]
@

\subsection{Testing ReviewSession}

<<test functions>>=
class TestReviewSession:
    """Tests for ReviewSession dataclass."""

    @pytest.fixture
    def sample_session(self):
        """Create a sample session with mixed decisions."""
        papers = [
            Paper(title="Paper A", authors=["Author A"], year=2020),
            Paper(title="Paper B", authors=["Author B"], year=2022),
            Paper(title="Paper C", authors=["Author C"], year=2021),
        ]
        session = ReviewSession(
            query="test query",
            providers=["provider1"],
            timestamp=datetime.now(),
        )
        session.decisions = [
            ReviewDecision(
                paper=papers[0], provider="p1", 
                status=DecisionStatus.KEPT, tags=["privacy", "ml"]
            ),
            ReviewDecision(
                paper=papers[1], provider="p1", 
                status=DecisionStatus.DISCARDED, tags=["off-topic"]
            ),
            ReviewDecision(paper=papers[2], provider="p1", status=DecisionStatus.PENDING),
        ]
        return session

    def test_kept_papers(self, sample_session):
        """Test filtering kept papers."""
        kept = sample_session.kept_papers
        assert len(kept) == 1
        assert kept[0].paper.title == "Paper A"

    def test_discarded_papers(self, sample_session):
        """Test filtering discarded papers."""
        discarded = sample_session.discarded_papers
        assert len(discarded) == 1
        assert discarded[0].paper.title == "Paper B"
        assert "off-topic" in discarded[0].tags

    def test_all_themes(self, sample_session):
        """Test getting all themes."""
        themes = sample_session.all_themes()
        assert themes == {"privacy", "ml"}

    def test_all_motivations(self, sample_session):
        """Test getting all motivations."""
        motivations = sample_session.all_motivations()
        assert motivations == {"off-topic"}

    def test_theme_counts(self, sample_session):
        """Test counting papers per theme."""
        counts = sample_session.theme_counts()
        assert counts["privacy"] == 1
        assert counts["ml"] == 1

    def test_papers_with_tag(self, sample_session):
        """Test finding papers with specific tag."""
        papers = sample_session.papers_with_tag("privacy")
        assert len(papers) == 1
        assert papers[0].paper.title == "Paper A"

    def test_sort_by_year(self, sample_session):
        """Test sorting by year."""
        sample_session.sort_by("year")
        years = [d.paper.year for d in sample_session.decisions]
        assert years == [2020, 2021, 2022]

    def test_sort_by_year_reverse(self, sample_session):
        """Test sorting by year descending."""
        sample_session.sort_by("year", reverse=True)
        years = [d.paper.year for d in sample_session.decisions]
        assert years == [2022, 2021, 2020]

    def test_llm_unreviewed_papers(self):
        """Test getting LLM unreviewed papers."""
        session = ReviewSession(
            query="test",
            providers=["test"],
            timestamp=datetime.now(),
        )
        papers = [
            Paper(title="Human", authors=["A"], year=2024),
            Paper(title="LLM Unreviewed", authors=["B"], year=2024),
            Paper(title="LLM Reviewed", authors=["C"], year=2024),
        ]
        session.decisions = [
            ReviewDecision(
                paper=papers[0], provider="test",
                status=DecisionStatus.KEPT,
                source=ReviewSource.HUMAN,
            ),
            ReviewDecision(
                paper=papers[1], provider="test",
                status=DecisionStatus.KEPT,
                source=ReviewSource.LLM_UNREVIEWED,
            ),
            ReviewDecision(
                paper=papers[2], provider="test",
                status=DecisionStatus.KEPT,
                source=ReviewSource.LLM_REVIEWED,
            ),
        ]
        unreviewed = session.llm_unreviewed_papers()
        assert len(unreviewed) == 1
        assert unreviewed[0].paper.title == "LLM Unreviewed"

    def test_example_papers(self):
        """Test getting training example papers."""
        session = ReviewSession(
            query="test",
            providers=["test"],
            timestamp=datetime.now(),
        )
        papers = [
            Paper(title="Human with tags", authors=["A"], year=2024),
            Paper(title="Human without tags", authors=["B"], year=2024),
            Paper(title="LLM corrected", authors=["C"], year=2024),
            Paper(title="LLM not example", authors=["D"], year=2024),
        ]
        session.decisions = [
            ReviewDecision(
                paper=papers[0], provider="test",
                status=DecisionStatus.KEPT,
                tags=["ml"],
                source=ReviewSource.HUMAN,
            ),
            ReviewDecision(
                paper=papers[1], provider="test",
                status=DecisionStatus.KEPT,
                source=ReviewSource.HUMAN,
            ),
            ReviewDecision(
                paper=papers[2], provider="test",
                status=DecisionStatus.KEPT,
                tags=["corrected"],
                source=ReviewSource.LLM_REVIEWED,
                is_example=True,
            ),
            ReviewDecision(
                paper=papers[3], provider="test",
                status=DecisionStatus.KEPT,
                tags=["ml"],
                source=ReviewSource.LLM_REVIEWED,
                is_example=False,
            ),
        ]
        examples = session.example_papers()
        assert len(examples) == 2
        titles = {e.paper.title for e in examples}
        assert "Human with tags" in titles
        assert "LLM corrected" in titles

    def test_llm_review_statistics(self):
        """Test LLM review statistics."""
        session = ReviewSession(
            query="test",
            providers=["test"],
            timestamp=datetime.now(),
        )
        papers = [
            Paper(title=f"Paper {i}", authors=["A"], year=2024)
            for i in range(5)
        ]
        session.decisions = [
            ReviewDecision(
                paper=papers[0], provider="test",
                status=DecisionStatus.KEPT, tags=["ml"],
                source=ReviewSource.HUMAN,
            ),
            ReviewDecision(
                paper=papers[1], provider="test",
                status=DecisionStatus.KEPT,
                source=ReviewSource.LLM_UNREVIEWED,
            ),
            ReviewDecision(
                paper=papers[2], provider="test",
                status=DecisionStatus.DISCARDED, tags=["off-topic"],
                source=ReviewSource.LLM_REVIEWED,
            ),
            ReviewDecision(
                paper=papers[3], provider="test",
                status=DecisionStatus.PENDING,
            ),
            ReviewDecision(
                paper=papers[4], provider="test",
                status=DecisionStatus.KEPT, tags=["corrected"],
                source=ReviewSource.LLM_REVIEWED,
                is_example=True,
            ),
        ]
        stats = session.llm_review_statistics()
        assert stats["human"] == 1
        assert stats["llm_unreviewed"] == 1
        assert stats["llm_reviewed"] == 2
        assert stats["examples"] == 2  # Human with tags + corrected
        assert stats["pending"] == 1

    def test_research_context(self):
        """Test research context field."""
        session = ReviewSession(
            query="test",
            providers=["test"],
            timestamp=datetime.now(),
            research_context="Focus on privacy-preserving machine learning",
        )
        assert session.research_context == "Focus on privacy-preserving machine learning"

    def test_add_query_provider_pair(self):
        """Test adding query-provider pairs."""
        session = ReviewSession(
            query="test",
            providers=["openalex", "wos"],
            timestamp=datetime.now(),
        )
        session.add_query_provider_pair("llm programming education", "openalex")
        session.add_query_provider_pair("llm AND programming AND education", "wos")
        
        assert len(session.query_provider_pairs) == 2
        assert ("llm programming education", "openalex") in session.query_provider_pairs
        assert ("llm AND programming AND education", "wos") in session.query_provider_pairs

    def test_add_query_provider_pair_no_duplicates(self):
        """Test that duplicate pairs are not added."""
        session = ReviewSession(
            query="test",
            providers=["openalex"],
            timestamp=datetime.now(),
        )
        session.add_query_provider_pair("test query", "openalex")
        session.add_query_provider_pair("test query", "openalex")  # duplicate
        
        assert len(session.query_provider_pairs) == 1

    def test_queries_for_provider(self):
        """Test retrieving queries for a specific provider."""
        session = ReviewSession(
            query="test",
            providers=["openalex", "wos"],
            timestamp=datetime.now(),
        )
        session.add_query_provider_pair("natural language query", "openalex")
        session.add_query_provider_pair("another query", "openalex")
        session.add_query_provider_pair("boolean AND query", "wos")
        
        openalex_queries = session.queries_for_provider("openalex")
        wos_queries = session.queries_for_provider("wos")
        
        assert len(openalex_queries) == 2
        assert "natural language query" in openalex_queries
        assert "another query" in openalex_queries
        assert len(wos_queries) == 1
        assert "boolean AND query" in wos_queries
@


\section{Session Functions}

\subsection{Creating Sessions}

We provide factory functions for creating review sessions from search results.
When appending to an existing session (using the [[-n]] flag), the function
merges query-provider pairs from the previous session with pairs from the new
search results. This allows different queries to be used with different
providers across multiple searches.

<<review session functions>>=
def create_review_session(
    results: list[Any],  # list[SearchResult]
    query: str,
    session_name: str | None = None,
) -> ReviewSession:
    """
    Create a review session from search results.
    
    Loads any previous decisions for the same query/session and merges
    them with the new search results.
    
    Args:
        results: List of SearchResult objects from a search.
        query: The search query string.
        session_name: Optional name for the session (uses query if not provided).
    
    Returns:
        A ReviewSession ready for review.
    """
    logger.info("Creating review session for query: %s", query)
    # Use session_name for persistence, fall back to query
    persistence_key = session_name if session_name else query
    
    # Load previous decisions for this session
    previous_decisions = load_search_decisions(persistence_key)
    if previous_decisions:
        logger.debug("Loaded %d previous decisions for session", 
                    len(previous_decisions.decisions))
    
    # Try to load existing session for query_provider_pairs
    previous_session = load_session(persistence_key)
    
    # Create review session
    providers = list(set(r.provider for r in results))
    logger.debug("Session includes providers: %s", providers)
    session = ReviewSession(
        query=query,
        providers=providers,
        timestamp=datetime.now(),
        name=session_name,
    )
    
    # Build query-provider pairs from results
    for result in results:
        session.add_query_provider_pair(result.query, result.provider)
    
    # Merge with previous session's query-provider pairs (if any)
    if previous_session:
        for pair_query, pair_provider in previous_session.query_provider_pairs:
            session.add_query_provider_pair(pair_query, pair_provider)
    
    # Track existing paper IDs to handle merging
    existing_paper_ids: set[str] = set()
    if previous_decisions:
        existing_paper_ids = set(previous_decisions.decisions.keys())
        
        # First, restore decisions for papers from previous session
        # that are NOT in the current search results
        current_paper_ids = set()
        for result in results:
            for paper in result.papers:
                current_paper_ids.add(get_paper_id(paper))
        
        # Add papers from previous session that aren't in current results
        for paper_id, prev in previous_decisions.decisions.items():
            if paper_id not in current_paper_ids:
                # Reconstruct paper from stored data
                paper = Paper(
                    title=prev.title,
                    authors=prev.authors if prev.authors else [],
                    year=prev.year,
                    doi=prev.doi,
                    abstract=prev.abstract,
                    venue=prev.venue,
                    url=prev.url,
                    pdf_url=prev.pdf_url,
                    source=getattr(prev, 'source', "previous"),
                )
                status = DecisionStatus.PENDING
                if prev.status == "kept":
                    status = DecisionStatus.KEPT
                elif prev.status == "discarded":
                    status = DecisionStatus.DISCARDED
                
                # Handle both old (motivation) and new (tags) formats
                tags = _extract_tags_from_record(prev)
                
                session.decisions.append(
                    ReviewDecision(
                        paper=paper,
                        provider=getattr(prev, 'provider', "previous"),
                        status=status,
                        tags=tags,
                    )
                )
    
    # Convert search results to review decisions
    for result in results:
        for paper in result.papers:
            paper_id = get_paper_id(paper)
            
            # Check for previous decision
            prev = None
            if previous_decisions:
                prev = previous_decisions.decisions.get(paper_id)
            
            status = DecisionStatus.PENDING
            tags: list[str] = []
            if prev:
                if prev.status == "kept":
                    status = DecisionStatus.KEPT
                elif prev.status == "discarded":
                    status = DecisionStatus.DISCARDED
                tags = _extract_tags_from_record(prev)
            
            session.decisions.append(
                ReviewDecision(
                    paper=paper,
                    provider=result.provider,
                    status=status,
                    tags=tags,
                )
            )
    
    logger.info("Created session with %d papers (%d kept, %d discarded, %d pending)",
               len(session.decisions), len(session.kept_papers),
               len(session.discarded_papers), len(session.pending_papers))
    return session


def _extract_tags_from_record(record: ReviewDecisionRecord) -> list[str]:
    """
    Extract tags from a ReviewDecisionRecord.
    
    Handles both old format (motivation as string) and new format (tags as list).
    """
    # Check for new tags field first
    if hasattr(record, 'tags') and record.tags:
        return list(record.tags)
    # Fall back to old motivation field
    if hasattr(record, 'motivation') and record.motivation:
        return [record.motivation]
    return []


def create_notes_session() -> ReviewSession:
    """
    Create a session for browsing papers with notes.
    
    This is a notes-only session (no keep/discard) for reviewing
    previously annotated papers.
    
    Returns:
        A ReviewSession containing all papers with notes.
    """
    from scholar.notes import list_papers_with_notes
    
    papers_with_notes = list_papers_with_notes()
    
    session = ReviewSession(
        query="",
        providers=[],
        timestamp=datetime.now(),
    )
    
    for paper_note in papers_with_notes:
        paper = Paper(
            title=paper_note.title,
            authors=getattr(paper_note, 'authors', []),
            year=getattr(paper_note, 'year', None),
            doi=getattr(paper_note, 'doi', None),
            abstract=None,
            venue=getattr(paper_note, 'venue', None),
            url=getattr(paper_note, 'url', None),
            pdf_url=getattr(paper_note, 'pdf_url', None),
            source="notes",
        )
        session.decisions.append(
            ReviewDecision(paper=paper, provider="notes")
        )
    
    return session
@

\subsection{Session Persistence}

Sessions can be saved and loaded for interrupted workflows. The session file
includes [[query_provider_pairs]] to track which queries were used with which
providers. For backward compatibility, when loading old session files without
this field, we reconstruct the pairs by pairing the single [[query]] with each
provider in [[providers]].

<<review session functions>>=
def _get_sessions_dir() -> Path:
    """Get the directory for storing review sessions."""
    path = get_data_dir() / SESSIONS_DIR
    path.mkdir(parents=True, exist_ok=True)
    return path


def save_session(session: ReviewSession) -> Path:
    """
    Save a review session to disk.
    
    Args:
        session: The ReviewSession to save.
    
    Returns:
        Path to the saved session file.
    """
    # Use session name or query hash as filename
    if session.name:
        filename = f"{session.name}.json"
    else:
        filename = f"{get_query_hash(session.query)}.json"
    
    session_path = _get_sessions_dir() / filename
    logger.info("Saving session to %s", session_path)
    
    # Convert to serializable format
    data = {
        "query": session.query,
        "name": session.name,
        "providers": session.providers,
        "timestamp": session.timestamp.isoformat(),
        "research_context": session.research_context,
        "query_provider_pairs": session.query_provider_pairs,
        "decisions": [
            {
                "paper_id": get_paper_id(d.paper),
                "provider": d.provider,
                "status": d.status.value,
                "tags": d.tags,
                "source": d.source.value,
                "is_example": d.is_example,
                "llm_confidence": d.llm_confidence,
                "paper": {
                    "title": d.paper.title,
                    "authors": list(d.paper.authors),
                    "year": d.paper.year,
                    "doi": d.paper.doi,
                    "abstract": d.paper.abstract,
                    "venue": d.paper.venue,
                    "url": d.paper.url,
                    "pdf_url": d.paper.pdf_url,
                    "source": d.paper.source,
                },
            }
            for d in session.decisions
        ],
    }
    
    with open(session_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    
    logger.debug("Saved %d decisions (%d kept, %d discarded, %d pending)",
                len(session.decisions), len(session.kept_papers),
                len(session.discarded_papers), len(session.pending_papers))
    return session_path


def load_session(name_or_query: str) -> ReviewSession | None:
    """
    Load a review session from disk.
    
    Args:
        name_or_query: Session name or query string.
    
    Returns:
        The loaded ReviewSession, or None if not found.
    """
    logger.debug("Loading session: %s", name_or_query)
    sessions_dir = _get_sessions_dir()
    
    # Try exact name first
    session_path = sessions_dir / f"{name_or_query}.json"
    if not session_path.exists():
        # Try query hash
        session_path = sessions_dir / f"{get_query_hash(name_or_query)}.json"
    
    if not session_path.exists():
        logger.debug("Session not found: %s", name_or_query)
        return None
    
    logger.debug("Found session file: %s", session_path)
    try:
        with open(session_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        session = ReviewSession(
            query=data["query"],
            providers=data["providers"],
            timestamp=datetime.fromisoformat(data["timestamp"]),
            name=data.get("name"),
            research_context=data.get("research_context"),
        )
        
        # Load query_provider_pairs with backward compatibility
        if "query_provider_pairs" in data:
            session.query_provider_pairs = [
                tuple(pair) for pair in data["query_provider_pairs"]
            ]
        else:
            # Reconstruct from query and providers for old sessions
            for provider in data["providers"]:
                session.add_query_provider_pair(data["query"], provider)

        for dec_data in data["decisions"]:
            paper_data = dec_data["paper"]
            paper = Paper(
                title=paper_data["title"],
                authors=paper_data.get("authors", []),
                year=paper_data.get("year"),
                doi=paper_data.get("doi"),
                abstract=paper_data.get("abstract"),
                venue=paper_data.get("venue"),
                url=paper_data.get("url"),
                pdf_url=paper_data.get("pdf_url"),
                source=paper_data.get("source", "session"),
            )
            status = DecisionStatus(dec_data["status"])
            tags = dec_data.get("tags", [])

            # Load LLM-related fields with backward compatibility
            source_str = dec_data.get("source", "human")
            try:
                source = ReviewSource(source_str)
            except ValueError:
                source = ReviewSource.HUMAN
            is_example = dec_data.get("is_example", False)
            llm_confidence = dec_data.get("llm_confidence")

            session.decisions.append(
                ReviewDecision(
                    paper=paper,
                    provider=dec_data["provider"],
                    status=status,
                    tags=tags,
                    source=source,
                    is_example=is_example,
                    llm_confidence=llm_confidence,
                )
            )
        
        logger.info("Loaded session with %d papers", len(session.decisions))
        return session
    except (json.JSONDecodeError, KeyError, OSError) as e:
        logger.warning("Failed to load session %s: %s", name_or_query, e)
        return None


def list_sessions() -> list[dict[str, Any]]:
    """
    List all saved review sessions.
    
    Returns:
        List of session metadata dictionaries with keys:
        - name: Session name or query
        - query: Original search query
        - timestamp: datetime object
        - kept: Number of kept papers
        - discarded: Number of discarded papers
        - pending: Number of pending papers
        - path: Path to session file
    """
    sessions_dir = _get_sessions_dir()
    logger.debug("Listing sessions from %s", sessions_dir)
    sessions = []
    
    for session_file in sessions_dir.glob("*.json"):
        try:
            with open(session_file, "r", encoding="utf-8") as f:
                data = json.load(f)
            
            # Parse timestamp string to datetime
            timestamp_str = data.get("timestamp", "")
            try:
                timestamp = datetime.fromisoformat(timestamp_str)
            except (ValueError, TypeError):
                timestamp = datetime.now()
            
            # Count decisions by status
            decisions = data.get("decisions", [])
            kept = sum(1 for d in decisions if d.get("status") == "kept")
            discarded = sum(1 for d in decisions if d.get("status") == "discarded")
            pending = sum(1 for d in decisions if d.get("status") == "pending")
            
            sessions.append({
                "name": data.get("name") or data.get("query"),
                "query": data.get("query"),
                "timestamp": timestamp,
                "kept": kept,
                "discarded": discarded,
                "pending": pending,
                "path": session_file,
            })
        except (json.JSONDecodeError, OSError) as e:
            logger.debug("Failed to read session file %s: %s", session_file, e)
    
    logger.debug("Found %d sessions", len(sessions))
    return sorted(sessions, key=lambda s: s.get("timestamp", datetime.min), reverse=True)


def save_search_decisions(query: str, decisions: list[ReviewDecision]) -> None:
    """
    Save decisions for a search query (compatible with notes module).
    
    This function bridges the review module with the notes persistence layer,
    converting ReviewDecision objects to the format expected by notes.py.
    
    Args:
        query: The search query string.
        decisions: List of ReviewDecision objects.
    """
    logger.debug("Saving %d decisions for query: %s", len(decisions), query)
    # Convert to notes module format
    decision_records = {}
    for d in decisions:
        paper_id = get_paper_id(d.paper)
        decision_records[paper_id] = ReviewDecisionRecord(
            status=d.status.value,
            tags=d.tags,  # Tags = motivations for discarded, themes for kept
            title=d.paper.title,
            authors=list(d.paper.authors),
            year=d.paper.year,
            doi=d.paper.doi,
            abstract=d.paper.abstract,
            venue=d.paper.venue,
            url=d.paper.url,
            pdf_url=d.paper.pdf_url,
            provider=d.provider,
        )
    
    notes_save_decisions(query, decision_records)
@

\subsection{Testing Session Functions}

<<test functions>>=
class TestSessionFunctions:
    """Tests for session creation and persistence."""

    def test_create_review_session(self):
        """Can create a review session from search results."""
        # Create mock search result
        result = Mock()
        result.provider = "test"
        result.query = "test query"
        result.papers = [
            Paper(title="Paper 1", authors=["Author"], year=2024),
        ]
        
        session = create_review_session([result], "test query")
        
        assert session.query == "test query"
        assert len(session.decisions) == 1
        assert session.decisions[0].status == DecisionStatus.PENDING
        # Check query_provider_pairs is populated
        assert len(session.query_provider_pairs) == 1
        assert ("test query", "test") in session.query_provider_pairs

    def test_save_and_load_session(self, tmp_path, monkeypatch):
        """Can save and load a session."""
        monkeypatch.setenv("SCHOLAR_DATA_DIR", str(tmp_path))
        
        paper = Paper(title="Test Paper", authors=["Author"], year=2024)
        session = ReviewSession(
            query="test query",
            providers=["test"],
            timestamp=datetime.now(),
            name="my-session",
        )
        session.decisions.append(
            ReviewDecision(
                paper=paper,
                provider="test",
                status=DecisionStatus.KEPT,
                tags=["relevant", "ml"],
            )
        )
        
        save_session(session)
        loaded = load_session("my-session")
        
        assert loaded is not None
        assert loaded.query == "test query"
        assert len(loaded.decisions) == 1
        assert loaded.decisions[0].status == DecisionStatus.KEPT
        assert loaded.decisions[0].tags == ["relevant", "ml"]

    def test_list_sessions(self, tmp_path, monkeypatch):
        """Can list all sessions."""
        monkeypatch.setenv("SCHOLAR_DATA_DIR", str(tmp_path))
        
        session1 = ReviewSession(
            query="query1", providers=["test"], 
            timestamp=datetime.now(), name="session1"
        )
        session2 = ReviewSession(
            query="query2", providers=["test"], 
            timestamp=datetime.now(), name="session2"
        )
        
        save_session(session1)
        save_session(session2)
        
        sessions = list_sessions()
        assert len(sessions) == 2

    def test_save_and_load_query_provider_pairs(self, tmp_path, monkeypatch):
        """Query-provider pairs are saved and loaded correctly."""
        monkeypatch.setenv("SCHOLAR_DATA_DIR", str(tmp_path))
        
        session = ReviewSession(
            query="test query",
            providers=["openalex", "wos"],
            timestamp=datetime.now(),
            name="pairs-session",
        )
        session.add_query_provider_pair("natural language", "openalex")
        session.add_query_provider_pair("boolean AND query", "wos")
        
        save_session(session)
        loaded = load_session("pairs-session")
        
        assert loaded is not None
        assert len(loaded.query_provider_pairs) == 2
        assert ("natural language", "openalex") in loaded.query_provider_pairs
        assert ("boolean AND query", "wos") in loaded.query_provider_pairs

    def test_load_session_backward_compatibility(self, tmp_path, monkeypatch):
        """Old sessions without query_provider_pairs get pairs reconstructed."""
        monkeypatch.setenv("SCHOLAR_DATA_DIR", str(tmp_path))
        
        # Create session file in old format (no query_provider_pairs)
        sessions_dir = tmp_path / "review_sessions"
        sessions_dir.mkdir(parents=True)
        old_session_data = {
            "query": "old query",
            "name": "old-session",
            "providers": ["openalex", "wos"],
            "timestamp": datetime.now().isoformat(),
            "research_context": None,
            "decisions": [],
        }
        session_file = sessions_dir / "old-session.json"
        with open(session_file, "w") as f:
            json.dump(old_session_data, f)
        
        loaded = load_session("old-session")
        
        assert loaded is not None
        # Backward compat: pairs should be reconstructed from query + providers
        assert len(loaded.query_provider_pairs) == 2
        assert ("old query", "openalex") in loaded.query_provider_pairs
        assert ("old query", "wos") in loaded.query_provider_pairs
@


\section{Filtering Decisions}

For convenience, we provide a function to filter a list of review decisions
using a [[SearchFilters]] object. This mirrors the [[filter_papers]] function
in the scholar module, but works directly with [[ReviewDecision]] objects.

This is useful when the TUI or other code needs to filter decisions locally
without re-querying the providers. The filtering is applied to each decision's
paper, not the decision metadata itself.

<<review filtering functions>>=
def filter_decisions(
    decisions: list[ReviewDecision],
    filters: SearchFilters
) -> list[ReviewDecision]:
    """
    Filter a list of review decisions using the given filters.
    
    This function applies filters locally to an existing collection of
    decisions, without making any API calls. The filtering is based on
    each decision's paper attributes (year, venue, etc.).
    
    Args:
        decisions: List of ReviewDecision objects to filter.
        filters: SearchFilters specifying the filtering criteria.
        
    Returns:
        List of decisions whose papers match all active filters.
        
    Example:
        >>> from scholar.review import ReviewDecision, filter_decisions
        >>> from scholar import Paper, SearchFilters
        >>> decisions = [
        ...     ReviewDecision(paper=Paper(title="New", year=2020), provider="test"),
        ...     ReviewDecision(paper=Paper(title="Old", year=2015), provider="test"),
        ... ]
        >>> filters = SearchFilters(year="2018-")
        >>> filter_decisions(decisions, filters)
        [ReviewDecision(paper=Paper(title="New", year=2020), ...)]
    """
    if filters.is_empty():
        return decisions
    return [d for d in decisions if filters.matches(d.paper)]
@

\subsection{Testing filter\_decisions}

Let's verify the filtering function works correctly with decisions:

<<test functions>>=
class TestFilterDecisions:
    """Tests for the filter_decisions function."""

    @pytest.fixture
    def sample_decisions(self):
        """Create sample decisions for testing."""
        return [
            ReviewDecision(
                paper=Paper(
                    title="Recent ML", authors=["Author A"], year=2023, venue="NeurIPS"
                ),
                provider="test",
            ),
            ReviewDecision(
                paper=Paper(
                    title="Older Work", authors=["Author B"], year=2018, venue="ICML"
                ),
                provider="test",
            ),
            ReviewDecision(
                paper=Paper(
                    title="Very Old", authors=["Author C"], year=2010, venue="Nature"
                ),
                provider="test",
            ),
        ]

    def test_filter_by_year(self, sample_decisions):
        """Can filter decisions by year range."""
        filters = SearchFilters(year="2015-")
        result = filter_decisions(sample_decisions, filters)
        assert len(result) == 2
        assert all(d.paper.year >= 2015 for d in result)

    def test_filter_by_venue(self, sample_decisions):
        """Can filter decisions by venue."""
        filters = SearchFilters(venue="NeurIPS")
        result = filter_decisions(sample_decisions, filters)
        assert len(result) == 1
        assert result[0].paper.title == "Recent ML"

    def test_empty_filter_returns_all(self, sample_decisions):
        """Empty filters return all decisions."""
        filters = SearchFilters()
        result = filter_decisions(sample_decisions, filters)
        assert len(result) == 3

    def test_combined_filters(self, sample_decisions):
        """Can combine multiple filters."""
        filters = SearchFilters(year="2015-", venue="ICML")
        result = filter_decisions(sample_decisions, filters)
        assert len(result) == 1
        assert result[0].paper.title == "Older Work"

    def test_no_matches_returns_empty(self, sample_decisions):
        """Returns empty list when no decisions match."""
        filters = SearchFilters(year="2025-")
        result = filter_decisions(sample_decisions, filters)
        assert result == []
@


\section{Report Generation}

The module provides \LaTeX\ report generation with theme and motivation
groupings, enabling systematic documentation of review decisions. When a session
has multiple query-provider pairs (from using different queries with different
providers), the report shows them as a table mapping providers to their
respective queries, rather than a single query field.

<<review report generation>>=
def escape_bibtex(text: str) -> str:
    """Escape special BibTeX characters."""
    text = text.replace('&', r'\&')
    text = text.replace('%', r'\%')
    text = text.replace('$', r'\$')
    text = text.replace('#', r'\#')
    text = text.replace('_', r'\_')
    return text


def escape_latex(text: str) -> str:
    """
    Escape special LaTeX characters.
    
    Order matters: backslash must be escaped first to avoid double-escaping.
    """
    text = text.replace('\\', r'\textbackslash{}')
    replacements = {
        '&': r'\&',
        '%': r'\%',
        '$': r'\$',
        '#': r'\#',
        '_': r'\_',
        '{': r'\{',
        '}': r'\}',
        '~': r'\textasciitilde{}',
        '^': r'\textasciicircum{}',
    }
    for char, replacement in replacements.items():
        text = text.replace(char, replacement)
    return text


def markdown_to_latex(markdown_text: str) -> str:
    """
    Convert Markdown text to LaTeX using pypandoc.
    
    Falls back to escaped plain text if pypandoc is not available.
    """
    try:
        import pypandoc
        return pypandoc.convert_text(markdown_text, 'latex', format='markdown')
    except Exception:
        return escape_latex(markdown_text)


def generate_latex_report(session: ReviewSession, output_path: Path) -> None:
    """
    Generate a LaTeX report of the review session.
    
    The report includes:
    - Search parameters (query, providers, date)
    - Summary statistics
    - Kept papers grouped by theme
    - Discarded papers grouped by motivation
    
    Also generates a .bib file with the same base name.
    
    Args:
        session: The ReviewSession to report on.
        output_path: Path for the .tex file (will also create .bib).
    """
    def make_cite_key(paper: Paper, index: int) -> str:
        """Generate a unique citation key for a paper."""
        if paper.authors:
            first_author = paper.authors[0].split()[-1].lower()
            first_author = ''.join(c for c in first_author if c.isalnum())
        else:
            first_author = "unknown"
        year = paper.year or "nd"
        return f"{first_author}{year}_{index}"
    
    def format_bibtex_entry(paper: Paper, cite_key: str) -> str:
        """Format a paper as a BibTeX entry."""
        lines = [f"@article{{{cite_key},"]
        lines.append(f"  title = {{{escape_bibtex(paper.title)}}},")
        if paper.authors:
            authors = " and ".join(paper.authors)
            lines.append(f"  author = {{{escape_bibtex(authors)}}},")
        if paper.year:
            lines.append(f"  year = {{{paper.year}}},")
        if paper.venue:
            lines.append(f"  journal = {{{escape_bibtex(paper.venue)}}},")
        if paper.doi:
            lines.append(f"  doi = {{{paper.doi}}},")
        if paper.url:
            lines.append(f"  url = {{{paper.url}}},")
        lines.append("}")
        return "\n".join(lines)

    def format_list_table(
        decisions: list[ReviewDecision],
        cite_keys: dict[int, str],
        tag_label: str,
    ) -> list[str]:
        """
        Generate LaTeX for a list table with Title, Year, Tags columns.

        Uses \\citetitle for paper titles to leverage biblatex formatting.
        """
        lines = [
            r"\begin{longtable}{>{\raggedright\arraybackslash}p{0.55\textwidth}"
            r"c>{\raggedright\arraybackslash}p{0.25\textwidth}}",
            r"\toprule",
            f"\\textbf{{Title}} & \\textbf{{Year}} & \\textbf{{{tag_label}}} \\\\",
            r"\midrule",
            r"\endhead",
            r"\bottomrule",
            r"\endlastfoot",
        ]
        for decision in decisions:
            cite_key = cite_keys[id(decision)]
            year = decision.paper.year or "---"
            tags = ", ".join(escape_latex(t) for t in sorted(decision.tags))
            lines.append(
                f"\\citetitle{{{cite_key}}} & {year} & {tags} \\\\"
            )
        lines.append(r"\end{longtable}")
        return lines

    def format_matrix_table(
        decisions: list[ReviewDecision],
        cite_keys: dict[int, str],
        tags: set[str],
    ) -> list[str]:
        """
        Generate LaTeX for a cross-tabulation matrix (papers x tags).

        Uses \\citetitle for paper titles and \\checkmark for presence.
        """
        sorted_tags = sorted(tags)
        num_tags = len(sorted_tags)

        # Build column spec: title column + one centered column per tag
        col_spec = r">{\raggedright\arraybackslash}p{0.4\textwidth}" + "c" * num_tags

        lines = [
            f"\\begin{{longtable}}{{{col_spec}}}",
            r"\toprule",
        ]

        # Header row with rotated tag names
        header_cells = [r"\textbf{Title}"]
        for tag in sorted_tags:
            header_cells.append(f"\\rotatebox{{90}}{{{escape_latex(tag)}}}")
        lines.append(" & ".join(header_cells) + r" \\")
        lines.append(r"\midrule")
        lines.append(r"\endhead")
        lines.append(r"\bottomrule")
        lines.append(r"\endlastfoot")

        # Data rows
        for decision in decisions:
            cite_key = cite_keys[id(decision)]
            row_cells = [f"\\citetitle{{{cite_key}}}"]
            for tag in sorted_tags:
                if decision.has_tag(tag):
                    row_cells.append(r"$\checkmark$")
                else:
                    row_cells.append("")
            lines.append(" & ".join(row_cells) + r" \\")

        lines.append(r"\end{longtable}")
        return lines

    # Generate citation keys for all papers
    all_decisions = session.kept_papers + session.discarded_papers
    cite_keys: dict[int, str] = {}
    for i, decision in enumerate(all_decisions):
        cite_keys[id(decision)] = make_cite_key(decision.paper, i)
    
    # Generate .bib file
    bib_entries = []
    for decision in all_decisions:
        cite_key = cite_keys[id(decision)]
        bib_entries.append(format_bibtex_entry(decision.paper, cite_key))
    
    bib_path = output_path.with_suffix('.bib')
    bib_path.write_text("\n\n".join(bib_entries))
    
    # Generate .tex file
    lines = [
        r"\documentclass{article}",
        r"\usepackage[utf8]{inputenc}",
        r"\usepackage{hyperref}",
        r"\usepackage{enumitem}",
        r"\usepackage{longtable}",
        r"\usepackage{booktabs}",
        r"\usepackage{array}",
        r"\usepackage{amssymb}",
        r"\usepackage{graphicx}",
        r"\usepackage{geometry}",
        r"\usepackage[backend=biber,style=authoryear]{biblatex}",
        f"\\addbibresource{{{bib_path.name}}}",
        r"\geometry{margin=1in}",
        r"",
        r"\title{Literature Review Report}",
        f"\\date{{{session.timestamp.strftime('%Y-%m-%d')}}}",
        r"",
        r"\begin{document}",
        r"\maketitle",
        r"",
        r"\section{Search Parameters}",
    ]
    
    # Show query-provider pairs as table if multiple, else simple list
    if len(session.query_provider_pairs) > 1:
        lines.extend([
            r"\begin{longtable}{lp{10cm}}",
            r"\toprule",
            r"Provider & Query \\",
            r"\midrule",
        ])
        for query, provider in session.query_provider_pairs:
            lines.append(
                f"{escape_latex(provider)} & {escape_latex(query)} \\\\"
            )
        lines.extend([
            r"\bottomrule",
            r"\end{longtable}",
        ])
    else:
        lines.extend([
            r"\begin{description}",
            f"\\item[Query] {escape_latex(session.query)}",
            f"\\item[Providers] {', '.join(session.providers)}",
            r"\end{description}",
        ])
    
    lines.extend([
        r"\begin{description}",
        f"\\item[Date] {session.timestamp.strftime('%Y-%m-%d %H:%M')}",
        f"\\item[Total Papers] {len(session.decisions)}",
        r"\end{description}",
        r"",
        r"\section{Summary}",
        r"\begin{description}",
        f"\\item[Kept] {len(session.kept_papers)}",
        f"\\item[Discarded] {len(session.discarded_papers)}",
        f"\\item[Pending] {len(session.pending_papers)}",
        r"\end{description}",
        r"",
    ])

    # Summary Tables section
    if session.kept_papers or session.discarded_papers:
        lines.extend([
            r"\section{Summary Tables}",
            r"",
        ])

        if session.kept_papers:
            lines.extend([
                r"\subsection{Kept Papers Overview}",
                r"",
            ])
            lines.extend(format_list_table(
                session.kept_papers, cite_keys, "Themes"
            ))
            lines.append(r"")
            themes = session.all_themes()
            if themes:
                lines.extend(format_matrix_table(
                    session.kept_papers, cite_keys, themes
                ))
                lines.append(r"")

        if session.discarded_papers:
            lines.extend([
                r"\subsection{Discarded Papers Overview}",
                r"",
            ])
            lines.extend(format_list_table(
                session.discarded_papers, cite_keys, "Motivations"
            ))
            lines.append(r"")
            motivations = session.all_motivations()
            if motivations:
                lines.extend(format_matrix_table(
                    session.discarded_papers, cite_keys, motivations
                ))
                lines.append(r"")

    # Kept papers section - grouped by theme
    if session.kept_papers:
        lines.extend([
            r"\section{Kept Papers}",
        ])
        
        themes = session.all_themes()
        if themes:
            # Group by theme
            for theme in sorted(themes):
                papers_with_theme = [
                    d for d in session.kept_papers if d.has_tag(theme)
                ]
                lines.append(f"\\subsection{{{escape_latex(theme)}}}")
                lines.append(r"\begin{enumerate}")
                for decision in papers_with_theme:
                    cite_key = cite_keys[id(decision)]
                    lines.append(f"\\item \\fullcite{{{cite_key}}}")
                lines.append(r"\end{enumerate}")
                lines.append("")
        else:
            # No themes, just list papers
            lines.append(r"\begin{enumerate}")
            for decision in session.kept_papers:
                cite_key = cite_keys[id(decision)]
                lines.append(f"\\item \\fullcite{{{cite_key}}}")
            lines.append(r"\end{enumerate}")
            lines.append("")
    
    # Discarded papers section - grouped by motivation
    if session.discarded_papers:
        lines.extend([
            r"\section{Discarded Papers}",
        ])
        
        motivations = session.all_motivations()
        if motivations:
            # Group by motivation
            for motivation in sorted(motivations):
                papers_with_motivation = [
                    d for d in session.discarded_papers if d.has_tag(motivation)
                ]
                lines.append(f"\\subsection{{{escape_latex(motivation)}}}")
                lines.append(r"\begin{enumerate}")
                for decision in papers_with_motivation:
                    cite_key = cite_keys[id(decision)]
                    lines.append(f"\\item \\fullcite{{{cite_key}}}")
                lines.append(r"\end{enumerate}")
                lines.append("")
        else:
            # No motivations, just list papers
            lines.append(r"\begin{enumerate}")
            for decision in session.discarded_papers:
                cite_key = cite_keys[id(decision)]
                lines.append(f"\\item \\fullcite{{{cite_key}}}")
            lines.append(r"\end{enumerate}")
            lines.append("")
    
    lines.extend([
        r"\end{document}",
    ])
    
    output_path.write_text("\n".join(lines))


def generate_csv_report(session: ReviewSession, output_path: Path) -> None:
    """
    Generate a CSV report of the review session.
    
    The CSV includes all papers (kept and discarded) with columns:
    - status: kept/discarded/pending
    - title, authors, year, venue, doi, url
    - tags: semicolon-separated list of themes/motivations
    - abstract: full abstract text
    - provider: source of the paper
    
    Args:
        session: The ReviewSession to report on.
        output_path: Path for the .csv file.
    """
    import csv
    
    with open(output_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        
        # Header row
        writer.writerow([
            'status', 'title', 'authors', 'year', 'venue', 
            'doi', 'url', 'tags', 'abstract', 'provider'
        ])
        
        # Write all decisions
        for decision in session.decisions:
            paper = decision.paper
            writer.writerow([
                decision.status.value,
                paper.title,
                '; '.join(paper.authors) if paper.authors else '',
                paper.year or '',
                paper.venue or '',
                paper.doi or '',
                paper.url or '',
                '; '.join(decision.tags) if decision.tags else '',
                paper.abstract or '',
                decision.provider,
            ])
@

\subsection{Testing Report Generation}

<<test functions>>=
class TestReportGeneration:
    """Tests for LaTeX report generation."""

    def test_generate_report_with_themes(self, tmp_path):
        """Report groups kept papers by theme."""
        paper1 = Paper(title="Paper 1", authors=["Author A"], year=2024)
        paper2 = Paper(title="Paper 2", authors=["Author B"], year=2024)
        
        session = ReviewSession(
            query="test",
            providers=["test"],
            timestamp=datetime.now(),
        )
        session.decisions = [
            ReviewDecision(
                paper=paper1, provider="test",
                status=DecisionStatus.KEPT, tags=["ml", "privacy"]
            ),
            ReviewDecision(
                paper=paper2, provider="test",
                status=DecisionStatus.KEPT, tags=["ml"]
            ),
        ]
        
        output_path = tmp_path / "report.tex"
        generate_latex_report(session, output_path)
        
        content = output_path.read_text()
        assert r"\subsection{ml}" in content
        assert r"\subsection{privacy}" in content

    def test_generate_report_with_motivations(self, tmp_path):
        """Report groups discarded papers by motivation."""
        paper = Paper(title="Discarded Paper", authors=["Author"], year=2024)
        
        session = ReviewSession(
            query="test",
            providers=["test"],
            timestamp=datetime.now(),
        )
        session.decisions = [
            ReviewDecision(
                paper=paper, provider="test",
                status=DecisionStatus.DISCARDED, tags=["off-topic"]
            ),
        ]
        
        output_path = tmp_path / "report.tex"
        generate_latex_report(session, output_path)
        
        content = output_path.read_text()
        assert r"\section{Discarded Papers}" in content
        assert r"\subsection{off-topic}" in content

    def test_escapes_special_characters(self, tmp_path):
        """Special characters are escaped in report."""
        paper = Paper(
            title="Test & Paper with 100% special_chars",
            authors=["O'Brien"],
            year=2024,
        )
        
        session = ReviewSession(
            query="test",
            providers=["test"],
            timestamp=datetime.now(),
        )
        session.decisions = [
            ReviewDecision(paper=paper, provider="test", status=DecisionStatus.KEPT),
        ]
        
        output_path = tmp_path / "report.tex"
        generate_latex_report(session, output_path)
        
        bib_content = output_path.with_suffix(".bib").read_text()
        assert r"\&" in bib_content
        assert r"\%" in bib_content

    def test_generate_report_with_list_table(self, tmp_path):
        """Report includes list table for kept papers with longtable."""
        paper1 = Paper(title="Paper 1", authors=["Author A"], year=2024)
        paper2 = Paper(title="Paper 2", authors=["Author B"], year=2023)

        session = ReviewSession(
            query="test",
            providers=["test"],
            timestamp=datetime.now(),
        )
        session.decisions = [
            ReviewDecision(
                paper=paper1, provider="test",
                status=DecisionStatus.KEPT, tags=["ml", "privacy"]
            ),
            ReviewDecision(
                paper=paper2, provider="test",
                status=DecisionStatus.KEPT, tags=["ml"]
            ),
        ]

        output_path = tmp_path / "report.tex"
        generate_latex_report(session, output_path)

        content = output_path.read_text()
        assert r"\begin{longtable}" in content
        assert r"\toprule" in content
        assert "Kept Papers Overview" in content

    def test_generate_report_with_matrix_table(self, tmp_path):
        """Report includes cross-tabulation matrix with checkmarks."""
        paper1 = Paper(title="Paper 1", authors=["Author A"], year=2024)
        paper2 = Paper(title="Paper 2", authors=["Author B"], year=2023)

        session = ReviewSession(
            query="test",
            providers=["test"],
            timestamp=datetime.now(),
        )
        session.decisions = [
            ReviewDecision(
                paper=paper1, provider="test",
                status=DecisionStatus.KEPT, tags=["ml", "privacy"]
            ),
            ReviewDecision(
                paper=paper2, provider="test",
                status=DecisionStatus.KEPT, tags=["ml"]
            ),
        ]

        output_path = tmp_path / "report.tex"
        generate_latex_report(session, output_path)

        content = output_path.read_text()
        assert r"$\checkmark$" in content
        assert r"\rotatebox{90}" in content

    def test_generate_report_uses_citetitle(self, tmp_path):
        """Report uses citetitle for paper titles in tables."""
        paper = Paper(title="Test Paper", authors=["Author"], year=2024)

        session = ReviewSession(
            query="test",
            providers=["test"],
            timestamp=datetime.now(),
        )
        session.decisions = [
            ReviewDecision(
                paper=paper, provider="test",
                status=DecisionStatus.KEPT, tags=["ml"]
            ),
        ]

        output_path = tmp_path / "report.tex"
        generate_latex_report(session, output_path)

        content = output_path.read_text()
        assert r"\citetitle{" in content

    def test_table_handles_no_tags(self, tmp_path):
        """Report shows list table but skips matrix when no tags present."""
        paper = Paper(title="Paper Without Tags", authors=["Author"], year=2024)

        session = ReviewSession(
            query="test",
            providers=["test"],
            timestamp=datetime.now(),
        )
        session.decisions = [
            ReviewDecision(
                paper=paper, provider="test",
                status=DecisionStatus.KEPT, tags=[]
            ),
        ]

        output_path = tmp_path / "report.tex"
        generate_latex_report(session, output_path)

        content = output_path.read_text()
        # Should have list table
        assert r"\begin{longtable}" in content
        # Should not have checkmarks (no matrix table)
        assert r"$\checkmark$" not in content

    def test_discarded_papers_tables(self, tmp_path):
        """Report includes tables for discarded papers."""
        paper = Paper(title="Discarded Paper", authors=["Author"], year=2024)

        session = ReviewSession(
            query="test",
            providers=["test"],
            timestamp=datetime.now(),
        )
        session.decisions = [
            ReviewDecision(
                paper=paper, provider="test",
                status=DecisionStatus.DISCARDED, tags=["off-topic", "not-peer-reviewed"]
            ),
        ]

        output_path = tmp_path / "report.tex"
        generate_latex_report(session, output_path)

        content = output_path.read_text()
        assert "Discarded Papers Overview" in content
        assert "off-topic" in content
        assert "not-peer-reviewed" in content
@
